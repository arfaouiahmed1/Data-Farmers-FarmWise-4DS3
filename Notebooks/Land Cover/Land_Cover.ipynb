{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb189ab",
   "metadata": {},
   "source": [
    "# Land Cover Classification using DeepLabV3+ (MobileNetV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc53f23",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install and import necessary libraries.\n",
    "\n",
    "**Note:** If using a **TPU accelerator** on Kaggle, uncomment the TPU-specific installation lines in the next cell. Ensure the dataset is correctly placed in the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install base libraries\n",
    "!pip install -q transformers datasets evaluate accelerate Pillow torch torchvision torchaudio numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# --- TPU Setup ---\n",
    "# Uncomment the following lines ONLY if using a TPU accelerator in your Kaggle session\n",
    "# print(\"Installing TPU-specific libraries using env-setup.py...\")\n",
    "# # Download the setup script\n",
    "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# # Run the setup script (adjust version if needed, e.g., \"2.1.0\", \"2.0.0\")\n",
    "# !python pytorch-xla-env-setup.py --version 2.1.0 --apt-packages libomp5 libopenblas-dev\n",
    "# print(\"TPU libraries setup complete (if script ran successfully).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict, Image as HFImage\n",
    "from transformers import AutoModelForSemanticSegmentation, AutoFeatureExtractor, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from huggingface_hub import notebook_login\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- TPU Imports (conditional) ---\n",
    "_TPU_AVAILABLE = False\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    # Check if TPU is actually available\n",
    "    if xm.xla_device(): \n",
    "        print(\"torch_xla found and TPU device is available.\")\n",
    "        _TPU_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"torch_xla found, but no TPU device detected. Check accelerator settings.\")\n",
    "except ImportError:\n",
    "    print(\"torch_xla not found. Running on CPU/GPU.\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44a034",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the DeepGlobe Land Cover Classification dataset. \n",
    "You might need to download it from Kaggle first: https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\n",
    "\n",
    "**Important:** Ensure the dataset is placed in the Kaggle input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard Kaggle input directory\n",
    "dataset_base_dir = '/kaggle/input/deepglobe-land-cover-classification-dataset'\n",
    "dataset_root_dir = os.path.join(dataset_base_dir, 'deepglobe')\n",
    "metadata_path = os.path.join(dataset_root_dir, 'metadata.csv')\n",
    "\n",
    "# Check if the dataset path exists\n",
    "if not os.path.exists(dataset_root_dir):\n",
    "    print(f\"Error: Dataset directory not found at {dataset_root_dir}\")\n",
    "    print(\"Please ensure the DeepGlobe dataset is correctly placed in the Kaggle input directory.\")\n",
    "    # You might want to raise an error or exit here in a real script\n",
    "    # For the notebook, we'll proceed but expect errors later.\n",
    "    metadata_df = pd.DataFrame(columns=['image_id', 'split', 'sat_image_path', 'mask_path']) # Dummy df\n",
    "else:\n",
    "    print(f\"Dataset found at: {dataset_root_dir}\")\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    # Prepend the root directory to the paths in the CSV\n",
    "    metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda x: os.path.join(dataset_root_dir, x))\n",
    "    metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda x: os.path.join(dataset_root_dir, x))\n",
    "\n",
    "# Define class names and their corresponding IDs\n",
    "id2label = {\n",
    "    0: 'urban_land',\n",
    "    1: 'agriculture_land',\n",
    "    2: 'rangeland',\n",
    "    3: 'forest_land',\n",
    "    4: 'water',\n",
    "    5: 'barren_land',\n",
    "    6: 'unknown'\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "class_names = list(id2label.values())\n",
    "\n",
    "# Function to load data paths based on split from metadata.csv\n",
    "def load_data_paths(df, split):\n",
    "    split_df = df[df['split'] == split]\n",
    "    image_paths = split_df['sat_image_path'].tolist()\n",
    "    mask_paths = split_df['mask_path'].tolist()\n",
    "    # Verify files exist (optional but recommended)\n",
    "    image_paths = [p for p in image_paths if os.path.exists(p)]\n",
    "    mask_paths = [p for p in mask_paths if os.path.exists(p)]\n",
    "    print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks for split '{split}'.\")\n",
    "    return image_paths, mask_paths\n",
    "\n",
    "train_image_paths, train_mask_paths = load_data_paths(metadata_df, 'train')\n",
    "val_image_paths, val_mask_paths = load_data_paths(metadata_df, 'valid')\n",
    "test_image_paths, test_mask_paths = load_data_paths(metadata_df, 'test')\n",
    "\n",
    "# Create Hugging Face Datasets\n",
    "def create_hf_dataset(image_paths, mask_paths):\n",
    "    if not image_paths or not mask_paths or len(image_paths) != len(mask_paths):\n",
    "        print(f\"Warning: Mismatch or empty paths. Creating empty dataset.\")\n",
    "        return Dataset.from_dict({'image': [], 'label': []}).cast_column('image', HFImage()).cast_column('label', HFImage())\n",
    "    dataset = Dataset.from_dict({'image': image_paths, 'label': mask_paths})\n",
    "    # Casting ensures the columns are treated as images\n",
    "    dataset = dataset.cast_column('image', HFImage())\n",
    "    dataset = dataset.cast_column('label', HFImage())\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_hf_dataset(train_image_paths, train_mask_paths)\n",
    "val_dataset = create_hf_dataset(val_image_paths, val_mask_paths)\n",
    "test_dataset = create_hf_dataset(test_image_paths, test_mask_paths)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84e93d",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Define feature extractor and transformations. The masks in DeepGlobe are RGB images. We need a function to convert these RGB masks to class ID masks (0-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the feature extractor corresponding to the new model\n",
    "model_checkpoint = \"google/deeplabv3_mobilenet_v2_1.0_513\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the RGB to Class ID mapping (Verify these with dataset documentation/inspection)\n",
    "rgb_to_id = {\n",
    "    (0, 255, 255): 0,  # Urban land (Cyan)\n",
    "    (255, 255, 0): 1,  # Agriculture land (Yellow)\n",
    "    (255, 0, 255): 2,  # Rangeland (Magenta)\n",
    "    (0, 255, 0): 3,    # Forest land (Green)\n",
    "    (0, 0, 255): 4,    # Water (Blue)\n",
    "    (255, 255, 255): 5,# Barren land (White)\n",
    "    (0, 0, 0): 6       # Unknown (Black)\n",
    "}\n",
    "id_to_rgb = {v: k for k, v in rgb_to_id.items()} # Invert mapping for visualization\n",
    "\n",
    "def rgb_mask_to_class_id_mask(mask_img):\n",
    "    \"\"\"Converts an RGB mask image (PIL Image) to a 2D array of class IDs.\"\"\"\n",
    "    mask_arr = np.array(mask_img.convert('RGB')) # Ensure it's RGB\n",
    "    class_mask = np.full(mask_arr.shape[:2], 6, dtype=np.uint8) # Default to 'unknown'\n",
    "    for rgb, class_id in rgb_to_id.items():\n",
    "        matches = np.all(mask_arr == np.array(rgb).reshape(1, 1, 3), axis=2)\n",
    "        class_mask[matches] = class_id\n",
    "    return Image.fromarray(class_mask) # Return as PIL Image\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    images = [img.convert(\"RGB\") for img in examples['image']]\n",
    "    labels = [rgb_mask_to_class_id_mask(mask) for mask in examples['label']]\n",
    "\n",
    "    inputs = feature_extractor(images, labels, return_tensors=\"pt\")\n",
    "\n",
    "    return inputs\n",
    "\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "if len(ds['train']) > 0:\n",
    "   processed_ds = ds.map(preprocess_data, batched=True, batch_size=4)\n",
    "else:\n",
    "   print(\"Skipping preprocessing as datasets are empty.\")\n",
    "   processed_ds = ds\n",
    "\n",
    "print(\"\\nProcessed dataset structure (first element example):\")\n",
    "if len(processed_ds['train']) > 0:\n",
    "    print(processed_ds['train'][0])\n",
    "else:\n",
    "    print(\"Train dataset is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168805cb",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Load a pre-trained DeepLabV3+ model and configure it for our specific number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new model using AutoModelForSemanticSegmentation\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "if _TPU_AVAILABLE:\n",
    "    device = xm.xla_device()\n",
    "    print(f\"Using TPU device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using non-TPU device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e97faa",
   "metadata": {},
   "source": [
    "**Note:** The model loaded here (`google/deeplabv3_mobilenet_v2_1.0_513`) is **DeepLabV3+** with a **MobileNetV2** backbone. This architecture is known for providing a good balance between segmentation accuracy and computational efficiency, making it suitable for faster inference compared to larger transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574995",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Set up `TrainingArguments` and define the evaluation metric (Mean Intersection over Union - mIoU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91176e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    if isinstance(logits, torch.Tensor):\n",
    "        logits = logits.detach().cpu()\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "\n",
    "    logits_shape = logits.shape[-2:]\n",
    "    labels_shape = labels.shape[-2:]\n",
    "\n",
    "    if logits_shape != labels_shape:\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels_shape,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "    else:\n",
    "        upsampled_logits = logits\n",
    "\n",
    "    pred_labels = upsampled_logits.argmax(dim=1).numpy()\n",
    "\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred_labels,\n",
    "        references=labels,\n",
    "        num_labels=num_labels,\n",
    "        ignore_index=6,\n",
    "        reduce_labels=False,\n",
    "    )\n",
    "\n",
    "    per_category_iou = metrics.pop('per_category_iou', [0.0] * num_labels)\n",
    "    per_category_accuracy = metrics.pop('per_category_accuracy', [0.0] * num_labels)\n",
    "    for i, label in id2label.items():\n",
    "        metrics[f\"iou_{label}\"] = per_category_iou[i]\n",
    "        metrics[f\"accuracy_{label}\"] = per_category_accuracy[i]\n",
    "\n",
    "    return {\n",
    "        \"mean_iou\": metrics.get(\"mean_iou\", 0.0),\n",
    "        \"mean_accuracy\": metrics.get(\"mean_accuracy\", 0.0),\n",
    "        \"overall_accuracy\": metrics.get(\"overall_accuracy\", 0.0),\n",
    "        **metrics\n",
    "    }\n",
    "\n",
    "train_batch_size = 16 if _TPU_AVAILABLE else 8\n",
    "eval_batch_size = 16 if _TPU_AVAILABLE else 8\n",
    "print(f\"Using Train Batch Size: {train_batch_size}, Eval Batch Size: {eval_batch_size}\")\n",
    "\n",
    "output_dir_name = \"./deeplabv3-mobilenetv2-finetuned-deepglobe\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_name,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=train_batch_size, \n",
    "    per_device_eval_batch_size=eval_batch_size,  \n",
    "    save_total_limit=2, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mean_iou\",\n",
    "    push_to_hub=False, \n",
    "    remove_unused_columns=False, \n",
    "    fp16=torch.cuda.is_available() and not _TPU_AVAILABLE, \n",
    "    tpu_num_cores=8 if _TPU_AVAILABLE else None, \n",
    "    dataloader_num_workers=2, \n",
    "    report_to=\"none\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c17c21",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning\n",
    "\n",
    "Instantiate the `Trainer` and start the fine-tuning process. You might want to log in to Hugging Face if you plan to push the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Hugging Face Hub if push_to_hub=True\n",
    "# notebook_login()\n",
    "\n",
    "# Check if datasets are valid before creating Trainer\n",
    "train_data_available = 'train' in processed_ds and len(processed_ds['train']) > 0\n",
    "eval_data_available = 'validation' in processed_ds and len(processed_ds['validation']) > 0\n",
    "\n",
    "trainer = None\n",
    "if train_data_available and eval_data_available:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_ds[\"train\"],\n",
    "        eval_dataset=processed_ds[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_results = trainer.train()\n",
    "    \n",
    "    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "        print(\"Saving model and state...\")\n",
    "        trainer.save_model() \n",
    "        trainer.save_state()\n",
    "        print(\"Model and state saved.\")\n",
    "    else:\n",
    "        print(f\"Skipping save on TPU replica {xm.get_ordinal()}\")\n",
    "        \n",
    "    if _TPU_AVAILABLE:\n",
    "        xm.rendezvous('save_model_done')\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    print(\"Training Results:\", train_results)\n",
    "    \n",
    "    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "        metrics = train_results.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "    \n",
    "    print(\"\\nEvaluating final model on validation set...\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "        trainer.log_metrics(\"eval\", eval_metrics)\n",
    "        trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping training as train or validation dataset is empty or invalid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd820e",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Visualization\n",
    "\n",
    "Evaluate the fine-tuned model on the test set and visualize some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_available = 'test' in processed_ds and len(processed_ds['test']) > 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_metrics = None\n",
    "\n",
    "if trainer is not None and test_data_available:\n",
    "    print(\"\\nEvaluating on the test set...\")\n",
    "    test_results = trainer.predict(processed_ds['test'])\n",
    "    test_metrics = test_results.metrics\n",
    "    \n",
    "    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "        print(\"\\nTest Set Evaluation Results:\")\n",
    "        print(test_metrics)\n",
    "        trainer.log_metrics(\"test\", test_metrics)\n",
    "        trainer.save_metrics(\"test\", test_metrics)\n",
    "    \n",
    "    logits = test_results.predictions\n",
    "    labels = test_results.label_ids\n",
    "    \n",
    "    if isinstance(logits, torch.Tensor) and logits.device.type == 'xla':\n",
    "        logits = logits.cpu()\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "        \n",
    "    if isinstance(labels, torch.Tensor) and labels.device.type == 'xla':\n",
    "        labels = labels.cpu()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "    logits_shape = logits.shape[-2:]\n",
    "    labels_shape = labels.shape[-2:]\n",
    "    if logits_shape != labels_shape:\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits,\n",
    "            size=labels_shape,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "    else:\n",
    "        upsampled_logits = logits\n",
    "        \n",
    "    all_preds = upsampled_logits.argmax(dim=1).detach().cpu().numpy().flatten()\n",
    "    all_labels = labels.flatten()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping test set evaluation as trainer was not initialized or test dataset is empty.\")\n",
    "\n",
    "if (not _TPU_AVAILABLE or xm.is_master_ordinal()) and len(all_preds) > 0 and len(all_labels) > 0:\n",
    "    ignore_idx = 6 \n",
    "    valid_indices = all_labels != ignore_idx\n",
    "    filtered_labels = all_labels[valid_indices]\n",
    "    filtered_preds = all_preds[valid_indices]\n",
    "    \n",
    "    print(f\"\\nGenerating Confusion Matrix (ignoring class {ignore_idx}: '{id2label.get(ignore_idx, 'N/A')}')\")\n",
    "    if len(filtered_labels) > 0:\n",
    "        cm = confusion_matrix(filtered_labels, filtered_preds, labels=list(range(num_labels-1)))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names[:-1], yticklabels=class_names[:-1])\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix (Test Set)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping confusion matrix: No valid labels after filtering.\")\n",
    "elif not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "    print(\"Skipping confusion matrix generation (no predictions/labels available).\")\n",
    "\n",
    "def visualize_predictions(num_samples=5):\n",
    "    if trainer is None or not test_data_available or len(test_image_paths) == 0:\n",
    "        print(\"Skipping visualization: Trainer not available, test data missing, or no test image paths.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nVisualizing predictions for {num_samples} random test samples...\")\n",
    "    \n",
    "    print(f\"Loading best model from {training_args.output_dir} for visualization...\")\n",
    "    try:\n",
    "        viz_model = AutoModelForSemanticSegmentation.from_pretrained(training_args.output_dir).cpu()\n",
    "        viz_model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading saved model for visualization: {e}. Using current model state on CPU.\")\n",
    "        viz_model = model.cpu()\n",
    "        viz_model.eval()\n",
    "    \n",
    "    try: \n",
    "        _ = feature_extractor\n",
    "    except NameError:\n",
    "        print(\"Error: feature_extractor not found. Reloading...\")\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "        \n",
    "    num_available = len(test_image_paths)\n",
    "    indices = random.sample(range(num_available), min(num_samples, num_available))\n",
    "    \n",
    "    for i in indices:\n",
    "        image_path = test_image_paths[i]\n",
    "        mask_path = test_mask_paths[i]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            true_mask_rgb = Image.open(mask_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "        pixel_values = encoding.pixel_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = viz_model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        logits_shape_viz = logits.shape[-2:]\n",
    "        image_shape_viz = image.size[::-1]\n",
    "        if logits_shape_viz != image_shape_viz:\n",
    "            upsampled_logits = F.interpolate(\n",
    "                logits,\n",
    "                size=image_shape_viz,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        else:\n",
    "            upsampled_logits = logits\n",
    "            \n",
    "        pred_mask_id = upsampled_logits.argmax(dim=1).squeeze().numpy()\n",
    "\n",
    "        pred_mask_rgb = np.zeros((*pred_mask_id.shape, 3), dtype=np.uint8)\n",
    "        for class_id, color in id_to_rgb.items():\n",
    "            pred_mask_rgb[pred_mask_id == class_id] = color\n",
    "            \n",
    "        legend_patches = [plt.Rectangle((0,0),1,1, fc=np.array(color)/255.0) for color in id_to_rgb.values()]\n",
    "        legend_labels = [f\"{idx}: {name}\" for idx, name in id2label.items()]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f\"Sample {i}: {os.path.basename(image_path)}\")\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Input Image\")\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(true_mask_rgb)\n",
    "        axes[1].set_title(\"True Mask (RGB)\")\n",
    "        axes[1].axis('off')\n",
    "        axes[2].imshow(pred_mask_rgb)\n",
    "        axes[2].set_title(\"Predicted Mask (RGB)\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        fig.legend(legend_patches, legend_labels, loc='lower center', ncol=len(id2label), bbox_to_anchor=(0.5, -0.05))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "if not _TPU_AVAILABLE or xm.is_master_ordinal():\n",
    "    visualize_predictions(num_samples=5)\n",
    "else:\n",
    "    print(f\"Skipping visualization on TPU replica {xm.get_ordinal()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
