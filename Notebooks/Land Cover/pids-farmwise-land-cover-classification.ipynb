{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1635643,"sourceType":"datasetVersion","datasetId":966962}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9cb189ab","cell_type":"markdown","source":"# Land Cover Classification using Vision Transformers (SegFormer)","metadata":{}},{"id":"efc53f23","cell_type":"markdown","source":"## 1. Setup Environment\n\nInstall and import necessary libraries.\n\n**Note:** If using a **TPU accelerator** on Kaggle, uncomment the TPU-specific installation lines in the next cell. Ensure the dataset is correctly placed in the input directory.","metadata":{}},{"id":"6eb2a745","cell_type":"code","source":"# Install base libraries\n!pip install --upgrade pip\n!pip install -q transformers datasets evaluate accelerate Pillow torch torchvision torchaudio numpy matplotlib seaborn scikit-learn\n\n# --- TPU Setup ---\n# Uncomment the following lines ONLY if using a TPU accelerator in your Kaggle session\nprint(\"Installing TPU-specific libraries...\")\n!pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\nprint(\"TPU libraries installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:25.059940Z","iopub.execute_input":"2025-04-16T19:27:25.060274Z","iopub.status.idle":"2025-04-16T19:27:33.761569Z","shell.execute_reply.started":"2025-04-16T19:27:25.060247Z","shell.execute_reply":"2025-04-16T19:27:33.756752Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\nCollecting pip\n  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.0.1\n    Uninstalling pip-23.0.1:\n      Successfully uninstalled pip-23.0.1\nSuccessfully installed pip-25.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mInstalling TPU-specific libraries...\nCollecting torch-xla==2.0\n\u001b[31m  ERROR: HTTP error 403 while getting https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not install requirement torch-xla==2.0 from https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl for URL https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\u001b[0m\u001b[31m\n\u001b[0mTPU libraries installed.\n","output_type":"stream"}],"execution_count":4},{"id":"78bcff2d","cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom datasets import Dataset, DatasetDict, Image as HFImage\nfrom transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, Trainer, TrainingArguments\nimport evaluate\nfrom huggingface_hub import notebook_login\nimport random\n\n# --- TPU Imports (conditional) ---\n_TPU_AVAILABLE = False\ntry:\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    # Check if TPU is actually available\n    if xm.xla_device(): \n        print(\"torch_xla found and TPU device is available.\")\n        _TPU_AVAILABLE = True\n    else:\n        print(\"torch_xla found, but no TPU device detected. Check accelerator settings.\")\nexcept ImportError:\n    print(\"torch_xla not found. Running on CPU/GPU.\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:33.763819Z","iopub.execute_input":"2025-04-16T19:27:33.764069Z","iopub.status.idle":"2025-04-16T19:27:54.122385Z","shell.execute_reply.started":"2025-04-16T19:27:33.764045Z","shell.execute_reply":"2025-04-16T19:27:54.116611Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1744831670.376840      10 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:232\n","output_type":"stream"},{"name":"stdout","text":"torch_xla found and TPU device is available.\nPyTorch version: 2.6.0+cu124\nCUDA available: False\n","output_type":"stream"}],"execution_count":5},{"id":"cf44a034","cell_type":"markdown","source":"## 2. Load Dataset\n\nLoad the DeepGlobe Land Cover Classification dataset. \nYou might need to download it from Kaggle first: https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\n\n**Important:** Ensure the dataset is placed in the Kaggle input directory.","metadata":{}},{"id":"309d278a","cell_type":"code","source":"# Define the standard Kaggle input directory\ndataset_base_dir = '/kaggle/input/deepglobe-land-cover-classification-dataset'\ndataset_root_dir = os.path.join(dataset_base_dir, 'deepglobe')\nmetadata_path = os.path.join(dataset_root_dir, 'metadata.csv')\n\n# Check if the dataset path exists\nif not os.path.exists(dataset_root_dir):\n    print(f\"Error: Dataset directory not found at {dataset_root_dir}\")\n    print(\"Please ensure the DeepGlobe dataset is correctly placed in the Kaggle input directory.\")\n    # You might want to raise an error or exit here in a real script\n    # For the notebook, we'll proceed but expect errors later.\n    metadata_df = pd.DataFrame(columns=['image_id', 'split', 'sat_image_path', 'mask_path']) # Dummy df\nelse:\n    print(f\"Dataset found at: {dataset_root_dir}\")\n    metadata_df = pd.read_csv(metadata_path)\n    # Prepend the root directory to the paths in the CSV\n    metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda x: os.path.join(dataset_root_dir, x))\n    metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda x: os.path.join(dataset_root_dir, x))\n\n# Define class names and their corresponding IDs\nid2label = {\n    0: 'urban_land',\n    1: 'agriculture_land',\n    2: 'rangeland',\n    3: 'forest_land',\n    4: 'water',\n    5: 'barren_land',\n    6: 'unknown'\n}\nlabel2id = {v: k for k, v in id2label.items()}\nnum_labels = len(id2label)\nclass_names = list(id2label.values())\n\n# Function to load data paths based on split from metadata.csv\ndef load_data_paths(df, split):\n    split_df = df[df['split'] == split]\n    image_paths = split_df['sat_image_path'].tolist()\n    mask_paths = split_df['mask_path'].tolist()\n    # Verify files exist (optional but recommended)\n    image_paths = [p for p in image_paths if os.path.exists(p)]\n    mask_paths = [p for p in mask_paths if os.path.exists(p)]\n    print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks for split '{split}'.\")\n    return image_paths, mask_paths\n\ntrain_image_paths, train_mask_paths = load_data_paths(metadata_df, 'train')\nval_image_paths, val_mask_paths = load_data_paths(metadata_df, 'valid')\ntest_image_paths, test_mask_paths = load_data_paths(metadata_df, 'test')\n\n# Create Hugging Face Datasets\ndef create_hf_dataset(image_paths, mask_paths):\n    if not image_paths or not mask_paths or len(image_paths) != len(mask_paths):\n        print(f\"Warning: Mismatch or empty paths. Creating empty dataset.\")\n        return Dataset.from_dict({'image': [], 'label': []}).cast_column('image', HFImage()).cast_column('label', HFImage())\n    dataset = Dataset.from_dict({'image': image_paths, 'label': mask_paths})\n    # Casting ensures the columns are treated as images\n    dataset = dataset.cast_column('image', HFImage())\n    dataset = dataset.cast_column('label', HFImage())\n    return dataset\n\ntrain_dataset = create_hf_dataset(train_image_paths, train_mask_paths)\nval_dataset = create_hf_dataset(val_image_paths, val_mask_paths)\ntest_dataset = create_hf_dataset(test_image_paths, test_mask_paths)\n\nds = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\nprint(\"\\nDataset structure:\")\nprint(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:54.124836Z","iopub.execute_input":"2025-04-16T19:27:54.125551Z","iopub.status.idle":"2025-04-16T19:27:54.181214Z","shell.execute_reply.started":"2025-04-16T19:27:54.125522Z","shell.execute_reply":"2025-04-16T19:27:54.174427Z"}},"outputs":[{"name":"stdout","text":"Error: Dataset directory not found at /kaggle/input/deepglobe-land-cover-classification-dataset/deepglobe\nPlease ensure the DeepGlobe dataset is correctly placed in the Kaggle input directory.\nFound 0 images and 0 masks for split 'train'.\nFound 0 images and 0 masks for split 'valid'.\nFound 0 images and 0 masks for split 'test'.\nWarning: Mismatch or empty paths. Creating empty dataset.\nWarning: Mismatch or empty paths. Creating empty dataset.\nWarning: Mismatch or empty paths. Creating empty dataset.\n\nDataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 0\n    })\n    validation: Dataset({\n        features: ['image', 'label'],\n        num_rows: 0\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 0\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"id":"bd84e93d","cell_type":"markdown","source":"## 3. Preprocessing\n\nDefine feature extractor and transformations. The masks in DeepGlobe are RGB images. We need a function to convert these RGB masks to class ID masks (0-6).","metadata":{}},{"id":"9e79827b","cell_type":"code","source":"feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n# do_reduce_labels (removed in newer transformers versions for SegformerFeatureExtractor)\n# If using an older version, you might need: feature_extractor.do_reduce_labels = False\n\n# Define the RGB to Class ID mapping (Verify these with dataset documentation/inspection)\nrgb_to_id = {\n    (0, 255, 255): 0,  # Urban land (Cyan)\n    (255, 255, 0): 1,  # Agriculture land (Yellow)\n    (255, 0, 255): 2,  # Rangeland (Magenta)\n    (0, 255, 0): 3,    # Forest land (Green)\n    (0, 0, 255): 4,    # Water (Blue)\n    (255, 255, 255): 5,# Barren land (White)\n    (0, 0, 0): 6       # Unknown (Black)\n}\nid_to_rgb = {v: k for k, v in rgb_to_id.items()} # Invert mapping for visualization\n\ndef rgb_mask_to_class_id_mask(mask_img):\n    \"\"\"Converts an RGB mask image (PIL Image) to a 2D array of class IDs.\"\"\"\n    mask_arr = np.array(mask_img.convert('RGB')) # Ensure it's RGB\n    class_mask = np.full(mask_arr.shape[:2], 6, dtype=np.uint8) # Default to 'unknown'\n    for rgb, class_id in rgb_to_id.items():\n        matches = np.all(mask_arr == np.array(rgb).reshape(1, 1, 3), axis=2)\n        class_mask[matches] = class_id\n    return Image.fromarray(class_mask) # Return as PIL Image\n\ndef preprocess_data(examples):\n    images = [img.convert(\"RGB\") for img in examples['image']]\n    # Convert RGB masks to class ID masks BEFORE passing to feature extractor\n    # The feature extractor expects labels as single-channel images with class IDs\n    labels = [rgb_mask_to_class_id_mask(mask) for mask in examples['label']]\n\n    # The feature extractor handles resizing and normalization\n    inputs = feature_extractor(images, labels, return_tensors=\"pt\")\n\n    # Squeeze the labels tensor to remove the channel dimension (1)\n    # Shape changes from (batch, 1, H, W) to (batch, H, W)\n    inputs['labels'] = inputs['labels'].squeeze(1)\n\n    return inputs\n\n# Apply preprocessing using .map() for efficiency\n# Use batched=True and consider num_proc > 1 on multi-core machines\nprint(\"\\nApplying preprocessing...\")\nif len(ds['train']) > 0:\n   processed_ds = ds.map(preprocess_data, batched=True, batch_size=4)\n   # Alternative: use with_transform for on-the-fly processing (less memory, more compute during training)\n   # processed_ds = ds.with_transform(preprocess_data)\nelse:\n   print(\"Skipping preprocessing as datasets are empty.\")\n   processed_ds = ds # Keep the empty structure\n\nprint(\"\\nProcessed dataset structure (first element example):\")\nif len(processed_ds['train']) > 0:\n    print(processed_ds['train'][0])\nelse:\n    print(\"Train dataset is empty.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:54.183301Z","iopub.execute_input":"2025-04-16T19:27:54.183631Z","iopub.status.idle":"2025-04-16T19:27:54.322046Z","shell.execute_reply.started":"2025-04-16T19:27:54.183604Z","shell.execute_reply":"2025-04-16T19:27:54.316604Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n  warnings.warn(\n/usr/local/lib/python3.10/site-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n  return func(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nApplying preprocessing...\nSkipping preprocessing as datasets are empty.\n\nProcessed dataset structure (first element example):\nTrain dataset is empty.\n","output_type":"stream"}],"execution_count":7},{"id":"168805cb","cell_type":"markdown","source":"## 4. Model Definition\n\nLoad a pre-trained SegFormer model and configure it for our specific number of classes.","metadata":{}},{"id":"284bafad","cell_type":"code","source":"model = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    num_labels=num_labels,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True, # Allow changing the classification head\n)\n\n# --- Device Selection (Prioritize TPU) ---\nif _TPU_AVAILABLE:\n    device = xm.xla_device()\n    print(f\"Using TPU device: {device}\")\nelse:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using non-TPU device: {device}\")\n\nmodel.to(device)\nprint(f\"Model loaded on {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:54.323902Z","iopub.execute_input":"2025-04-16T19:27:54.324164Z","iopub.status.idle":"2025-04-16T19:27:54.962564Z","shell.execute_reply.started":"2025-04-16T19:27:54.324141Z","shell.execute_reply":"2025-04-16T19:27:54.953143Z"}},"outputs":[{"name":"stderr","text":"Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([7]) in the model instantiated\n- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([7, 256, 1, 1]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using TPU device: xla:0\nModel loaded on xla:0\n","output_type":"stream"}],"execution_count":8},{"id":"bd574995","cell_type":"markdown","source":"## 5. Training Configuration\n\nSet up `TrainingArguments` and define the evaluation metric (Mean Intersection over Union - mIoU).","metadata":{}},{"id":"91176e94","cell_type":"code","source":"metric = evaluate.load(\"mean_iou\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Logits shape: (batch_size, num_labels, height/4, width/4)\n    # Labels shape: (batch_size, height, width)\n\n    # Move logits to CPU for upsampling if they are on TPU\n    # Note: Upsampling might be faster on CPU depending on the setup\n    if isinstance(logits, torch.Tensor) and logits.device.type == 'xla':\n        logits = logits.cpu()\n        \n    # Convert logits to torch tensor if they are numpy arrays\n    if isinstance(logits, np.ndarray):\n        logits = torch.from_numpy(logits)\n        \n    # Ensure labels are also tensors for interpolation size check\n    if isinstance(labels, np.ndarray):\n        labels_tensor = torch.from_numpy(labels)\n    else:\n        labels_tensor = labels\n        \n    # Move labels tensor to CPU if needed for size check\n    if labels_tensor.device.type == 'xla':\n        labels_tensor = labels_tensor.cpu()\n        \n    upsampled_logits = torch.nn.functional.interpolate(\n        logits,\n        size=labels_tensor.shape[-2:], # Target (height, width)\n        mode='bilinear',\n        align_corners=False\n    )\n\n    # Get predicted class IDs\n    pred_labels = upsampled_logits.argmax(dim=1).detach().cpu().numpy()\n    # Ensure labels are numpy arrays on CPU for metric computation\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n\n    # Compute metrics\n    metrics = metric.compute(\n        predictions=pred_labels,\n        references=labels,\n        num_labels=num_labels,\n        ignore_index=6, # Ignore the 'unknown' class if desired, otherwise use 255 or remove\n        reduce_labels=False, # We are not reducing labels\n    )\n\n    # Add per-category IoU metrics for better interpretation\n    # Handle potential KeyError if metric computation failed for some reason\n    per_category_iou = metrics.pop('per_category_iou', [0.0] * num_labels)\n    per_category_accuracy = metrics.pop('per_category_accuracy', [0.0] * num_labels)\n    for i, label in id2label.items():\n        metrics[f\"iou_{label}\"] = per_category_iou[i]\n        metrics[f\"accuracy_{label}\"] = per_category_accuracy[i]\n\n    # Return main metrics\n    return {\n        \"mean_iou\": metrics.get(\"mean_iou\", 0.0),\n        \"mean_accuracy\": metrics.get(\"mean_accuracy\", 0.0),\n        \"overall_accuracy\": metrics.get(\"overall_accuracy\", 0.0),\n        **metrics # Include per-category metrics as well\n    }\n\n# Define Training Arguments (adjust batch size based on device)\n# TPUs often benefit from larger batch sizes than GPUs\ntrain_batch_size = 16 if _TPU_AVAILABLE else 8\neval_batch_size = 16 if _TPU_AVAILABLE else 8\nprint(f\"Using Train Batch Size: {train_batch_size}, Eval Batch Size: {eval_batch_size}\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./segformer-finetuned-deepglobe-kaggle\",\n    learning_rate=6e-5, # Starting point, may need tuning\n    num_train_epochs=15, # Adjust based on convergence and time limits (e.g., 10-50)\n    per_device_train_batch_size=train_batch_size, \n    per_device_eval_batch_size=eval_batch_size,  \n    save_total_limit=2, # Keep only the best and the latest checkpoints\n    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n    save_strategy=\"epoch\", # Save at the end of each epoch\n    logging_strategy=\"steps\",\n    logging_steps=100, # Log every 100 steps\n    load_best_model_at_end=True,\n    metric_for_best_model=\"mean_iou\",\n    push_to_hub=False, # Set to True if you want to upload (requires login)\n    remove_unused_columns=False, # Keep original columns like file paths if needed\n    fp16=torch.cuda.is_available() and not _TPU_AVAILABLE, # Enable mixed precision for GPU, TPUs handle precision differently (bfloat16)\n    tpu_num_cores=8 if _TPU_AVAILABLE else None, # Specify number of TPU cores if available (usually 8 on Kaggle v3-8)\n    dataloader_num_workers=2, # Use multiple workers for data loading if CPU allows\n    report_to=\"none\" # Disable default reporting like wandb unless configured\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:54.963700Z","iopub.execute_input":"2025-04-16T19:27:54.963964Z","iopub.status.idle":"2025-04-16T19:27:55.676895Z","shell.execute_reply.started":"2025-04-16T19:27:54.963938Z","shell.execute_reply":"2025-04-16T19:27:55.673032Z"}},"outputs":[{"name":"stderr","text":"Downloading builder script: 100%|██████████| 12.9k/12.9k [00:00<00:00, 5.68MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Using Train Batch Size: 16, Eval Batch Size: 16\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m eval_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _TPU_AVAILABLE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Train Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Eval Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./segformer-finetuned-deepglobe-kaggle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Starting point, may need tuning\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjust based on convergence and time limits (e.g., 10-50)\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Keep only the best and the latest checkpoints\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Evaluate at the end of each epoch\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Save at the end of each epoch\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Log every 100 steps\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_iou\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want to upload (requires login)\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Keep original columns like file paths if needed\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enable mixed precision for GPU, TPUs handle precision differently (bfloat16)\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_num_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify number of TPU cores if available (usually 8 on Kaggle v3-8)\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use multiple workers for data loading if CPU allows\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Disable default reporting like wandb unless configured\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"],"ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","output_type":"error"}],"execution_count":9},{"id":"50c17c21","cell_type":"markdown","source":"## 6. Fine-tuning\n\nInstantiate the `Trainer` and start the fine-tuning process. You might want to log in to Hugging Face if you plan to push the model.","metadata":{}},{"id":"9fe83e82","cell_type":"code","source":"# Optional: Login to Hugging Face Hub if push_to_hub=True\n# notebook_login()\n\n# Check if datasets are valid before creating Trainer\ntrain_data_available = 'train' in processed_ds and len(processed_ds['train']) > 0\neval_data_available = 'validation' in processed_ds and len(processed_ds['validation']) > 0\n\ntrainer = None # Initialize trainer to None\nif train_data_available and eval_data_available:\n    trainer = Trainer(\n        model=model, # Model is already on the correct device (TPU or GPU/CPU)\n        args=training_args,\n        train_dataset=processed_ds[\"train\"],\n        eval_dataset=processed_ds[\"validation\"],\n        compute_metrics=compute_metrics,\n        # feature_extractor is not needed here if using .map() for preprocessing\n    )\n    \n    # Start training\n    print(\"Starting training...\")\n    train_results = trainer.train()\n    \n    # Save the best model and training state\n    # On TPU, saving needs to happen from the main process\n    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n        print(\"Saving model and state...\")\n        trainer.save_model() \n        trainer.save_state()\n        print(\"Model and state saved.\")\n    else:\n        print(f\"Skipping save on TPU replica {xm.get_ordinal()}\")\n        \n    # Wait for all processes to finish saving if on TPU\n    if _TPU_AVAILABLE:\n        xm.rendezvous('save_model_done')\n        \n    print(\"Training finished.\")\n    print(\"Training Results:\", train_results)\n    \n    # Log metrics (only on main process for TPU)\n    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n        metrics = train_results.metrics\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n    \n    # Evaluate after training (on validation set)\n    print(\"\\nEvaluating final model on validation set...\")\n    eval_metrics = trainer.evaluate() # Evaluation runs on all cores, results aggregated\n    # Log/save metrics (only on main process for TPU)\n    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n        trainer.log_metrics(\"eval\", eval_metrics)\n        trainer.save_metrics(\"eval\", eval_metrics)\n    \nelse:\n    print(\"Skipping training as train or validation dataset is empty or invalid.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:55.680485Z","iopub.status.idle":"2025-04-16T19:27:55.681710Z","shell.execute_reply.started":"2025-04-16T19:27:55.680871Z","shell.execute_reply":"2025-04-16T19:27:55.680888Z"}},"outputs":[],"execution_count":null},{"id":"7efd820e","cell_type":"markdown","source":"## 7. Evaluation & Visualization\n\nEvaluate the fine-tuned model on the test set and visualize some predictions.","metadata":{}},{"id":"8ccd3d15","cell_type":"code","source":"test_data_available = 'test' in processed_ds and len(processed_ds['test']) > 0\nall_preds = []\nall_labels = []\ntest_metrics = None\n\nif trainer is not None and test_data_available:\n    print(\"\\nEvaluating on the test set...\")\n    # Use the predict method to get raw predictions and labels\n    # Prediction also runs on all cores if TPU is used\n    test_results = trainer.predict(processed_ds['test'])\n    test_metrics = test_results.metrics # Metrics are computed based on aggregated predictions/labels\n    \n    # Log/save metrics (only on main process for TPU)\n    if not _TPU_AVAILABLE or xm.is_master_ordinal():\n        print(\"\\nTest Set Evaluation Results:\")\n        print(test_metrics)\n        trainer.log_metrics(\"test\", test_metrics)\n        trainer.save_metrics(\"test\", test_metrics)\n    \n    # Extract predictions and labels for confusion matrix and visualization\n    # These should be available on the main process after predict\n    logits = test_results.predictions\n    labels = test_results.label_ids\n    \n    # Upsample logits and get predicted labels (needs to be done on CPU)\n    if isinstance(logits, torch.Tensor) and logits.device.type == 'xla':\n        logits = logits.cpu()\n    if isinstance(logits, np.ndarray):\n        logits = torch.from_numpy(logits)\n        \n    if isinstance(labels, torch.Tensor) and labels.device.type == 'xla':\n        labels = labels.cpu()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.numpy() # Ensure labels are numpy array\n        \n    upsampled_logits = torch.nn.functional.interpolate(\n        logits,\n        size=labels.shape[-2:], # Target (height, width)\n        mode='bilinear',\n        align_corners=False\n    )\n    all_preds = upsampled_logits.argmax(dim=1).detach().cpu().numpy().flatten()\n    all_labels = labels.flatten()\n    \nelse:\n    print(\"Skipping test set evaluation as trainer was not initialized or test dataset is empty.\")\n\n# --- Confusion Matrix (Run only on main process) ---\nif (not _TPU_AVAILABLE or xm.is_master_ordinal()) and len(all_preds) > 0 and len(all_labels) > 0:\n    # Filter out ignored labels if necessary (e.g., 'unknown' class with id 6)\n    ignore_idx = 6 \n    valid_indices = all_labels != ignore_idx\n    filtered_labels = all_labels[valid_indices]\n    filtered_preds = all_preds[valid_indices]\n    \n    print(f\"\\nGenerating Confusion Matrix (ignoring class {ignore_idx}: '{id2label.get(ignore_idx, 'N/A')}')\")\n    if len(filtered_labels) > 0: # Ensure there are valid labels left\n        cm = confusion_matrix(filtered_labels, filtered_preds, labels=list(range(num_labels-1))) # Exclude ignored class from labels\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names[:-1], yticklabels=class_names[:-1])\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title('Confusion Matrix (Test Set)')\n        plt.show()\n    else:\n        print(\"Skipping confusion matrix: No valid labels after filtering.\")\nelif not _TPU_AVAILABLE or xm.is_master_ordinal():\n    print(\"Skipping confusion matrix generation (no predictions/labels available).\")\n\n# --- Visualization (Run only on main process) ---\ndef visualize_predictions(num_samples=5):\n    if trainer is None or not test_data_available or len(test_image_paths) == 0:\n        print(\"Skipping visualization: Trainer not available, test data missing, or no test image paths.\")\n        return\n        \n    print(f\"\\nVisualizing predictions for {num_samples} random test samples...\")\n    \n    # Ensure model is on CPU for visualization\n    # Load the *saved* best model from disk onto CPU for consistent visualization\n    print(f\"Loading best model from {training_args.output_dir} for visualization...\")\n    try:\n        viz_model = SegformerForSemanticSegmentation.from_pretrained(training_args.output_dir).cpu()\n        viz_model.eval() # Set model to evaluation mode\n    except Exception as e:\n        print(f\"Error loading saved model for visualization: {e}. Using current model state on CPU.\")\n        # Fallback to using the current model state moved to CPU\n        viz_model = model.cpu()\n        viz_model.eval()\n    \n    # Get random indices\n    num_available = len(test_image_paths)\n    indices = random.sample(range(num_available), min(num_samples, num_available))\n    \n    for i in indices:\n        image_path = test_image_paths[i]\n        mask_path = test_mask_paths[i]\n        \n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            true_mask_rgb = Image.open(mask_path).convert(\"RGB\")\n            # true_mask_id = rgb_mask_to_class_id_mask(true_mask_rgb)\n        except Exception as e:\n            print(f\"Error loading image/mask {i}: {e}\")\n            continue\n\n        # Preprocess image for model\n        # Feature extractor should be run on CPU for visualization consistency\n        encoding = feature_extractor(image, return_tensors=\"pt\")\n        pixel_values = encoding.pixel_values # Already on CPU\n\n        # Predict using the CPU model\n        with torch.no_grad():\n            outputs = viz_model(pixel_values=pixel_values)\n            logits = outputs.logits # Shape: (1, num_labels, H/4, W/4)\n\n        # Upsample logits to original image size\n        upsampled_logits = torch.nn.functional.interpolate(\n            logits,\n            size=image.size[::-1], # (height, width)\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        pred_mask_id = upsampled_logits.argmax(dim=1).squeeze().numpy()\n\n        # Convert predicted IDs back to RGB\n        pred_mask_rgb = np.zeros((*pred_mask_id.shape, 3), dtype=np.uint8)\n        for class_id, color in id_to_rgb.items():\n            pred_mask_rgb[pred_mask_id == class_id] = color\n            \n        # Create color legend patches\n        legend_patches = [plt.Rectangle((0,0),1,1, fc=np.array(color)/255.0) for color in id_to_rgb.values()]\n        legend_labels = [f\"{idx}: {name}\" for idx, name in id2label.items()]\n\n        # Plot\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        fig.suptitle(f\"Sample {i}: {os.path.basename(image_path)}\")\n        axes[0].imshow(image)\n        axes[0].set_title(\"Input Image\")\n        axes[0].axis('off')\n        axes[1].imshow(true_mask_rgb)\n        axes[1].set_title(\"True Mask (RGB)\")\n        axes[1].axis('off')\n        axes[2].imshow(pred_mask_rgb)\n        axes[2].set_title(\"Predicted Mask (RGB)\")\n        axes[2].axis('off')\n        \n        # Add legend to the figure\n        fig.legend(legend_patches, legend_labels, loc='lower center', ncol=len(id2label), bbox_to_anchor=(0.5, -0.05))\n        \n        plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust layout to make space for legend\n        plt.show()\n        \n    # Optional: Move the original model back to its device if needed, though usually evaluation is the last step.\n    # model.to(device)\n\n# Visualize some predictions (only on main process)\nif not _TPU_AVAILABLE or xm.is_master_ordinal():\n    visualize_predictions(num_samples=5)\nelse:\n    print(f\"Skipping visualization on TPU replica {xm.get_ordinal()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:27:55.685253Z","iopub.status.idle":"2025-04-16T19:27:55.686840Z","shell.execute_reply.started":"2025-04-16T19:27:55.685418Z","shell.execute_reply":"2025-04-16T19:27:55.685432Z"}},"outputs":[],"execution_count":null}]}