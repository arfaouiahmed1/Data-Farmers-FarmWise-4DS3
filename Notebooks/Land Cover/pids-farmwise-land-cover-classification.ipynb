{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb189ab",
   "metadata": {},
   "source": [
    "# Land Cover Classification using DeepLabV3+ (ResNet50 Backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991762c",
   "metadata": {},
   "source": [
    "## Kaggle Environment Setup\n",
    "\n",
    "Follow these steps to run this notebook on Kaggle:\n",
    "\n",
    "1.  **Dataset:** Add the `deepglobe-land-cover-classification-dataset` via \\\n",
    ". The notebook expects it at `/kaggle/input/deepglobe-land-cover-classification-dataset/`.\n",
    "2.  **Internet:** Turn **ON** Internet access in the Notebook settings (right panel -> Settings) to allow package installation.\n",
    "3.  **Accelerator:** Select a **GPU** (e.g., T4 x2, P100) for faster training.\n",
    "4.  **Resource Limits:** If you hit Kaggle's time or memory limits:\n",
    "    *   Reduce `NUM_EPOCHS` (e.g., to 5-10).\n",
    "    *   Reduce `BATCH_SIZE` (e.g., to 4 or 2).\n",
    "    *   Consider reducing `IMAGE_SIZE` (e.g., to 256 or 384), though this might affect accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc53f23",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2a745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:25.060274Z",
     "iopub.status.busy": "2025-04-16T19:27:25.059940Z",
     "iopub.status.idle": "2025-04-16T19:27:33.761569Z",
     "shell.execute_reply": "2025-04-16T19:27:33.756752Z",
     "shell.execute_reply.started": "2025-04-16T19:27:25.060247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install --upgrade pip -q\n",
    "# Install core libraries + torch_xla for TPU support + torchmetrics\n",
    "!pip install -q segmentation-models-pytorch albumentations Pillow torch torchvision torchaudio numpy matplotlib seaborn scikit-learn torchmetrics torchinfo torch_xla\n",
    "# Note: evaluate & datasets might be used for metrics/data handling, keeping them for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e15276",
   "metadata": {},
   "source": [
    "**Note on Dependency Conflicts:**\n",
    "\n",
    "You might see `ERROR: pip's dependency resolver...` messages after the installation. These often occur in pre-configured environments like Kaggle or Colab where existing packages (e.g., `gcsfs`, `bigframes`, `rich`, RAPIDS libraries) have version constraints that conflict with newly installed ones.\n",
    "\n",
    "For this specific notebook, these conflicts are generally **safe to ignore** as they typically involve packages not directly used for the core DeepLabV3+ training and prediction tasks (which rely on `torch`, `segmentation-models-pytorch`, `albumentations`, `torchmetrics` etc.). Attempting to fix them might break other functionalities of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcff2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:33.764069Z",
     "iopub.status.busy": "2025-04-16T19:27:33.763819Z",
     "iopub.status.idle": "2025-04-16T19:27:54.122385Z",
     "shell.execute_reply": "2025-04-16T19:27:54.116611Z",
     "shell.execute_reply.started": "2025-04-16T19:27:33.764045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchmetrics # Use torchmetrics\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm.notebook import tqdm # Progress bars\n",
    "import time\n",
    "from torchinfo import summary # For model summary\n",
    "import torch.nn.functional as F # For loss functions\n",
    "import gc # Garbage collection\n",
    "\n",
    "# --- TPU Setup (if applicable) ---\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    print(\"torch_xla found.\")\n",
    "    _xla_available = True\n",
    "except ImportError:\n",
    "    print(\"torch_xla not found, TPU support disabled.\")\n",
    "    _xla_available = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Segmentation Models Pytorch version: {smp.__version__}\")\n",
    "print(f\"TorchMetrics version: {torchmetrics.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- Device Selection (Prioritize TPU > CUDA > CPU) ---\n",
    "if _xla_available:\n",
    "    DEVICE = xm.xla_device()\n",
    "    print(f\"Using TPU: {xm.xla_real_devices([str(DEVICE)])[0]}\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44a034",
   "metadata": {},
   "source": [
    "## 2. Load Dataset Metadata & Initial Analysis\n",
    "\n",
    "Load the metadata file, perform initial checks, handle missing mask paths, and analyze the data split.\n",
    "\n",
    "**Important:** Assumes the dataset is at `/kaggle/input/deepglobe-land-cover-classification-dataset/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d278a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:54.125551Z",
     "iopub.status.busy": "2025-04-16T19:27:54.124836Z",
     "iopub.status.idle": "2025-04-16T19:27:54.181214Z",
     "shell.execute_reply": "2025-04-16T19:27:54.174427Z",
     "shell.execute_reply.started": "2025-04-16T19:27:54.125522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "dataset_base_dir = '/kaggle/input/deepglobe-land-cover-classification-dataset' # Original Kaggle path\n",
    "\n",
    "# Corrected: dataset_root_dir should be the base directory itself, as metadata.csv is directly inside it.\n",
    "dataset_root_dir = dataset_base_dir\n",
    "metadata_path = os.path.join(dataset_root_dir, 'metadata.csv')\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = None\n",
    "if not os.path.exists(dataset_root_dir):\n",
    "    print(f\"Error: Dataset directory not found at {dataset_root_dir}\")\n",
    "    print(\"Please ensure the DeepGlobe dataset is added via '+ Add Data' and the path is correct.\")\n",
    "elif not os.path.exists(metadata_path):\n",
    "    print(f\"Error: metadata.csv not found at {metadata_path}\")\n",
    "else:\n",
    "    print(f\"Dataset directory found. Loading metadata from: {metadata_path}\")\n",
    "    try:\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        print(f\"Metadata loaded successfully. Initial shape: {metadata_df.shape}\")\n",
    "        \n",
    "        # --- Data Cleaning and Path Handling ---\n",
    "        print(\"\\n--- Initial Data Stats ---\")\n",
    "        print(\"Value counts for 'split' column:\")\n",
    "        print(metadata_df['split'].value_counts())\n",
    "        print(\"\\nNull values per column:\")\n",
    "        print(metadata_df.isnull().sum())\n",
    "        \n",
    "        # Check for nulls specifically in path columns\n",
    "        null_sat_paths = metadata_df['sat_image_path'].isnull().sum()\n",
    "        null_mask_paths = metadata_df['mask_path'].isnull().sum()\n",
    "        print(f\"\\nNumber of null 'sat_image_path': {null_sat_paths}\")\n",
    "        print(f\"Number of null 'mask_path': {null_mask_paths}\")\n",
    "        \n",
    "        # Drop rows where mask_path is null, as they are unusable for supervised learning\n",
    "        if null_mask_paths > 0:\n",
    "            print(f\"\\nDropping {null_mask_paths} rows with missing 'mask_path'...\")\n",
    "            metadata_df.dropna(subset=['mask_path'], inplace=True)\n",
    "            print(f\"Shape after dropping null mask paths: {metadata_df.shape}\")\n",
    "        \n",
    "        # Drop rows where sat_image_path is null (if any)\n",
    "        if null_sat_paths > 0 and 'sat_image_path' in metadata_df.columns: # Check if column still exists\n",
    "             initial_rows = metadata_df.shape[0]\n",
    "             metadata_df.dropna(subset=['sat_image_path'], inplace=True)\n",
    "             rows_dropped = initial_rows - metadata_df.shape[0]\n",
    "             if rows_dropped > 0:\n",
    "                 print(f\"Dropped {rows_dropped} rows with missing 'sat_image_path'.\")\n",
    "                 print(f\"Shape after dropping null sat image paths: {metadata_df.shape}\")\n",
    "        \n",
    "        # Prepend the root directory to the relative paths\n",
    "        metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda x: os.path.join(dataset_root_dir, x) if isinstance(x, str) else x)\n",
    "        metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda x: os.path.join(dataset_root_dir, x) if isinstance(x, str) else x)\n",
    "        \n",
    "        print(\"\\n--- Data Stats After Cleaning --- \")\n",
    "        print(\"Value counts for 'split' column:\")\n",
    "        print(metadata_df['split'].value_counts())\n",
    "        print(\"\\nNull values per column (should be 0 for paths):\")\n",
    "        print(metadata_df.isnull().sum())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing metadata.csv: {e}\")\n",
    "        metadata_df = None # Ensure df is None if error occurred\n",
    "\n",
    "# Define class labels and color mapping (assuming this remains the same)\n",
    "id2label = {\n",
    "    0: 'urban_land', 1: 'agriculture_land', 2: 'rangeland',\n",
    "    3: 'forest_land', 4: 'water', 5: 'barren_land',\n",
    "    6: 'unknown' # Often used for background or ignored areas\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "NUM_CLASSES = len(id2label) # Define NUM_CLASSES based on the dataset\n",
    "class_names = list(id2label.values())\n",
    "IGNORE_INDEX = 6 # Class ID to ignore during training and evaluation\n",
    "\n",
    "# RGB color to Class ID mapping (Verify these with dataset documentation)\n",
    "rgb_to_id = {\n",
    "    (0, 255, 255): 0,  # Urban (Cyan)\n",
    "    (255, 255, 0): 1,  # Agriculture (Yellow)\n",
    "    (255, 0, 255): 2,  # Rangeland (Magenta)\n",
    "    (0, 255, 0): 3,    # Forest (Green)\n",
    "    (0, 0, 255): 4,    # Water (Blue)\n",
    "    (255, 255, 255): 5,# Barren (White)\n",
    "    (0, 0, 0): 6       # Unknown (Black)\n",
    "}\n",
    "id_to_rgb = {v: k for k, v in rgb_to_id.items()}\n",
    "\n",
    "def rgb_mask_to_class_id_mask(mask_img):\n",
    "    \"\"\"Converts an RGB mask image (PIL Image) to a 2D numpy array of class IDs.\"\"\"\n",
    "    mask_arr = np.array(mask_img.convert('RGB'))\n",
    "    class_mask = np.full(mask_arr.shape[:2], IGNORE_INDEX, dtype=np.uint8) # Default to ignore index\n",
    "    for rgb, class_id in rgb_to_id.items():\n",
    "        matches = np.all(mask_arr == np.array(rgb).reshape(1, 1, 3), axis=2)\n",
    "        class_mask[matches] = class_id\n",
    "    return class_mask\n",
    "\n",
    "# Function to load and verify data paths for a given split\n",
    "# Updated: Assumes df has already been cleaned (null paths dropped)\n",
    "def load_data_paths(df, split):\n",
    "    if df is None:\n",
    "        print(f\"Metadata DataFrame not loaded. Cannot load paths for split '{split}'.\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"\\nLoading paths for split: '{split}'\")\n",
    "    split_df = df[df['split'] == split].copy()\n",
    "\n",
    "    if 'sat_image_path' not in split_df.columns or 'mask_path' not in split_df.columns:\n",
    "        print(f\"Error: 'sat_image_path' or 'mask_path' column not found in metadata for split '{split}'.\")\n",
    "        return [], []\n",
    "\n",
    "    if split_df.empty:\n",
    "        print(f\"No entries found for split '{split}' in metadata.\")\n",
    "        return [], []\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "    skipped_missing_file = 0\n",
    "\n",
    "    # Iterate through rows, checking file existence (paths assumed valid strings now)\n",
    "    for index, row in split_df.iterrows():\n",
    "        img_path = row['sat_image_path']\n",
    "        mask_path = row['mask_path']\n",
    "\n",
    "        # Check if paths are strings before checking existence\n",
    "        if isinstance(img_path, str) and isinstance(mask_path, str):\n",
    "            if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "                valid_image_paths.append(img_path)\n",
    "                valid_mask_paths.append(mask_path)\n",
    "            else:\n",
    "                skipped_missing_file += 1\n",
    "                # Optional: Print missing paths for debugging\n",
    "                # if not os.path.exists(img_path): print(f\"Missing image: {img_path}\")\n",
    "                # if not os.path.exists(mask_path): print(f\"Missing mask: {mask_path}\")\n",
    "        else:\n",
    "             # This case should ideally not happen after dropping NaNs, but good to handle\n",
    "             skipped_missing_file += 1 \n",
    "             # print(f\"Skipping row {index} due to non-string path: Img={img_path}, Mask={mask_path}\")\n",
    "\n",
    "    if skipped_missing_file > 0:\n",
    "         print(f\"Warning: Skipped {skipped_missing_file} pairs due to missing files or invalid paths for split '{split}'.\")\n",
    "\n",
    "    if not valid_image_paths:\n",
    "        print(f\"No valid image-mask pairs found for split '{split}' after checking existence.\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"Returning {len(valid_image_paths)} valid pairs for split '{split}'.\")\n",
    "    return valid_image_paths, valid_mask_paths\n",
    "\n",
    "# Load paths for each split using the cleaned DataFrame\n",
    "train_image_paths, train_mask_paths = load_data_paths(metadata_df, 'train')\n",
    "val_image_paths, val_mask_paths = load_data_paths(metadata_df, 'valid')\n",
    "test_image_paths, test_mask_paths = load_data_paths(metadata_df, 'test')\n",
    "\n",
    "# --- Create Validation Split from Training Data if Validation is Empty ---\n",
    "if not val_image_paths and train_image_paths:\n",
    "    print(\"\\nValidation set from metadata is empty. Creating validation split from training data (20%).\")\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        # Split the training data into new train and validation sets\n",
    "        train_image_paths, val_image_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "            train_image_paths, \n",
    "            train_mask_paths, \n",
    "            test_size=0.2, # Use 20% for validation\n",
    "            random_state=42 # For reproducibility\n",
    "        )\n",
    "        print(f\"  New training set size: {len(train_image_paths)}\")\n",
    "        print(f\"  New validation set size: {len(val_image_paths)}\")\n",
    "    except ImportError:\n",
    "        print(\"  Warning: sklearn not found. Cannot create validation split automatically.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating validation split: {e}\")\n",
    "elif val_image_paths:\n",
    "    print(\"\\nUsing validation set defined in metadata.\")\n",
    "else:\n",
    "    print(\"\\nNo training data available to create a validation split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa305f",
   "metadata": {},
   "source": [
    "### Clarification on Missing Validation/Test Paths (Original Issue)\n",
    "\n",
    "Previously, warnings like `Skipped ... pairs due to invalid path types (e.g., NaN)` occurred because the `metadata.csv` file had **empty values** (read as `NaN` by pandas) in the `mask_path` column for rows marked with `split='valid'` or `split='test'`.\n",
    "\n",
    "**Resolution:** The code above now explicitly checks for and drops rows with null `mask_path` values *before* attempting to load paths for each split. This ensures that only rows with valid, non-null mask paths are considered. If the 'valid' and 'test' splits still show 0 pairs after this cleaning, it means the original `metadata.csv` truly lacked mask paths for those entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac924b",
   "metadata": {},
   "source": [
    "### 2.1 Visualize Data Split Distribution\n",
    "\n",
    "Plot the number of samples in each data split (train, validation, test) after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_split_distribution(df):\n",
    "    if df is None or 'split' not in df.columns:\n",
    "        print(\"Cannot plot distribution: DataFrame is None or 'split' column missing.\")\n",
    "        return\n",
    "        \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    split_counts = df['split'].value_counts()\n",
    "    sns.barplot(x=split_counts.index, y=split_counts.values, palette='viridis')\n",
    "    plt.title('Data Split Distribution (After Cleaning)')\n",
    "    plt.xlabel('Split')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=0)\n",
    "    # Add counts on top of bars\n",
    "    for index, value in enumerate(split_counts):\n",
    "        plt.text(index, value + 0.1, str(value), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the distribution using the cleaned metadata_df\n",
    "plot_split_distribution(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bd9f6",
   "metadata": {},
   "source": [
    "### 2.2 Data Loading Status Note\n",
    "\n",
    "Based on the cleaning and path loading steps above:\n",
    "*   If **training data paths** were found, training can proceed.\n",
    "*   If **validation data paths** were found (`val_image_paths` is not empty), validation during training and saving the best model based on validation loss/mIoU will occur.\n",
    "*   If **validation data paths** were *not* found, validation steps will be skipped, and the model will be saved after the final training epoch.\n",
    "*   If **test data paths** were found (`test_image_paths` is not empty), evaluation on the test set can be performed after training.\n",
    "*   If **test data paths** were *not* found, evaluation on the test set will be skipped.\n",
    "*   Visualization will prioritize the test set, then validation, then training set, depending on availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028fdc1",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Sample Data\n",
    "\n",
    "Display a few image-mask pairs from the training set (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf50bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_samples_from_paths(image_paths, mask_paths, num_samples=3):\n",
    "    if not image_paths or len(image_paths) < num_samples:\n",
    "        print(f\"Cannot show samples: Need at least {num_samples} valid image paths, found {len(image_paths)}.\")\n",
    "        return\n",
    "    \n",
    "    indices = random.sample(range(len(image_paths)), num_samples)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img_path = image_paths[idx]\n",
    "        mask_path = mask_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            mask = Image.open(mask_path).convert(\"RGB\") # Keep mask as RGB for visualization\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading image/mask at index {idx} ({img_path}): {e}\")\n",
    "             # Optionally add placeholder plots or skip\n",
    "             plt.subplot(num_samples, 2, 2*i + 1).set_title(f\"Error loading image {idx}\").axis('off')\n",
    "             plt.subplot(num_samples, 2, 2*i + 2).set_title(f\"Error loading mask {idx}\").axis('off')\n",
    "             continue\n",
    "                 \n",
    "        plt.subplot(num_samples, 2, 2*i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Sample {idx} - Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_samples, 2, 2*i + 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(f\"Sample {idx} - Mask (RGB)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e5bad",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4e2f8",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50f783",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e075bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a55b8c",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06ea32",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"```json\n",
    "Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8004dc",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ec984",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d9ba6",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bda99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625f6dc",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffea5a",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e682785",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e301e",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning```json\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff9ff1",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913938",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08657dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a39",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cffa9",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861d51d",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4427388",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8094f",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d32d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for```json\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcde6cc",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e415e9b",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b5780",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe017609",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba0a05",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62401fde",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72174d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83ebf2",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512 # Input size for the model. Reduce if memory errors occur.\n",
    "ENCODER = 'resnet50' # Changed to ResNet50 for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "# Get the preprocessing function specific to the ResNet50 encoder\n",
    "try:\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "except KeyError:\n",
    "    print(f\"Warning: Preprocessing function not found for {ENCODER} with {ENCODER_WEIGHTS}. Using standard ImageNet normalization.\")\n",
    "    # Define a fallback or standard normalization if needed\n",
    "    preprocessing_fn = lambda x: (x / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define Albumentations Transforms\n",
    "def get_transforms(phase, image_size, preprocessing_fn):\n",
    "    common_transforms = [A.Resize(image_size, image_size)]\n",
    "    if phase == 'train':\n",
    "        # Augmentations: Add more as needed (e.g., Rotate, ShiftScaleRotate)\n",
    "        aug_transforms = [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # Add more augmentations here if desired\n",
    "            # A.RandomBrightnessContrast(p=0.2),\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        ]\n",
    "    else:\n",
    "        aug_transforms = [] # No augmentation for validation/test\n",
    "\n",
    "    # Preprocessing (normalization) and tensor conversion\n",
    "    # Note: Apply preprocessing_fn *before* ToTensorV2 if it expects a numpy array\n",
    "    # If preprocessing_fn expects a tensor, apply it after ToTensorV2\n",
    "    final_transforms = [\n",
    "        A.Lambda(image=preprocessing_fn), # Apply model-specific preprocessing\n",
    "        ToTensorV2(), # Convert image and mask to PyTorch tensors (C, H, W)\n",
    "    ]\n",
    "\n",
    "    return A.Compose(common_transforms + aug_transforms + final_transforms)\n",
    "\n",
    "# Create transforms for each phase\n",
    "train_transforms = get_transforms('train', IMAGE_SIZE, preprocessing_fn)\n",
    "val_transforms = get_transforms('val', IMAGE_SIZE, preprocessing_fn)\n",
    "test_transforms = get_transforms('test', IMAGE_SIZE, preprocessing_fn)\n",
    "\n",
    "# Custom Dataset Class (Remains the same)\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None, rgb_to_id_func=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.rgb_to_id_func = rgb_to_id_func\n",
    "\n",
    "        if len(self.image_paths) != len(self.mask_paths):\n",
    "             raise ValueError(\"Number of images and masks must be the same.\")\n",
    "        if not self.image_paths:\n",
    "             print(\"Warning: Initializing dataset with zero image paths.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.image_paths):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "            \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask_rgb = Image.open(mask_path) # Load mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask at index {idx} ({img_path} / {mask_path}): {e}\")\n",
    "            # Return dummy data or raise error, depending on desired behavior\n",
    "            # For simplicity, returning None here, handle appropriately in DataLoader/training loop\n",
    "            # A better approach might be to filter out bad data beforehand\n",
    "            return None, None \n",
    "\n",
    "        # Convert RGB mask to class ID mask\n",
    "        if self.rgb_to_id_func:\n",
    "            mask = self.rgb_to_id_func(mask_rgb)\n",
    "        else:\n",
    "            # Fallback or error if function not provided\n",
    "            mask = np.array(mask_rgb) # Assuming mask is already single channel if no func\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Ensure mask is LongTensor for CrossEntropyLoss/FocalLoss\n",
    "        # Mask shape should be (H, W), not (1, H, W)\n",
    "        mask = mask.squeeze().long() \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create Datasets\n",
    "# Check if paths are loaded before creating datasets\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if train_image_paths:\n",
    "    train_dataset = LandCoverDataset(train_image_paths, train_mask_paths, transforms=train_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created training dataset with {len(train_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping training dataset creation: No valid training paths found.\")\n",
    "\n",
    "if val_image_paths:\n",
    "    val_dataset = LandCoverDataset(val_image_paths, val_mask_paths, transforms=val_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created validation dataset with {len(val_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping validation dataset creation: No valid validation paths found.\")\n",
    "\n",
    "if test_image_paths:\n",
    "    test_dataset = LandCoverDataset(test_image_paths, test_mask_paths, transforms=test_transforms, rgb_to_id_func=rgb_mask_to_class_id_mask)\n",
    "    print(f\"Created test dataset with {len(test_dataset)} samples.\")\n",
    "else:\n",
    "    print(\"Skipping test dataset creation: No valid test paths found.\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8 # Adjust based on GPU/TPU memory (e.g., 4, 8, 16)\n",
    "NUM_WORKERS = 2 # Adjust based on system capabilities\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created train DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping train DataLoader creation.\")\n",
    "\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created validation DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping validation DataLoader creation.\")\n",
    "\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    print(f\"Created test DataLoader with batch size {BATCH_SIZE}.\")\n",
    "else:\n",
    "    print(\"Skipping test DataLoader creation.\")\n",
    "\n",
    "# --- Verification Step: Check a batch ---\n",
    "if train_loader:\n",
    "    print(\"\\nVerifying a batch from train_loader...\")\n",
    "    try:\n",
    "        images, masks = next(iter(train_loader))\n",
    "        print(f\"Image batch shape: {images.shape}, dtype: {images.dtype}\")\n",
    "        print(f\"Mask batch shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "        print(f\"Mask unique values: {torch.unique(masks)}\")\n",
    "        \n",
    "        # Visualize one sample from the batch\n",
    "        img_sample = images[0].permute(1, 2, 0).cpu().numpy() # C, H, W -> H, W, C\n",
    "        mask_sample = masks[0].cpu().numpy()\n",
    "        \n",
    "        # Need to denormalize image for visualization if normalized\n",
    "        # This depends on the exact preprocessing_fn. Assuming standard ImageNet normalization:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_sample = std * img_sample + mean\n",
    "        img_sample = np.clip(img_sample, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.title(f\"Sample Image (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask_sample, cmap='viridis') # Use a colormap suitable for class IDs\n",
    "        plt.title(f\"Sample Mask (from DataLoader)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize samples from the training dataset paths\n",
    "print(\"Displaying samples from the training set paths:\")\n",
    "show_samples_from_paths(train_image_paths, train_mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2d10d",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Data Loading\n",
    "\n",
    "Define augmentations using Albumentations, preprocessing steps suitable for the ResNet backbone, and create a custom PyTorch Dataset and DataLoaders.\n",
    "\n",
    "**Note:** `IMAGE_SIZE` impacts memory usage and training time. Adjust if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3fcd4",
   "metadata": {},
   "source": [
    "## 4. Define Model (DeepLabV3+ with ResNet50 Backbone)\n",
    "\n",
    "Using the `segmentation-models-pytorch` library to create a DeepLabV3+ model with a pre-trained ResNet50 encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050af21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    in_channels=3,             # Input channels (RGB)\n",
    "    classes=NUM_CLASSES,       # Output classes\n",
    "    activation=None            # Output logits for combined loss\n",
    ")\n",
    "\n",
    "# Move model to the selected device\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(f\"Model: DeepLabV3+ with {ENCODER} backbone\")\n",
    "# Use torchinfo for a detailed summary\n",
    "# Check if train_loader exists to get BATCH_SIZE for summary\n",
    "if train_loader:\n",
    "    try:\n",
    "        # Provide input size including batch dimension (B, C, H, W)\n",
    "        summary(model, input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate model summary with torchinfo: {e}\")\n",
    "        # Fallback to basic print\n",
    "        # print(model)\n",
    "else:\n",
    "    print(\"Skipping model summary: train_loader not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce165ef",
   "metadata": {},
   "source": [
    "## 5. Define Loss Function, Optimizer, Scheduler, and Metrics\n",
    "\n",
    "*   **Loss:** Combined Focal Loss + Dice Loss. We'll ignore the 'unknown' class (`IGNORE_INDEX`).\n",
    "*   **Optimizer:** AdamW.\n",
    "*   **Scheduler:** ReduceLROnPlateau monitors validation loss.\n",
    "*   **Metrics:** Using `torchmetrics` for Accuracy and Mean Intersection over Union (mIoU), ignoring the specified class index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8013dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4 # Adjusted learning rate\n",
    "PATIENCE = 5 # For early stopping and scheduler\n",
    "\n",
    "# --- Loss Functions ---\n",
    "# Optional: Define class weights (adjust based on dataset analysis if needed)\n",
    "# class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1], device=DEVICE) # Example: Downweight 'unknown'\n",
    "class_weights = None # Start without weights\n",
    "\n",
    "def focal_loss(outputs, targets, alpha=None, gamma=2.0, ignore_index=None):\n",
    "    \"\"\" Focal Loss, adapted to ignore an index. \"\"\"\n",
    "    ce_loss = F.cross_entropy(outputs, targets, reduction='none', weight=alpha, ignore_index=ignore_index if ignore_index is not None else -100)\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    # Only apply focal loss calculation to valid (non-ignored) pixels\n",
    "    if ignore_index is not None:\n",
    "        valid_mask = (targets != ignore_index)\n",
    "        focal = ((1 - pt[valid_mask]) ** gamma) * ce_loss[valid_mask]\n",
    "        return focal.mean()\n",
    "    else:\n",
    "        focal = ((1 - pt) ** gamma) * ce_loss\n",
    "        return focal.mean()\n",
    "\n",
    "def dice_loss(outputs, targets, smooth=1e-6, ignore_index=None):\n",
    "    \"\"\" Dice Loss, adapted to ignore an index. \"\"\"\n",
    "    num_classes = outputs.shape[1]\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Create one-hot targets, considering ignore_index\n",
    "    if ignore_index is not None:\n",
    "        valid_mask = (targets != ignore_index).unsqueeze(1) # (B, 1, H, W)\n",
    "        targets_masked = torch.where(targets == ignore_index, num_classes, targets) # Temp replace ignore_index\n",
    "        targets_one_hot = F.one_hot(targets_masked, num_classes + 1).permute(0, 3, 1, 2).float()\n",
    "        targets_one_hot = targets_one_hot[:, :num_classes, :, :] # Remove the extra class channel\n",
    "        targets_one_hot = targets_one_hot * valid_mask # Zero out ignored pixels\n",
    "        outputs = outputs * valid_mask # Zero out predictions for ignored pixels\n",
    "    else:\n",
    "        targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Calculate intersection and union per class, averaged over batch\n",
    "    intersection = (outputs * targets_one_hot).sum(dim=(0, 2, 3)) # Sum over batch and spatial dims\n",
    "    union = outputs.sum(dim=(0, 2, 3)) + targets_one_hot.sum(dim=(0, 2, 3))\n",
    "\n",
    "    # Calculate Dice coefficient per class, then average\n",
    "    # Exclude the ignore_index class if specified (it won't be in the one-hot encoding anyway)\n",
    "    dice_per_class = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Average Dice score across relevant classes\n",
    "    # If ignore_index is used, we might want to average only over non-ignored classes\n",
    "    # For simplicity here, we average over all output classes.\n",
    "    # A more refined approach could mask the average based on class presence.\n",
    "    dice = dice_per_class.mean()\n",
    "    \n",
    "    return 1 - dice\n",
    "\n",
    "def combined_loss(outputs, targets, ignore_index=None):\n",
    "    \"\"\" Combination of Focal Loss and Dice Loss. \"\"\"\n",
    "    fl = focal_loss(outputs, targets, alpha=class_weights, gamma=2.0, ignore_index=ignore_index)\n",
    "    dl = dice_loss(outputs, targets, ignore_index=ignore_index)\n",
    "    return fl + dl\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Scheduler ---\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=PATIENCE // 2, verbose=True)\n",
    "\n",
    "# --- Metrics (using torchmetrics) ---\n",
    "# Instantiate metrics - specify task, num_classes, and ignore_index\n",
    "# Note: Metrics are often computed on CPU to save GPU memory, especially during aggregation.\n",
    "metrics_device = 'cpu' # Compute metrics on CPU\n",
    "\n",
    "train_accuracy = torchmetrics.classification.MulticlassAccuracy(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    average='micro' # Pixel accuracy across all classes\n",
    ").to(metrics_device)\n",
    "\n",
    "train_miou = torchmetrics.classification.MulticlassJaccardIndex(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    average='macro' # Mean IoU across classes\n",
    ").to(metrics_device)\n",
    "\n",
    "val_accuracy = torchmetrics.classification.MulticlassAccuracy(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    average='micro'\n",
    ").to(metrics_device)\n",
    "\n",
    "val_miou = torchmetrics.classification.MulticlassJaccardIndex(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    ignore_index=IGNORE_INDEX,\n",
    "    average='macro'\n",
    ").to(metrics_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca203ca3",
   "metadata": {},
   "source": [
    "## 6. Training and Validation Loops\n",
    "\n",
    "Includes early stopping based on validation loss and LR scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, acc_metric, miou_metric, ignore_index):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    # Wrap loader with xm.ParallelLoader for TPU training if applicable\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "        para_loader = xm.ParallelLoader(loader, [device])\n",
    "        progress_bar = tqdm(para_loader.per_device_loader(device), desc=\"Training\", leave=False, total=len(loader))\n",
    "    else:\n",
    "        progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "    for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) # Shape: (B, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) # Shape: (B, C, H, W) - Logits\n",
    "        # Handle potential dict output from some models\n",
    "        if isinstance(outputs, dict):\n",
    "            outputs = outputs.get('out', outputs) # Default to 'out' key\n",
    "\n",
    "        loss = criterion(outputs, masks, ignore_index=ignore_index)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use xm.optimizer_step for TPU\n",
    "        if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update metrics (move preds/masks to metrics_device - CPU)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            acc_metric.update(preds.to(acc_metric.device), masks.to(acc_metric.device))\n",
    "            miou_metric.update(preds.to(miou_metric.device), masks.to(miou_metric.device))\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Reduce loss across cores if using TPU\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "         epoch_loss = xm.mesh_reduce('train_loss_reduce', epoch_loss, np.mean)\n",
    "         \n",
    "    avg_loss = epoch_loss / len(loader) \n",
    "    \n",
    "    # Compute final epoch metrics\n",
    "    # For TPU, ensure compute is called on all devices or gather results\n",
    "    # torchmetrics handles synchronization with `sync_on_compute=True` (default)\n",
    "    final_acc = acc_metric.compute().item()\n",
    "    final_miou = miou_metric.compute().item()\n",
    "    \n",
    "    # Mark step for TPU execution graph\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "        xm.mark_step()\n",
    "        \n",
    "    return avg_loss, {'Accuracy': final_acc, 'mIoU': final_miou}\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, acc_metric, miou_metric, ignore_index):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    acc_metric.reset()\n",
    "    miou_metric.reset()\n",
    "\n",
    "    # Wrap loader for TPU if applicable\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "        para_loader = xm.ParallelLoader(loader, [device])\n",
    "        progress_bar = tqdm(para_loader.per_device_loader(device), desc=\"Validation\", leave=False, total=len(loader))\n",
    "    else:\n",
    "        progress_bar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, dict):\n",
    "                outputs = outputs.get('out', outputs)\n",
    "                \n",
    "            loss = criterion(outputs, masks, ignore_index=ignore_index)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Update metrics (move preds/masks to metrics_device - CPU)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            acc_metric.update(preds.to(acc_metric.device), masks.to(acc_metric.device))\n",
    "            miou_metric.update(preds.to(miou_metric.device), masks.to(miou_metric.device))\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Reduce loss across cores if using TPU\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "         epoch_loss = xm.mesh_reduce('val_loss_reduce', epoch_loss, np.mean)\n",
    "         \n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    \n",
    "    # Compute final epoch metrics\n",
    "    final_acc = acc_metric.compute().item()\n",
    "    final_miou = miou_metric.compute().item()\n",
    "\n",
    "    # Mark step for TPU execution graph\n",
    "    if _xla_available and isinstance(device, torch.device) and device.type == 'xla':\n",
    "        xm.mark_step()\n",
    "        \n",
    "    return avg_loss, {'Accuracy': final_acc, 'mIoU': final_miou}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243ed59",
   "metadata": {},
   "source": [
    "## 7. Run Training\n",
    "\n",
    "Execute the training and validation loops for a specified number of epochs, with early stopping and model saving.\n",
    "\n",
    "**Note:** Training can take a significant amount of time. Adjust `NUM_EPOCHS` based on available time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd44da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 25 # Adjust as needed (e.g., 15, 25, 50)\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "model_save_path = f\"deeplabv3plus_{ENCODER}_best_model.pth\" # Save best based on validation loss\n",
    "final_model_save_path = f\"deeplabv3plus_{ENCODER}_final_epoch_model.pth\" # Always save the final epoch model\n",
    "\n",
    "# Initialize history dictionary to store metrics\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_mIoU': [], 'val_mIoU': [],\n",
    "    'train_Accuracy': [], 'val_Accuracy': [],\n",
    "    'lr': [] # Track learning rate\n",
    "}\n",
    "\n",
    "# Check if train_loader exists before starting training\n",
    "if train_loader:\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    if not val_loader:\n",
    "        print(\"Warning: val_loader not available. Validation, LR scheduling, and early stopping will be skipped. Saving final model only.\")\n",
    "        # If no validation, 'best' path is the same as 'final'\n",
    "        model_save_path = final_model_save_path \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Clear CUDA cache (optional, might help with memory fragmentation)\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        # --- Training ---\n",
    "        train_loss, train_metrics = train_epoch(model, train_loader, combined_loss, optimizer, DEVICE, train_accuracy, train_miou, IGNORE_INDEX)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_mIoU'].append(train_metrics.get('mIoU', 0.0))\n",
    "        history['train_Accuracy'].append(train_metrics.get('Accuracy', 0.0))\n",
    "\n",
    "        # --- Validation (Optional) ---\n",
    "        val_loss = float('nan')\n",
    "        val_metrics = {'Accuracy': 0.0, 'mIoU': 0.0} # Initialize with defaults\n",
    "        if val_loader:\n",
    "            val_loss, val_metrics = validate_epoch(model, val_loader, combined_loss, DEVICE, val_accuracy, val_miou, IGNORE_INDEX)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_mIoU'].append(val_metrics.get('mIoU', 0.0))\n",
    "            history['val_Accuracy'].append(val_metrics.get('Accuracy', 0.0))\n",
    "            \n",
    "            # --- Scheduler Step ---\n",
    "            # ReduceLROnPlateau steps based on validation loss\n",
    "            scheduler.step(val_loss)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "        else:\n",
    "            # Append placeholders if validation is skipped\n",
    "            history['val_loss'].append(float('nan'))\n",
    "            history['val_mIoU'].append(float('nan'))\n",
    "            history['val_Accuracy'].append(float('nan'))\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr']) # Still record LR\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        # --- Logging (using xm.master_print on TPU) ---\n",
    "        log_fn = xm.master_print if _xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla' else print\n",
    "        \n",
    "        log_fn(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Duration: {epoch_duration:.2f}s | LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "        log_fn(f\"  Train Loss: {train_loss:.4f} | mIoU: {train_metrics.get('mIoU', 0.0):.4f} | Acc: {train_metrics.get('Accuracy', 0.0):.4f}\")\n",
    "        if val_loader:\n",
    "            log_fn(f\"  Val   Loss: {val_loss:.4f} | mIoU: {val_metrics.get('mIoU', 0.0):.4f} | Acc: {val_metrics.get('Accuracy', 0.0):.4f}\")\n",
    "        else:\n",
    "            log_fn(\"  Validation skipped.\")\n",
    "\n",
    "        # --- Save Best Model (based on validation loss) & Early Stopping ---\n",
    "        if val_loader:\n",
    "            current_val_loss = val_loss\n",
    "            # Use xm.save for TPU, ensuring it runs only on the master process\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                patience_counter = 0 # Reset patience\n",
    "                if _xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla':\n",
    "                    # Save on master process\n",
    "                    xm.save(model.state_dict(), model_save_path, master_only=True)\n",
    "                    log_fn(f\"  -> Best model saved to {model_save_path} (Val Loss: {best_val_loss:.4f})\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    log_fn(f\"  -> Best model saved to {model_save_path} (Val Loss: {best_val_loss:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                log_fn(f\"  -> Patience: {patience_counter}/{PATIENCE}\")\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    log_fn(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n",
    "                    break # Exit training loop\n",
    "\n",
    "        # --- Save Final Epoch Model ---\n",
    "        if epoch == NUM_EPOCHS - 1:\n",
    "             log_fn(f\"\\nReached final epoch {NUM_EPOCHS}.\")\n",
    "             if _xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla':\n",
    "                 xm.save(model.state_dict(), final_model_save_path, master_only=True)\n",
    "                 log_fn(f\"  -> Final epoch model saved to {final_model_save_path}\")\n",
    "             else:\n",
    "                 torch.save(model.state_dict(), final_model_save_path)\n",
    "                 log_fn(f\"  -> Final epoch model saved to {final_model_save_path}\")\n",
    "\n",
    "    total_training_time = time.time() - start_time\n",
    "    log_fn = xm.master_print if _xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla' else print\n",
    "    log_fn(f\"\\nTraining finished in {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "    if val_loader:\n",
    "        log_fn(f\"Best Validation Loss achieved: {best_val_loss:.4f} (saved to {model_save_path})\")\n",
    "    log_fn(f\"Model after final epoch saved to {final_model_save_path}\")\n",
    "else:\n",
    "    print(\"Cannot start training: train_loader is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba6f97",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    has_val_data = 'val_loss' in history and any(not np.isnan(x) for x in history['val_loss'])\n",
    "\n",
    "    # Determine number of plots needed (Loss, mIoU, Accuracy, LR)\n",
    "    metrics_to_plot = ['Loss', 'mIoU', 'Accuracy']\n",
    "    num_plots = len(metrics_to_plot) + 1 # Add one for LR\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols # Calculate rows needed\n",
    "\n",
    "    plt.figure(figsize=(14, 5 * num_rows))\n",
    "\n",
    "    # Plot Loss, mIoU, Accuracy\n",
    "    for i, metric_name in enumerate(metrics_to_plot):\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        \n",
    "        # Construct history keys (e.g., 'train_loss', 'val_mIoU')\n",
    "        train_key = f\"train_{metric_name.lower().replace('miou', 'mIoU')}\"\n",
    "        val_key = f\"val_{metric_name.lower().replace('miou', 'mIoU')}\"\n",
    "        \n",
    "        if train_key in history and history[train_key]:\n",
    "            plt.plot(epochs, history[train_key], 'bo-', label=f'Training {metric_name}')\n",
    "        if has_val_data and val_key in history and history[val_key]:\n",
    "            # Ensure validation data aligns with training epochs plotted\n",
    "            val_epochs = range(1, len(history[val_key]) + 1)\n",
    "            plt.plot(val_epochs, history[val_key], 'ro-', label=f'Validation {metric_name}')\n",
    "            \n",
    "        plt.title(f'Training and Validation {metric_name}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_name)\n",
    "        # Only show legend if there's data to plot\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        if handles:\n",
    "            plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # Plot Learning Rate\n",
    "    plt.subplot(num_rows, num_cols, len(metrics_to_plot) + 1)\n",
    "    if 'lr' in history and history['lr']:\n",
    "        lr_epochs = range(1, len(history['lr']) + 1)\n",
    "        plt.plot(lr_epochs, history['lr'], 'go-', label='Learning Rate')\n",
    "        plt.title('Learning Rate over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.title('Learning Rate (No Data)')\n",
    "        plt.text(0.5, 0.5, 'No LR data recorded', ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check if history has data before plotting\n",
    "if history['train_loss']:\n",
    "    plot_history(history)\n",
    "else:\n",
    "    print(\"No training history to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46805dc0",
   "metadata": {},
   "source": [
    "## 9. Evaluation on Test Set\n",
    "\n",
    "Load the best saved model (based on validation loss) and evaluate its performance on the unseen test set using Accuracy and mIoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which model to load for evaluation (prefer best based on val loss)\n",
    "eval_model_path = None\n",
    "log_fn = xm.master_print if _xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla' else print\n",
    "\n",
    "# Check if the 'best' model file exists (saved based on val loss)\n",
    "if os.path.exists(model_save_path):\n",
    "    eval_model_path = model_save_path\n",
    "    log_fn(f\"\\nAttempting to load best model (lowest val loss) from: {eval_model_path}\")\n",
    "elif os.path.exists(final_model_save_path): # Fallback to the final epoch model\n",
    "    eval_model_path = final_model_save_path\n",
    "    log_fn(f\"\\nBest model not found. Attempting to load final epoch model from: {eval_model_path}\")\n",
    "else:\n",
    "    log_fn(f\"\\nNo model file found at {model_save_path} or {final_model_save_path}. Cannot perform evaluation.\")\n",
    "\n",
    "model_loaded = False\n",
    "if eval_model_path:\n",
    "    # Re-initialize model architecture\n",
    "    model_eval = smp.DeepLabV3Plus( # Use a different variable name\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights=None, # No need to load pretrained weights again\n",
    "        in_channels=3,\n",
    "        classes=NUM_CLASSES,\n",
    "        activation=None\n",
    "    )\n",
    "    try:\n",
    "        # Load state dict - use map_location to load onto the correct device (CPU first if loading TPU model on non-TPU)\n",
    "        map_loc = 'cpu' if _xla_available else DEVICE # Load to CPU if XLA exists, otherwise current device\n",
    "        model_eval.load_state_dict(torch.load(eval_model_path, map_location=map_loc))\n",
    "        model_eval.to(DEVICE) # Move model to the target device (TPU/CUDA/CPU)\n",
    "        model_eval.eval() # Set to evaluation mode\n",
    "        log_fn(\"Model loaded successfully for evaluation.\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        log_fn(f\"Error loading model state dict from {eval_model_path}: {e}\")\n",
    "        model_loaded = False\n",
    "\n",
    "# Evaluate on the test set (if available and model loaded)\n",
    "if model_loaded and test_loader:\n",
    "    log_fn(\"\\nEvaluating on the test set...\")\n",
    "    # Instantiate test metrics\n",
    "    test_accuracy = torchmetrics.classification.MulticlassAccuracy(\n",
    "        num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX, average='micro'\n",
    "    ).to(metrics_device)\n",
    "    test_miou = torchmetrics.classification.MulticlassJaccardIndex(\n",
    "        num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX, average='macro'\n",
    "    ).to(metrics_device)\n",
    "    \n",
    "    # Use validate_epoch function for evaluation logic\n",
    "    test_loss, test_metrics = validate_epoch(model_eval, test_loader, combined_loss, DEVICE, test_accuracy, test_miou, IGNORE_INDEX)\n",
    "    \n",
    "    log_fn(f\"\\n--- Test Set Performance ---\")\n",
    "    log_fn(f\"  Test Loss:      {test_loss:.4f}\")\n",
    "    log_fn(f\"  Test Accuracy:  {test_metrics.get('Accuracy', 0.0):.4f}\")\n",
    "    log_fn(f\"  Test mIoU:      {test_metrics.get('mIoU', 0.0):.4f}\")\n",
    "    \n",
    "    # --- Detailed Confusion Matrix (Run on master/single device) ---\n",
    "    # Ensure this part runs only once, e.g., using xm.is_master_ordinal() for TPU\n",
    "    run_cm = not (_xla_available and isinstance(DEVICE, torch.device) and DEVICE.type == 'xla') or xm.is_master_ordinal()\n",
    "    if run_cm:\n",
    "        log_fn(\"\\nCalculating Confusion Matrix on Test Set (master/single device)...\")\n",
    "        all_preds = []\n",
    "        all_masks = []\n",
    "        # Use the standard test_loader for CM calculation on CPU/single GPU\n",
    "        cm_loader = test_loader \n",
    "        cm_device = DEVICE # Use the main device unless forcing CPU\n",
    "        # If on TPU, might need to run CM calculation on CPU with a CPU model copy\n",
    "        # cm_device = torch.device('cpu')\n",
    "        # model_eval_cpu = model_eval.to(cm_device) # Create CPU copy if needed\n",
    "        \n",
    "        model_eval.eval() # Ensure model is in eval mode\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(cm_loader, desc=\"Test Prediction for CM\"):\n",
    "                images = images.to(cm_device)\n",
    "                masks = masks.to(cm_device) # (B, H, W)\n",
    "                outputs = model_eval(images) # Use model_eval (on cm_device)\n",
    "                if isinstance(outputs, dict):\n",
    "                    outputs = outputs.get('out', outputs)\n",
    "                preds = torch.argmax(outputs, dim=1) # (B, H, W)\n",
    "                \n",
    "                # Flatten masks and predictions, ignore IGNORE_INDEX\n",
    "                mask_flat = masks.view(-1).cpu().numpy()\n",
    "                pred_flat = preds.view(-1).cpu().numpy()\n",
    "                \n",
    "                # Filter out ignored index pixels\n",
    "                valid_indices = mask_flat != IGNORE_INDEX\n",
    "                all_masks.extend(mask_flat[valid_indices])\n",
    "                all_preds.extend(pred_flat[valid_indices])\n",
    "        \n",
    "        if all_masks:\n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(all_masks, all_preds, labels=list(range(NUM_CLASSES - 1))) # Exclude ignore index from labels\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                        xticklabels=class_names[:-1], yticklabels=class_names[:-1]) # Exclude ignore index name\n",
    "            plt.xlabel('Predicted Labels')\n",
    "            plt.ylabel('True Labels')\n",
    "            plt.title('Confusion Matrix (Test Set - Excluding Unknown Class)')\n",
    "            plt.show()\n",
    "        else:\n",
    "            log_fn(\"Could not calculate confusion matrix: No valid pixels found after filtering ignore index.\")\n",
    "    else:\n",
    "         log_fn(\"Skipping Confusion Matrix calculation on non-master TPU core.\")\n",
    "        \n",
    "elif model_loaded and not test_loader:\n",
    "    log_fn(\"\\nSkipping test set evaluation: test_loader not available (likely no test data found).\")\n",
    "elif not model_loaded:\n",
    "    log_fn(\"\\nSkipping test set evaluation: Model could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f379832",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions\n",
    "\n",
    "Display some predictions from the test set alongside the original images and ground truth masks, using the loaded evaluation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2421e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def id_to_rgb_mask(mask_id, id_to_rgb_map):\n",
    "    \"\"\"Converts a class ID mask (H, W) to an RGB mask (H, W, 3).\"\"\"\n",
    "    rgb_mask = np.zeros((*mask_id.shape, 3), dtype=np.uint8)\n",
    "    for class_id, color in id_to_rgb_map.items():\n",
    "        rgb_mask[mask_id == class_id] = color\n",
    "    return rgb_mask\n",
    "\n",
    "def visualize_predictions(model, loader, device, num_samples=5, id_to_rgb_map=None):\n",
    "    if not loader:\n",
    "        print(\"Cannot visualize predictions: Loader not available.\")\n",
    "        return\n",
    "        \n",
    "    if not hasattr(model, 'eval'): # Basic check if model is a PyTorch model\n",
    "         print(\"Cannot visualize predictions: Invalid model object provided.\")\n",
    "         return\n",
    "         \n",
    "    model.eval()\n",
    "    samples_shown = 0\n",
    "    \n",
    "    # Create a colormap and norm for the ID masks if id_to_rgb_map is not provided\n",
    "    cmap = None\n",
    "    norm = None\n",
    "    if id_to_rgb_map is None:\n",
    "        # Check if matplotlib version supports get_cmap\n",
    "        try:\n",
    "            cmap = plt.colormaps.get_cmap('viridis', num_labels)\n",
    "        except AttributeError:\n",
    "            cmap = plt.cm.get_cmap('viridis', num_labels) # Fallback for older versions\n",
    "        norm = mcolors.Normalize(vmin=0, vmax=num_labels-1)\n",
    "    \n",
    "    plt.figure(figsize=(18, 6 * num_samples)) # Adjusted figure size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            data_iterator = iter(loader)\n",
    "            while samples_shown < num_samples:\n",
    "                try:\n",
    "                    images, masks_true_id = next(data_iterator)\n",
    "                except StopIteration:\n",
    "                    print(\"\\nReached end of loader before showing desired number of samples.\")\n",
    "                    break # Exit outer loop if loader is exhausted\n",
    "                    \n",
    "                images = images.to(device)\n",
    "                # masks_true_id are already (B, H, W) LongTensor\n",
    "\n",
    "                outputs = model(images) # (B, C, H, W)\n",
    "                masks_pred_id = torch.argmax(outputs, dim=1) # (B, H, W)\n",
    "\n",
    "                for i in range(images.shape[0]):\n",
    "                    if samples_shown >= num_samples:\n",
    "                        break # Exit inner loop\n",
    "\n",
    "                    img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "                    true_mask_id = masks_true_id[i].cpu().numpy()\n",
    "                    pred_mask_id = masks_pred_id[i].cpu().numpy()\n",
    "\n",
    "                    # Denormalize image for display (assuming standard ImageNet normalization)\n",
    "                    mean = np.array([0.485, 0.456, 0.406])\n",
    "                    std = np.array([0.229, 0.224, 0.225])\n",
    "                    img = std * img + mean\n",
    "                    img = np.clip(img, 0, 1)\n",
    "\n",
    "                    # Convert ID masks to RGB for visualization\n",
    "                    if id_to_rgb_map:\n",
    "                        true_mask_rgb = id_to_rgb_mask(true_mask_id, id_to_rgb_map)\n",
    "                        pred_mask_rgb = id_to_rgb_mask(pred_mask_id, id_to_rgb_map)\n",
    "                    elif cmap and norm:\n",
    "                        # Use colormap if no RGB map provided\n",
    "                        true_mask_rgb = cmap(norm(true_mask_id))[:, :, :3] # Get RGB from colormap\n",
    "                        pred_mask_rgb = cmap(norm(pred_mask_id))[:, :, :3]\n",
    "                        # Convert to uint8 if needed, though imshow handles float [0,1]\n",
    "                        # true_mask_rgb = (true_mask_rgb * 255).astype(np.uint8)\n",
    "                        # pred_mask_rgb = (pred_mask_rgb * 255).astype(np.uint8)\n",
    "                    else: # Fallback if no map and cmap failed\n",
    "                         true_mask_rgb = true_mask_id # Show raw IDs\n",
    "                         pred_mask_rgb = pred_mask_id\n",
    "\n",
    "                    plt.subplot(num_samples, 3, samples_shown * 3 + 1)\n",
    "                    plt.imshow(img)\n",
    "                    plt.title(f\"Sample {samples_shown+1} - Image\")\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.subplot(num_samples, 3, samples_shown * 3 + 2)\n",
    "                    plt.imshow(true_mask_rgb)\n",
    "                    plt.title(f\"Sample {samples_shown+1} - True Mask\")\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.subplot(num_samples, 3, samples_shown * 3 + 3)\n",
    "                    plt.imshow(pred_mask_rgb)\n",
    "                    plt.title(f\"Sample {samples_shown+1} - Predicted Mask\")\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    samples_shown += 1\n",
    "                # End of inner loop (batch processing)\n",
    "            # End of outer loop (sample count check)\n",
    "        except Exception as e:\n",
    "             print(f\"An error occurred during visualization: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc() # Print detailed traceback\n",
    "            \n",
    "    if samples_shown > 0:\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.show()\n",
    "    elif loader```json\n",
    "\n",
    "# Determine which model and loader to use for visualization\n",
    "vis_model = None\n",
    "vis_loader = None\n",
    "vis_loader_name = \"\"\n",
    "\n",
    "# Prioritize using the evaluated model ('model_eval') if it was loaded\n",
    "if 'model_eval' in locals() and model_loaded:\n",
    "    vis_model = model_eval\n",
    "    # Prioritize loader: Test > Validation > Train\n",
    "    if test_loader:\n",
    "        vis_loader = test_loader\n",
    "        vis_loader_name = \"test set\"\n",
    "    elif val_loader:\n",
    "        vis_loader = val_loader\n",
    "        vis_loader_name = \"validation set\"\n",
    "    elif train_loader:\n",
    "        vis_loader = train_loader\n",
    "        vis_loader_name = \"training set\"\n",
    "# Fallback to the model currently in memory ('model') if eval failed/skipped\n",
    "elif 'model' in locals():\n",
    "    vis_model = model\n",
    "    print(\"\\nWarning: Using model from training state for visualization (evaluation model not loaded/available).\")\n",
    "    # Prioritize loader: Test > Validation > Train\n",
    "    if test_loader:\n",
    "        vis_loader = test_loader\n",
    "        vis_loader_name = \"test set\"\n",
    "    elif val_loader:\n",
    "        vis_loader = val_loader\n",
    "        vis_loader_name = \"validation set\"\n",
    "    elif train_loader:\n",
    "        vis_loader = train_loader\n",
    "        vis_loader_name = \"training set\"\n",
    "\n",
    "# Visualize predictions if a model and loader were selected\n",
    "if vis_model and vis_loader:\n",
    "    print(f\"\\nVisualizing predictions on {vis_loader_name}...\")\n",
    "    visualize_predictions(vis_model, vis_loader, DEVICE, num_samples=5, id_to_rgb_map=id_to_rgb)\n",
    "else:\n",
    "    print(\"\\nCannot visualize predictions: Suitable model or data loader not available.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 966962,
     "sourceId": 1635643,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31013,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
