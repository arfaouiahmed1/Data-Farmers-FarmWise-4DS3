{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb189ab",
   "metadata": {},
   "source": [
    "# Land Cover Classification using Vision Transformers (SegFormer)\n",
    "\n",
    "Optimized for NVIDIA P100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc53f23",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install and import necessary libraries.\n",
    "\n",
    "**Note:** This notebook is optimized for NVIDIA P100 GPU with 16GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb2a745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:25.060274Z",
     "iopub.status.busy": "2025-04-16T19:27:25.059940Z",
     "iopub.status.idle": "2025-04-16T19:27:33.761569Z",
     "shell.execute_reply": "2025-04-16T19:27:33.756752Z",
     "shell.execute_reply.started": "2025-04-16T19:27:25.060247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [evaluate]8/9\u001b[0m [evaluate]solver-cu12]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'gputil' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gputil'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pytables (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pytables\u001b[0m\u001b[31m\n",
      "\u001b[0mCloning into 'apex'...\n",
      "remote: Enumerating objects: 12081, done.\u001b[K\n",
      "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 12081 (delta 16), reused 9 (delta 9), pack-reused 12047 (from 2)\u001b[K\n",
      "Receiving objects: 100% (12081/12081), 15.75 MiB | 19.16 MiB/s, done.\n",
      "Resolving deltas: 100% (8385/8385), done.\n",
      "Submodule 'apex/contrib/csrc/cudnn-frontend' (https://github.com/NVIDIA/cudnn-frontend.git) registered for path 'apex/contrib/csrc/cudnn-frontend'\n",
      "Submodule 'apex/contrib/csrc/multihead_attn/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'apex/contrib/csrc/multihead_attn/cutlass'\n",
      "Cloning into '/kaggle/working/apex/apex/contrib/csrc/cudnn-frontend'...\n",
      "remote: Enumerating objects: 8412, done.        \n",
      "remote: Counting objects: 100% (210/210), done.        \n",
      "remote: Compressing objects: 100% (77/77), done.        \n",
      "remote: Total 8412 (delta 175), reused 133 (delta 133), pack-reused 8202 (from 2)        \n",
      "Receiving objects: 100% (8412/8412), 36.39 MiB | 18.58 MiB/s, done.\n",
      "Resolving deltas: 100% (4882/4882), done.\n",
      "Cloning into '/kaggle/working/apex/apex/contrib/csrc/multihead_attn/cutlass'...\n",
      "remote: Enumerating objects: 33203, done.        \n",
      "remote: Counting objects: 100% (103/103), done.        \n",
      "remote: Compressing objects: 100% (88/88), done.        \n",
      "remote: Total 33203 (delta 50), reused 15 (delta 15), pack-reused 33100 (from 3)        \n",
      "Receiving objects: 100% (33203/33203), 50.04 MiB | 19.18 MiB/s, done.\n",
      "Resolving deltas: 100% (25054/25054), done.\n",
      "Submodule path 'apex/contrib/csrc/cudnn-frontend': checked out '171a7a986f7fbd9ed71bd0cf3c7ad4f55843d6b3'\n",
      "Submodule path 'apex/contrib/csrc/multihead_attn/cutlass': checked out '66d9cddc832c1cdc2b30a8755274f7f74640cfe6'\n",
      "Using pip 25.1 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
      "\u001b[33mDEPRECATION: --build-option and --global-option are deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11859\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option. \u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /kaggle/working/apex\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.5.1+cu124\n",
      "\n",
      "\n",
      "\u001b[?25hdone\n",
      "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.11/dist-packages (from apex==0.1) (24.2)\n",
      "Building wheels for collected packages: apex\n",
      "\u001b[33m  WARNING: Ignoring --global-option when building apex using PEP 517\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for apex (pyproject.toml) ... \u001b[?25l  Running command Building wheel for apex (pyproject.toml)\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.5.1+cu124\n",
      "\n",
      "\n",
      "\u001b[?25hdone\n",
      "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=432395 sha256=2a924b8351a8a42cf4719241b7c4636d48cdd01fb0e8e0f1bbe3c49678514c6a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yhrq52cw/wheels/87/23/02/434387616bb8697ec957db96af9df5ee19ea53d86807cdfb95\n",
      "Successfully built apex\n",
      "Installing collected packages: apex\n",
      "Successfully installed apex-0.1\n"
     ]
    }
   ],
   "source": [
    "# Install base libraries with optimizations for full system utilization\n",
    "!pip install --upgrade pip\n",
    "!pip install -q transformers datasets evaluate accelerate Pillow torch torchvision torchaudio numpy matplotlib seaborn scikit-learn\n",
    "# Install additional performance optimization libraries\n",
    "!pip install -q ninja psutil gputil\n",
    "!pip install -q pyarrow fastparquet # For faster data serialization\n",
    "!pip install -q pytables h5py # For efficient data storage\n",
    "!pip install -q albumentations # For optimized image augmentations\n",
    "\n",
    "# Install NVIDIA Apex for mixed precision training optimization\n",
    "!pip install -q ninja\n",
    "!git clone --recursive https://github.com/NVIDIA/apex\n",
    "!cd apex && pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13c56c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured for maximum performance\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for maximum performance\n",
    "import os\n",
    "\n",
    "# Maximum thread optimization\n",
    "os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())\n",
    "os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = str(os.cpu_count())\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(os.cpu_count())\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = str(os.cpu_count()) \n",
    "\n",
    "# GPU optimization\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'false'  # Allocate all memory immediately\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "# Enable TF32 on Ampere and newer GPUs\n",
    "os.environ['TORCH_ALLOW_TF32_CUBLAS_OVERRIDE'] = '1'\n",
    "os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'\n",
    "\n",
    "print(\"Environment configured for maximum performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bcff2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:33.764069Z",
     "iopub.status.busy": "2025-04-16T19:27:33.763819Z",
     "iopub.status.idle": "2025-04-16T19:27:54.122385Z",
     "shell.execute_reply": "2025-04-16T19:27:54.116611Z",
     "shell.execute_reply.started": "2025-04-16T19:27:33.764045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:15:15.768122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745759715.946254      71 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745759715.996995      71 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System resources: 4 CPU cores, 31.4GB RAM\n",
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Number of GPUs: 1\n",
      "GPU Memory: 17.06 GB\n",
      "CUDA Capability: (6, 0)\n",
      "GPU 0: Memory reserved: 0.00GB\n",
      "GPU 0: Memory allocated: 0.00GB\n",
      "GPU 0: Name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict, Image as HFImage\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from huggingface_hub import notebook_login\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Maximum GPU optimization\n",
    "torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster math\n",
    "torch.backends.cudnn.deterministic = False  # Non-deterministic mode for better performance\n",
    "torch.backends.cudnn.enabled = True  # Ensure cuDNN is used\n",
    "\n",
    "# Set numpy to use all cores\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# Garbage collect to free memory\n",
    "gc.collect()\n",
    "\n",
    "# Check system resources\n",
    "cpu_count = os.cpu_count()\n",
    "ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "\n",
    "print(f\"System resources: {cpu_count} CPU cores, {ram_gb:.1f}GB RAM\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    # Display detailed GPU information\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability()}\")\n",
    "    \n",
    "    # Configure PyTorch to use all available GPU memory\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)\n",
    "        torch.cuda.empty_cache()\n",
    "        if hasattr(torch.cuda, 'memory_reserved'):\n",
    "            print(f\"GPU {i}: Memory reserved: {torch.cuda.memory_reserved(i) / 1e9:.2f}GB\")\n",
    "            print(f\"GPU {i}: Memory allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f}GB\")\n",
    "        if hasattr(torch.cuda, 'get_device_properties'):\n",
    "            print(f\"GPU {i}: Name: {torch.cuda.get_device_properties(i).name}\")\n",
    "else:\n",
    "    print(\"Warning: No GPU detected. Running on CPU will be significantly slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44a034",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the DeepGlobe Land Cover Classification dataset. \n",
    "You might need to download it from Kaggle first: https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\n",
    "\n",
    "**Important:** Ensure the dataset is placed in the Kaggle input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309d278a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:54.125551Z",
     "iopub.status.busy": "2025-04-16T19:27:54.124836Z",
     "iopub.status.idle": "2025-04-16T19:27:54.181214Z",
     "shell.execute_reply": "2025-04-16T19:27:54.174427Z",
     "shell.execute_reply.started": "2025-04-16T19:27:54.125522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset base directory found at: /kaggle/input/deepglobe-land-cover-classification-dataset\n",
      "No metadata.csv found. Using direct directory structure.\n",
      "Loading train data from: /kaggle/input/deepglobe-land-cover-classification-dataset/train\n",
      "Found 1606 potential image files in /kaggle/input/deepglobe-land-cover-classification-dataset/train\n",
      "Found 1606 image-mask pairs.\n",
      "Loading valid data from: /kaggle/input/deepglobe-land-cover-classification-dataset/valid\n",
      "Found 171 potential image files in /kaggle/input/deepglobe-land-cover-classification-dataset/valid\n",
      "Found 171 image-mask pairs.\n",
      "Loading test data from: /kaggle/input/deepglobe-land-cover-classification-dataset/test\n",
      "Found 172 potential image files in /kaggle/input/deepglobe-land-cover-classification-dataset/test\n",
      "Found 172 image-mask pairs.\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 1606\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 171\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 172\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define the standard Kaggle input directory\n",
    "dataset_base_dir = '/kaggle/input/deepglobe-land-cover-classification-dataset'\n",
    "# Check for different directory structures\n",
    "deepglobe_subdir = os.path.join(dataset_base_dir, 'deepglobe')\n",
    "metadata_path = os.path.join(deepglobe_subdir, 'metadata.csv')\n",
    "\n",
    "# Check if the dataset path exists in either structure\n",
    "if os.path.exists(dataset_base_dir):\n",
    "    print(f\"Dataset base directory found at: {dataset_base_dir}\")\n",
    "    \n",
    "    # Check if we have the expected subdirectory structure with metadata\n",
    "    if os.path.exists(deepglobe_subdir) and os.path.exists(metadata_path):\n",
    "        print(f\"Using metadata-based loading from: {metadata_path}\")\n",
    "        metadata_df = pd.read_csv(metadata_path)\n",
    "        # Prepend the root directory to the paths in the CSV\n",
    "        metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda x: os.path.join(deepglobe_subdir, x))\n",
    "        metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda x: os.path.join(deepglobe_subdir, x))\n",
    "        \n",
    "        # Define function to load data paths from metadata\n",
    "        def load_data_paths(df, split):\n",
    "            split_df = df[df['split'] == split]\n",
    "            image_paths = split_df['sat_image_path'].tolist()\n",
    "            mask_paths = split_df['mask_path'].tolist()\n",
    "            # Verify files exist\n",
    "            image_paths = [p for p in image_paths if os.path.exists(p)]\n",
    "            mask_paths = [p for p in mask_paths if os.path.exists(p)]\n",
    "            print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks for split '{split}'.\")\n",
    "            return image_paths, mask_paths\n",
    "    else:\n",
    "        # No metadata.csv found - try direct directory structure\n",
    "        print(\"No metadata.csv found. Using direct directory structure.\")\n",
    "        metadata_df = None\n",
    "        \n",
    "        # Function to find image-mask pairs in directories\n",
    "        def find_image_mask_pairs(image_dir, mask_dir=None):\n",
    "            \"\"\"Find matching images and masks in the given directories.\"\"\"\n",
    "            if not os.path.exists(image_dir):\n",
    "                print(f\"Warning: Image directory {image_dir} not found.\")\n",
    "                return [], []\n",
    "                \n",
    "            # If mask_dir is not specified, try to infer it\n",
    "            if mask_dir is None:\n",
    "                # Check common mask directory naming patterns\n",
    "                possible_mask_dirs = [\n",
    "                    image_dir.replace('sat', 'mask').replace('image', 'mask'),\n",
    "                    os.path.join(os.path.dirname(image_dir), 'mask'),\n",
    "                    os.path.join(os.path.dirname(image_dir), 'masks'),\n",
    "                    os.path.join(os.path.dirname(image_dir), 'label'),\n",
    "                    os.path.join(os.path.dirname(image_dir), 'labels')\n",
    "                ]\n",
    "                \n",
    "                # Try standard pattern where masks are in same directory with different extension\n",
    "                mask_dir = image_dir  # Default: assume masks are in same directory\n",
    "                \n",
    "                # Check if any of the possible mask directories exist\n",
    "                for dir_path in possible_mask_dirs:\n",
    "                    if os.path.exists(dir_path) and dir_path != image_dir:\n",
    "                        mask_dir = dir_path\n",
    "                        print(f\"Found mask directory at: {mask_dir}\")\n",
    "                        break\n",
    "            \n",
    "            # Get image files with common extensions\n",
    "            image_files = []\n",
    "            for ext in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]:\n",
    "                image_files.extend(glob.glob(os.path.join(image_dir, f\"*{ext}\")))\n",
    "            \n",
    "            if not image_files:\n",
    "                print(f\"No image files found in {image_dir}\")\n",
    "                return [], []\n",
    "                \n",
    "            print(f\"Found {len(image_files)} potential image files in {image_dir}\")\n",
    "            \n",
    "            # Find matching mask files\n",
    "            image_paths = []\n",
    "            mask_paths = []\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                img_name = os.path.basename(img_path)\n",
    "                img_stem = os.path.splitext(img_name)[0]\n",
    "                \n",
    "                # Try different possible mask naming patterns\n",
    "                mask_patterns = [\n",
    "                    os.path.join(mask_dir, f\"{img_stem}*.png\"),\n",
    "                    os.path.join(mask_dir, f\"{img_stem}*.jpg\"),\n",
    "                    os.path.join(mask_dir, f\"{img_stem.replace('sat', 'mask')}*.png\"),\n",
    "                    os.path.join(mask_dir, f\"{img_stem}_mask.*\"),\n",
    "                    os.path.join(mask_dir, f\"{img_stem}_label.*\")\n",
    "                ]\n",
    "                \n",
    "                # Try to find matching mask\n",
    "                mask_found = False\n",
    "                for pattern in mask_patterns:\n",
    "                    matching_masks = glob.glob(pattern)\n",
    "                    if matching_masks:\n",
    "                        mask_path = matching_masks[0]  # Take the first match\n",
    "                        image_paths.append(img_path)\n",
    "                        mask_paths.append(mask_path)\n",
    "                        mask_found = True\n",
    "                        break\n",
    "                        \n",
    "            print(f\"Found {len(image_paths)} image-mask pairs.\")\n",
    "            return image_paths, mask_paths\n",
    "\n",
    "        # Function to load data paths directly from directories\n",
    "        def load_data_paths(df, split):\n",
    "            \"\"\"Load data paths for the given split using directory structure.\"\"\"\n",
    "            # Check common directory naming patterns for this split\n",
    "            possible_dirs = [\n",
    "                os.path.join(dataset_base_dir, split),\n",
    "                os.path.join(dataset_base_dir, f\"{split}_set\")\n",
    "            ]\n",
    "            \n",
    "            split_dir = None\n",
    "            for dir_path in possible_dirs:\n",
    "                if os.path.exists(dir_path):\n",
    "                    split_dir = dir_path\n",
    "                    break\n",
    "                    \n",
    "            if split_dir is None:\n",
    "                print(f\"No directory found for split '{split}'\")\n",
    "                return [], []\n",
    "                \n",
    "            print(f\"Loading {split} data from: {split_dir}\")\n",
    "            return find_image_mask_pairs(split_dir)\n",
    "else:\n",
    "    print(f\"Error: Dataset directory not found at {dataset_base_dir}\")\n",
    "    print(\"Please ensure the DeepGlobe dataset is correctly placed in the Kaggle input directory.\")\n",
    "    metadata_df = pd.DataFrame(columns=['image_id', 'split', 'sat_image_path', 'mask_path']) # Dummy df\n",
    "\n",
    "# Import glob for file pattern matching\n",
    "import glob\n",
    "\n",
    "# Define class names and their corresponding IDs\n",
    "id2label = {\n",
    "    0: 'urban_land',\n",
    "    1: 'agriculture_land',\n",
    "    2: 'rangeland',\n",
    "    3: 'forest_land',\n",
    "    4: 'water',\n",
    "    5: 'barren_land',\n",
    "    6: 'unknown'\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "class_names = list(id2label.values())\n",
    "\n",
    "# Load data for each split\n",
    "train_image_paths, train_mask_paths = load_data_paths(metadata_df, 'train')\n",
    "val_image_paths, val_mask_paths = load_data_paths(metadata_df, 'valid')\n",
    "test_image_paths, test_mask_paths = load_data_paths(metadata_df, 'test')\n",
    "\n",
    "# If no validation set found, use a portion of train or test as validation\n",
    "if not val_image_paths and train_image_paths:\n",
    "    # Use 15% of train data as validation\n",
    "    val_size = max(1, int(len(train_image_paths) * 0.15))\n",
    "    val_indices = random.sample(range(len(train_image_paths)), val_size)\n",
    "    val_image_paths = [train_image_paths[i] for i in val_indices]\n",
    "    val_mask_paths = [train_mask_paths[i] for i in val_indices]\n",
    "    \n",
    "    # Remove validation examples from train set\n",
    "    val_set = set(val_image_paths)\n",
    "    train_image_paths = [p for p in train_image_paths if p not in val_set]\n",
    "    train_mask_paths = [p for i, p in enumerate(train_mask_paths) if train_image_paths[i] not in val_set]\n",
    "    \n",
    "    print(f\"Created validation set with {len(val_image_paths)} samples from train data.\")\n",
    "    print(f\"Updated train set has {len(train_image_paths)} samples.\")\n",
    "\n",
    "# Create Hugging Face Datasets\n",
    "def create_hf_dataset(image_paths, mask_paths):\n",
    "    if not image_paths or not mask_paths or len(image_paths) != len(mask_paths):\n",
    "        print(f\"Warning: Mismatch or empty paths. Creating empty dataset.\")\n",
    "        return Dataset.from_dict({'image': [], 'label': []}).cast_column('image', HFImage()).cast_column('label', HFImage())\n",
    "    dataset = Dataset.from_dict({'image': image_paths, 'label': mask_paths})\n",
    "    # Casting ensures the columns are treated as images\n",
    "    dataset = dataset.cast_column('image', HFImage())\n",
    "    dataset = dataset.cast_column('label', HFImage())\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_hf_dataset(train_image_paths, train_mask_paths)\n",
    "val_dataset = create_hf_dataset(val_image_paths, val_mask_paths)\n",
    "test_dataset = create_hf_dataset(test_image_paths, test_mask_paths)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84e93d",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Define feature extractor and transformations. The masks are converted to class ID masks using maximum performance optimization for Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e79827b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:54.183631Z",
     "iopub.status.busy": "2025-04-16T19:27:54.183301Z",
     "iopub.status.idle": "2025-04-16T19:27:54.322046Z",
     "shell.execute_reply": "2025-04-16T19:27:54.316604Z",
     "shell.execute_reply.started": "2025-04-16T19:27:54.183604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8368686178c34e3e9a872587ff22ca6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SegformerImageProcessor\n",
      "Cache directory: /dev/shm/segformer_cache\n",
      "\n",
      "Applying preprocessing using 4 processes with batch size 16...\n",
      "Available CPU cores: 4, Total RAM: 33.7GB\n",
      "Processing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441dda3ed2814873916fa5e2aced1454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train images (num_proc=4):   0%|          | 0/1606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset processed: 1606 items\n",
      "Processing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d4ba3f93ab4dbe879ec79a7f7556a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation images (num_proc=4):   0%|          | 0/171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset processed: 171 items\n",
      "Processing test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33ed4dbb1a94a9fa0706c8dcea1f37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test images (num_proc=4):   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset processed: 172 items\n",
      "\n",
      "Preprocessing completed!\n",
      "Processed dataset sizes: Train: 1606 images,  Validation: 171 images,  Test: 172 images\n",
      "\n",
      "Processed example structure:\n",
      "image: <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "label: <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "pixel_values: <class 'list'>\n",
      "labels: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor  # Use the updated class instead of FeatureExtractor\n",
    "import multiprocessing\n",
    "import psutil\n",
    "import tempfile\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Use the recommended ImageProcessor instead of FeatureExtractor\n",
    "try:\n",
    "    feature_extractor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "    print(\"Using SegformerImageProcessor\")\n",
    "except:\n",
    "    from transformers import SegformerFeatureExtractor\n",
    "    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "    print(\"Using SegformerFeatureExtractor (deprecated)\")\n",
    "\n",
    "# Define the RGB to Class ID mapping\n",
    "rgb_to_id = {\n",
    "    (0, 255, 255): 0,  # Urban land (Cyan)\n",
    "    (255, 255, 0): 1,  # Agriculture land (Yellow)\n",
    "    (255, 0, 255): 2,  # Rangeland (Magenta)\n",
    "    (0, 255, 0): 3,    # Forest land (Green)\n",
    "    (0, 0, 255): 4,    # Water (Blue)\n",
    "    (255, 255, 255): 5,# Barren land (White)\n",
    "    (0, 0, 0): 6       # Unknown (Black)\n",
    "}\n",
    "id_to_rgb = {v: k for k, v in rgb_to_id.items()} # Invert mapping for visualization\n",
    "\n",
    "# Specialized RGB to class ID conversion optimized for Kaggle\n",
    "def rgb_mask_to_class_id_mask_optimized(mask_img):\n",
    "    \"\"\"Highly optimized version that converts an RGB mask to class ID mask\"\"\"\n",
    "    # Convert to numpy array with uint8 for memory efficiency\n",
    "    mask_arr = np.array(mask_img.convert('RGB'), dtype=np.uint8)\n",
    "    \n",
    "    # Pre-allocate output array with default class (unknown)\n",
    "    class_mask = np.full(mask_arr.shape[:2], 6, dtype=np.uint8)\n",
    "    \n",
    "    # Use memory-efficient operations - creates a single view instead of multiple copies\n",
    "    # This specialized version shifts image data to create a single integer for comparison\n",
    "    # which is much faster than comparing three channels separately\n",
    "    rgb_packed = (mask_arr[:,:,0].astype(np.uint32) << 16) + \\\n",
    "                 (mask_arr[:,:,1].astype(np.uint32) << 8) + \\\n",
    "                 (mask_arr[:,:,2].astype(np.uint32))\n",
    "    \n",
    "    # Convert RGB tuples to packed integers for faster comparison\n",
    "    for rgb, class_id in rgb_to_id.items():\n",
    "        rgb_packed_val = (rgb[0] << 16) + (rgb[1] << 8) + rgb[2]\n",
    "        class_mask[rgb_packed == rgb_packed_val] = class_id\n",
    "    \n",
    "    return Image.fromarray(class_mask)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    \"\"\"Optimized preprocessing function\"\"\"\n",
    "    # Convert all images to RGB mode\n",
    "    images = [img.convert(\"RGB\") for img in examples['image']]\n",
    "    \n",
    "    # Process masks in parallel for better performance\n",
    "    # Note: This uses a thread pool, which works better for I/O-bound operations\n",
    "    # than Python's multiprocessing due to the GIL\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        labels = list(executor.map(rgb_mask_to_class_id_mask_optimized, examples['label']))\n",
    "    \n",
    "    # Process with feature extractor\n",
    "    inputs = feature_extractor(images, labels, return_tensors=\"pt\")\n",
    "    inputs['labels'] = inputs['labels'].squeeze(1)  # Remove channel dimension\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Set up proper cache directory for Kaggle\n",
    "# First try the shared memory location\n",
    "if os.path.exists(\"/dev/shm\"):\n",
    "    cache_dir = \"/dev/shm/segformer_cache\"\n",
    "else:\n",
    "    # Fallback to temp directory\n",
    "    cache_dir = os.path.join(tempfile.gettempdir(), 'segformer_cache')\n",
    "    \n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# Use all available CPU resources for Kaggle\n",
    "num_cpu = os.cpu_count()\n",
    "optimal_processes = max(1, min(8, num_cpu))  # Use between 1 and 8 processes\n",
    "batch_size = 16  # Batch size that works reliably\n",
    "\n",
    "print(f\"\\nApplying preprocessing using {optimal_processes} processes with batch size {batch_size}...\")\n",
    "print(f\"Available CPU cores: {num_cpu}, Total RAM: {psutil.virtual_memory().total / 1e9:.1f}GB\")\n",
    "\n",
    "if len(ds['train']) > 0:\n",
    "    # Create properly formatted cache file names with extensions to avoid the rindex error\n",
    "    cache_files = {\n",
    "        \"train\": os.path.join(cache_dir, \"train_processed.arrow\"),\n",
    "        \"validation\": os.path.join(cache_dir, \"val_processed.arrow\"),\n",
    "        \"test\": os.path.join(cache_dir, \"test_processed.arrow\"),\n",
    "    }\n",
    "    \n",
    "    # Clear any existing cache files if they exist\n",
    "    for cache_file in cache_files.values():\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                os.remove(cache_file)\n",
    "                print(f\"Removed existing cache file: {cache_file}\")\n",
    "            except:\n",
    "                print(f\"Could not remove existing cache file: {cache_file}\")\n",
    "    \n",
    "    # Process datasets one by one to avoid memory issues\n",
    "    processed_train = None\n",
    "    processed_validation = None\n",
    "    processed_test = None\n",
    "    \n",
    "    try:\n",
    "        # Process train dataset if available\n",
    "        if len(ds['train']) > 0:\n",
    "            print(\"Processing training dataset...\")\n",
    "            processed_train = ds['train'].map(\n",
    "                preprocess_data,\n",
    "                batched=True,\n",
    "                batch_size=batch_size,\n",
    "                num_proc=optimal_processes,\n",
    "                desc=\"Processing train images\",\n",
    "                cache_file_name=cache_files[\"train\"],\n",
    "                # Don't load from cache to force fresh processing\n",
    "                load_from_cache_file=False\n",
    "            )\n",
    "            print(f\"Train dataset processed: {len(processed_train)} items\")\n",
    "        \n",
    "        # Process validation dataset if available\n",
    "        if len(ds['validation']) > 0:\n",
    "            print(\"Processing validation dataset...\")\n",
    "            processed_validation = ds['validation'].map(\n",
    "                preprocess_data,\n",
    "                batched=True,\n",
    "                batch_size=batch_size,\n",
    "                num_proc=optimal_processes,\n",
    "                desc=\"Processing validation images\",\n",
    "                cache_file_name=cache_files[\"validation\"],\n",
    "                load_from_cache_file=False\n",
    "            )\n",
    "            print(f\"Validation dataset processed: {len(processed_validation)} items\")\n",
    "        \n",
    "        # Process test dataset if available\n",
    "        if len(ds['test']) > 0:\n",
    "            print(\"Processing test dataset...\")\n",
    "            processed_test = ds['test'].map(\n",
    "                preprocess_data,\n",
    "                batched=True,\n",
    "                batch_size=batch_size,\n",
    "                num_proc=optimal_processes,\n",
    "                desc=\"Processing test images\",\n",
    "                cache_file_name=cache_files[\"test\"],\n",
    "                load_from_cache_file=False\n",
    "            )\n",
    "            print(f\"Test dataset processed: {len(processed_test)} items\")\n",
    "        \n",
    "        # Create processed dataset dictionary\n",
    "        processed_ds = DatasetDict({\n",
    "            'train': processed_train if processed_train is not None else Dataset.from_dict({'pixel_values': [], 'labels': []}),\n",
    "            'validation': processed_validation if processed_validation is not None else Dataset.from_dict({'pixel_values': [], 'labels': []}),\n",
    "            'test': processed_test if processed_test is not None else Dataset.from_dict({'pixel_values': [], 'labels': []})\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        # Fallback to simpler processing without caching if error occurs\n",
    "        print(\"Falling back to simple processing without caching...\")\n",
    "        \n",
    "        # Process directly without caching for better compatibility\n",
    "        processed_ds = {}\n",
    "        for split in ['train', 'validation', 'test']:\n",
    "            if split in ds and len(ds[split]) > 0:\n",
    "                print(f\"Processing {split} dataset...\")\n",
    "                processed_ds[split] = ds[split].map(\n",
    "                    preprocess_data,\n",
    "                    batched=True,\n",
    "                    batch_size=batch_size,\n",
    "                    num_proc=1,  # Use single process for compatibility\n",
    "                    load_from_cache_file=False\n",
    "                )\n",
    "            else:\n",
    "                processed_ds[split] = Dataset.from_dict({'pixel_values': [], 'labels': []})\n",
    "        \n",
    "        processed_ds = DatasetDict(processed_ds)\n",
    "    \n",
    "    # Force garbage collection after preprocessing\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "   print(\"Skipping preprocessing as datasets are empty.\")\n",
    "   processed_ds = ds # Keep the empty structure\n",
    "\n",
    "print(\"\\nPreprocessing completed!\")\n",
    "print(f\"Processed dataset sizes: Train: {len(processed_ds['train'])} images, \",\n",
    "      f\"Validation: {len(processed_ds['validation'])} images, \",\n",
    "      f\"Test: {len(processed_ds['test'])} images\")\n",
    "\n",
    "# Show sample of processed data\n",
    "if len(processed_ds['train']) > 0:\n",
    "    print(\"\\nProcessed example structure:\")\n",
    "    for key, value in list(processed_ds['train'][0].items())[:5]:\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key}: Tensor of shape {value.shape}, dtype {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"{key}: {type(value)}\")\n",
    "else:\n",
    "    print(\"Train dataset is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168805cb",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Load a pre-trained SegFormer model and configure it for our specific number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284bafad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:54.324164Z",
     "iopub.status.busy": "2025-04-16T19:27:54.323902Z",
     "iopub.status.idle": "2025-04-16T19:27:54.962564Z",
     "shell.execute_reply": "2025-04-16T19:27:54.953143Z",
     "shell.execute_reply.started": "2025-04-16T19:27:54.324141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b649aff9356b4f5e8094d2fd29e984cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad9d22b2944f6aa956a06896bdc741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([7, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.01GB allocated,  0.01GB reserved\n",
      "Model loaded on cuda with 3.72M parameters (3.72M trainable)\n"
     ]
    }
   ],
   "source": [
    "# Clear CUDA cache before loading model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True, # Allow changing the classification head\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, # Use half precision if possible\n",
    "    low_cpu_mem_usage=True, # Minimize CPU memory usage during loading\n",
    ")\n",
    "\n",
    "# Move model to GPU and optimize for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimize memory usage on Kaggle\n",
    "if torch.cuda.is_available():\n",
    "    # Display GPU memory usage\n",
    "    print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f}GB allocated, \",\n",
    "          f\"{torch.cuda.memory_reserved() / 1e9:.2f}GB reserved\")\n",
    "\n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model loaded on {device} with {total_params/1e6:.2f}M parameters ({trainable_params/1e6:.2f}M trainable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574995",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Set up `TrainingArguments` with optimizations for P100 GPU and define the evaluation metric (Mean Intersection over Union - mIoU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91176e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:27:54.963964Z",
     "iopub.status.busy": "2025-04-16T19:27:54.963700Z",
     "iopub.status.idle": "2025-04-16T19:27:55.676895Z",
     "shell.execute_reply": "2025-04-16T19:27:55.673032Z",
     "shell.execute_reply.started": "2025-04-16T19:27:54.963938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3754a74fd5db4e078a26a4f8e4bd6de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Train Batch Size: 24, Eval Batch Size: 48\n",
      "Transformers version: 4.51.1\n",
      "Model does NOT support gradient checkpointing. Will disable it.\n",
      "\n",
      "Training arguments set for 100% Kaggle system utilization:\n",
      "- Output directory: ./segformer-finetuned-deepglobe-kaggle\n",
      "- Learning rate: 6e-05\n",
      "- Number of epochs: 15\n",
      "- Per device batch size: 24\n",
      "- Effective batch size: 24\n",
      "- Dataloader workers: 4 (using all CPU cores)\n",
      "- FP16 (mixed precision): True\n",
      "- Group by length: False (disabled for image segmentation)\n",
      "- Gradient checkpointing: False\n"
     ]
    }
   ],
   "source": [
    "# Import needed modules first\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load mean_iou metric\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Logits shape: (batch_size, num_labels, height/4, width/4)\n",
    "    # Labels shape: (batch_size, height, width)\n",
    "\n",
    "    # Move logits to CPU for upsampling if needed\n",
    "    if isinstance(logits, torch.Tensor) and logits.device.type == 'cuda':\n",
    "        logits = logits.cpu()\n",
    "        \n",
    "    # Convert logits to torch tensor if they are numpy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "        \n",
    "    # Ensure labels are also tensors for interpolation size check\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels_tensor = torch.from_numpy(labels)\n",
    "    else:\n",
    "        labels_tensor = labels\n",
    "        \n",
    "    # Move labels tensor to CPU if needed for size check\n",
    "    if hasattr(labels_tensor, 'device') and labels_tensor.device.type == 'cuda':\n",
    "        labels_tensor = labels_tensor.cpu()\n",
    "        \n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=labels_tensor.shape[-2:], # Target (height, width)\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    # Get predicted class IDs\n",
    "    pred_labels = upsampled_logits.argmax(dim=1).detach().cpu().numpy()\n",
    "    # Ensure labels are numpy arrays on CPU for metric computation\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred_labels,\n",
    "        references=labels,\n",
    "        num_labels=num_labels,\n",
    "        ignore_index=6, # Ignore the 'unknown' class if desired, otherwise use 255 or remove\n",
    "        reduce_labels=False, # We are not reducing labels\n",
    "    )\n",
    "\n",
    "    # Add per-category IoU metrics for better interpretation\n",
    "    # Handle potential KeyError if metric computation failed for some reason\n",
    "    per_category_iou = metrics.pop('per_category_iou', [0.0] * num_labels)\n",
    "    per_category_accuracy = metrics.pop('per_category_accuracy', [0.0] * num_labels)\n",
    "    for i, label in id2label.items():\n",
    "        metrics[f\"iou_{label}\"] = per_category_iou[i]\n",
    "        metrics[f\"accuracy_{label}\"] = per_category_accuracy[i]\n",
    "\n",
    "    # Return main metrics\n",
    "    return {\n",
    "        \"mean_iou\": metrics.get(\"mean_iou\", 0.0),\n",
    "        \"mean_accuracy\": metrics.get(\"mean_accuracy\", 0.0),\n",
    "        \"overall_accuracy\": metrics.get(\"overall_accuracy\", 0.0),\n",
    "        **metrics # Include per-category metrics as well\n",
    "    }\n",
    "\n",
    "# Maximizing batch sizes for Kaggle P100 - values tuned for best performance\n",
    "train_batch_size = 24  # Maximum for P100 with this model\n",
    "eval_batch_size = 48   # Can be larger for evaluation (no gradients stored)\n",
    "gradient_accumulation_steps = 1  # No accumulation needed with large batch\n",
    "\n",
    "print(f\"Using Train Batch Size: {train_batch_size}, Eval Batch Size: {eval_batch_size}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Create a data collator for image segmentation\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class SegmentationDataCollator:\n",
    "    \"\"\"Data collator for image segmentation that handles the conversion of data structures to tensors\"\"\"\n",
    "    def __call__(self, batch: List[Dict]) -> Dict:\n",
    "        # Initialize with empty lists\n",
    "        pixel_values = []\n",
    "        labels = []\n",
    "        \n",
    "        # Extract all values into lists\n",
    "        for example in batch:\n",
    "            if 'pixel_values' in example:\n",
    "                if isinstance(example['pixel_values'], torch.Tensor):\n",
    "                    pixel_values.append(example['pixel_values'])\n",
    "                elif isinstance(example['pixel_values'], list):\n",
    "                    # Convert nested lists to tensors\n",
    "                    pixel_values.append(torch.tensor(example['pixel_values']))\n",
    "            \n",
    "            if 'labels' in example:\n",
    "                if isinstance(example['labels'], torch.Tensor):\n",
    "                    labels.append(example['labels'])\n",
    "                elif isinstance(example['labels'], list):\n",
    "                    # Convert nested lists to tensors\n",
    "                    labels.append(torch.tensor(example['labels']))\n",
    "        \n",
    "        # Create the batch - check for empty lists to avoid errors\n",
    "        batch_dict = {}\n",
    "        if pixel_values:\n",
    "            # Stack tensors into batches\n",
    "            try:\n",
    "                batch_dict['pixel_values'] = torch.stack(pixel_values)\n",
    "            except:\n",
    "                print(\"Error stacking pixel_values, shapes may be inconsistent\")\n",
    "                # Attempt to handle arrays of different shapes (not ideal, but fallback)\n",
    "                batch_dict['pixel_values'] = pixel_values\n",
    "                \n",
    "        if labels:\n",
    "            try:\n",
    "                batch_dict['labels'] = torch.stack(labels)\n",
    "            except:\n",
    "                print(\"Error stacking labels, shapes may be inconsistent\")\n",
    "                batch_dict['labels'] = labels\n",
    "                \n",
    "        return batch_dict\n",
    "\n",
    "# Import TrainingArguments after we ensure transformers is properly imported\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Check if the model supports gradient checkpointing before enabling it\n",
    "def supports_gradient_checkpointing(model):\n",
    "    \"\"\"Check if the model supports gradient checkpointing\"\"\"\n",
    "    if hasattr(model, 'supports_gradient_checkpointing'):\n",
    "        return model.supports_gradient_checkpointing\n",
    "    # Try to access the gradient_checkpointing_enable method\n",
    "    return hasattr(model, 'gradient_checkpointing_enable')\n",
    "\n",
    "# Check the model supports gradient checkpointing\n",
    "use_gradient_checkpointing = False\n",
    "try:\n",
    "    # This will raise an error if gradient_checkpointing is not supported\n",
    "    if supports_gradient_checkpointing(model):\n",
    "        use_gradient_checkpointing = True\n",
    "        print(\"Model supports gradient checkpointing. Enabling it.\")\n",
    "    else:\n",
    "        print(\"Model does NOT support gradient checkpointing. Will disable it.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking gradient checkpointing support: {e}\")\n",
    "    print(\"Will disable gradient checkpointing to be safe.\")\n",
    "\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./segformer-finetuned-deepglobe-kaggle\",\n",
    "        learning_rate=6e-5,\n",
    "        num_train_epochs=15,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Kaggle full system utilization optimizations\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=True,  # Use mixed precision\n",
    "        fp16_opt_level=\"O2\",  # Aggressive mixed precision\n",
    "        half_precision_backend=\"auto\",\n",
    "        optim=\"adamw_torch\",\n",
    "        dataloader_num_workers=os.cpu_count(),  # Use ALL CPU cores\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_persistent_workers=True,  # Keep workers alive between epochs\n",
    "        \n",
    "        # Performance tuning for Kaggle\n",
    "        gradient_checkpointing=use_gradient_checkpointing,  # Only use if supported\n",
    "        bf16=False,  # P100 doesn't support bfloat16\n",
    "        label_smoothing_factor=0.1,  # Helps generalization\n",
    "        weight_decay=0.01,          # Prevents overfitting\n",
    "        max_grad_norm=1.0,          # Gradient clipping for stability\n",
    "        warmup_ratio=0.1,           # Warmup for learning rate\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",  # Better convergence\n",
    "        \n",
    "        # Evaluation and save strategies\n",
    "        eval_strategy=\"steps\", \n",
    "        save_strategy=\"steps\", \n",
    "        logging_strategy=\"steps\",\n",
    "        eval_steps=300,  # More frequent evaluation \n",
    "        save_steps=300,  # More frequent saving\n",
    "        logging_steps=50,  # More frequent logging\n",
    "        \n",
    "        # IMPORTANT: DISABLE group_by_length for image data\n",
    "        group_by_length=False,  # This was causing the error\n",
    "        \n",
    "        # Other training parameters\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mean_iou\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "except TypeError as e:\n",
    "    print(f\"First attempt failed with error: {e}\")\n",
    "    # Fallback to compatible parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./segformer-finetuned-deepglobe-kaggle\",\n",
    "        learning_rate=6e-5,\n",
    "        num_train_epochs=15,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Kaggle full system utilization optimizations (compatible subset)\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=os.cpu_count(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        # Explicitly disable gradient checkpointing in fallback\n",
    "        gradient_checkpointing=False,\n",
    "        \n",
    "        # Try appropriate parameter names\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        logging_strategy=\"steps\",\n",
    "        \n",
    "        eval_steps=300,\n",
    "        save_steps=300,\n",
    "        logging_steps=50,\n",
    "        \n",
    "        # IMPORTANT: DISABLE group_by_length for image data\n",
    "        group_by_length=False,  # This was causing the error\n",
    "        \n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mean_iou\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "# Calculate effective batch size\n",
    "effective_batch_size = train_batch_size * gradient_accumulation_steps\n",
    "print(\"\\nTraining arguments set for 100% Kaggle system utilization:\")\n",
    "print(f\"- Output directory: {training_args.output_dir}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Per device batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Effective batch size: {effective_batch_size}\")\n",
    "print(f\"- Dataloader workers: {training_args.dataloader_num_workers} (using all CPU cores)\")\n",
    "print(f\"- FP16 (mixed precision): {training_args.fp16}\")\n",
    "print(f\"- Group by length: {training_args.group_by_length} (disabled for image segmentation)\")\n",
    "print(f\"- Gradient checkpointing: {getattr(training_args, 'gradient_checkpointing', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83e82",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-16T19:27:55.680485Z",
     "iopub.status.idle": "2025-04-16T19:27:55.681710Z",
     "shell.execute_reply": "2025-04-16T19:27:55.680888Z",
     "shell.execute_reply.started": "2025-04-16T19:27:55.680871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before training: 0.01GB allocated,  0.01GB reserved\n",
      "Training dataset size: 1606 samples\n",
      "- image: type <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "- label: type <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      "- pixel_values: list of length 3, element type <class 'list'>\n",
      "- labels: list of length 512, element type <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with 100% Kaggle system utilization...\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Trainer explicitly here to avoid any issues\n",
    "from transformers import Trainer\n",
    "\n",
    "# Clean up memory before training\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory before training: {torch.cuda.memory_allocated() / 1e9:.2f}GB allocated, \",\n",
    "          f\"{torch.cuda.memory_reserved() / 1e9:.2f}GB reserved\")\n",
    "\n",
    "# Check if datasets are valid before creating Trainer\n",
    "train_data_available = 'train' in processed_ds and len(processed_ds['train']) > 0\n",
    "eval_data_available = 'validation' in processed_ds and len(processed_ds['validation']) > 0\n",
    "\n",
    "# Display dataset shapes - safely check the structure\n",
    "if train_data_available:\n",
    "    print(f\"Training dataset size: {len(processed_ds['train'])} samples\")\n",
    "\n",
    "    # Get the first element to inspect\n",
    "    sample = processed_ds['train'][0]\n",
    "\n",
    "    # Safely check and print the structure\n",
    "    for key in list(sample.keys()):\n",
    "        value = sample[key]\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"- {key}: Tensor of shape {value.size()}, dtype {value.dtype}\")\n",
    "        elif hasattr(value, 'shape'):  # For numpy arrays or other objects with shape attribute\n",
    "            print(f\"- {key}: shape {value.shape}, dtype {type(value)}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"- {key}: list of length {len(value)}, element type {type(value[0]) if value else 'empty list'}\")\n",
    "        else:\n",
    "            print(f\"- {key}: type {type(value)}\")\n",
    "\n",
    "# Instantiate the custom data collator defined previously\n",
    "# This ensures the Trainer uses our custom logic for batching segmentation data\n",
    "data_collator = SegmentationDataCollator()\n",
    "\n",
    "trainer = None\n",
    "if train_data_available and eval_data_available:\n",
    "    # Initialize Trainer with maximum performance settings\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_ds[\"train\"],\n",
    "        eval_dataset=processed_ds[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator  # Pass the custom collator here\n",
    "    )\n",
    "\n",
    "    # Start training with full system utilization\n",
    "    # NOTE: If you encounter Out-of-Memory (OOM) errors, try reducing\n",
    "    # per_device_train_batch_size and per_device_eval_batch_size in the TrainingArguments cell above.\n",
    "    print(\"\\nStarting training with 100% Kaggle system utilization...\")\n",
    "    try:\n",
    "        train_results = trainer.train()\n",
    "\n",
    "        # Show memory usage during training\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Peak GPU Memory: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # Save the best model and training state\n",
    "        print(\"Saving model and state...\")\n",
    "        trainer.save_model()\n",
    "        trainer.save_state()\n",
    "        print(\"Model and state saved.\")\n",
    "\n",
    "        # Clean memory after training\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        print(\"Training Results:\", train_results)\n",
    "\n",
    "        # Log metrics\n",
    "        metrics = train_results.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "        # Evaluate after training (on validation set)\n",
    "        print(\"\\nEvaluating final model on validation set...\")\n",
    "        eval_metrics = trainer.evaluate(eval_dataset=processed_ds[\"validation\"])\n",
    "        trainer.log_metrics(\"eval\", eval_metrics)\n",
    "        trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        # It might be helpful to raise the error here to stop execution if training fails critically\n",
    "        # raise e\n",
    "        print(\"\\nAttempting to continue with evaluation or visualization if possible...\")\n",
    "        # Even if training fails, we might still be able to evaluate the model\n",
    "        # or perform visualization with the current model state\n",
    "else:\n",
    "    print(\"Skipping training as train or validation dataset is empty or invalid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd820e",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Visualization\n",
    "\n",
    "Evaluate the fine-tuned model on the test set and visualize some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd3d15",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-16T19:27:55.685253Z",
     "iopub.status.idle": "2025-04-16T19:27:55.686840Z",
     "shell.execute_reply": "2025-04-16T19:27:55.685432Z",
     "shell.execute_reply.started": "2025-04-16T19:27:55.685418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_available = 'test' in processed_ds and len(processed_ds['test']) > 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_metrics = None\n",
    "\n",
    "if trainer is not None and test_data_available:\n",
    "    print(\"\\nEvaluating on the test set...\")\n",
    "    # Use the predict method to get raw predictions and labels\n",
    "    test_results = trainer.predict(processed_ds['test'])\n",
    "    test_metrics = test_results.metrics # Metrics are computed based on aggregated predictions/labels\n",
    "    \n",
    "    print(\"\\nTest Set Evaluation Results:\")\n",
    "    print(test_metrics)\n",
    "    trainer.log_metrics(\"test\", test_metrics)\n",
    "    trainer.save_metrics(\"test\", test_metrics)\n",
    "    \n",
    "    # Extract predictions and labels for confusion matrix and visualization\n",
    "    logits = test_results.predictions\n",
    "    labels = test_results.label_ids\n",
    "    \n",
    "    # Upsample logits and get predicted labels (needs to be done on CPU)\n",
    "    if isinstance(logits, torch.Tensor) and logits.device.type == 'cuda':\n",
    "        logits = logits.cpu()\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "        \n",
    "    if isinstance(labels, torch.Tensor) and labels.device.type == 'cuda':\n",
    "        labels = labels.cpu()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.numpy() # Ensure labels are numpy array\n",
    "        \n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=labels.shape[-2:], # Target (height, width)\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    all_preds = upsampled_logits.argmax(dim=1).detach().cpu().numpy().flatten()\n",
    "    all_labels = labels.flatten()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping test set evaluation as trainer was not initialized or test dataset is empty.\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "if len(all_preds) > 0 and len(all_labels) > 0:\n",
    "    # Filter out ignored labels if necessary (e.g., 'unknown' class with id 6)\n",
    "    ignore_idx = 6 \n",
    "    valid_indices = all_labels != ignore_idx\n",
    "    filtered_labels = all_labels[valid_indices]\n",
    "    filtered_preds = all_preds[valid_indices]\n",
    "    \n",
    "    print(f\"\\nGenerating Confusion Matrix (ignoring class {ignore_idx}: '{id2label.get(ignore_idx, 'N/A')}')\")\n",
    "    if len(filtered_labels) > 0: # Ensure there are valid labels left\n",
    "        cm = confusion_matrix(filtered_labels, filtered_preds, labels=list(range(num_labels-1))) # Exclude ignored class from labels\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names[:-1], yticklabels=class_names[:-1])\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix (Test Set)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping confusion matrix: No valid labels after filtering.\")\n",
    "else:\n",
    "    print(\"Skipping confusion matrix generation (no predictions/labels available).\")\n",
    "\n",
    "# --- Visualization ---\n",
    "def visualize_predictions(num_samples=5):\n",
    "    if trainer is None or not test_data_available or len(test_image_paths) == 0:\n",
    "        print(\"Skipping visualization: Trainer not available, test data missing, or no test image paths.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nVisualizing predictions for {num_samples} random test samples...\")\n",
    "    \n",
    "    # Ensure model is on CPU for visualization\n",
    "    # Load the *saved* best model from disk onto CPU for consistent visualization\n",
    "    print(f\"Loading best model from {training_args.output_dir} for visualization...\")\n",
    "    try:\n",
    "        viz_model = SegformerForSemanticSegmentation.from_pretrained(training_args.output_dir).cpu()\n",
    "        viz_model.eval() # Set model to evaluation mode\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading saved model for visualization: {e}. Using current model state on CPU.\")\n",
    "        # Fallback to using the current model state moved to CPU\n",
    "        viz_model = model.cpu()\n",
    "        viz_model.eval()\n",
    "    \n",
    "    # Get random indices\n",
    "    num_available = len(test_image_paths)\n",
    "    indices = random.sample(range(num_available), min(num_samples, num_available))\n",
    "    \n",
    "    for i in indices:\n",
    "        image_path = test_image_paths[i]\n",
    "        mask_path = test_mask_paths[i]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            true_mask_rgb = Image.open(mask_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image/mask {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess image for model\n",
    "        # Feature extractor runs on CPU for visualization consistency\n",
    "        encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "        pixel_values = encoding.pixel_values # Already on CPU\n",
    "\n",
    "        # Predict using the CPU model\n",
    "        with torch.no_grad():\n",
    "            outputs = viz_model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits # Shape: (1, num_labels, H/4, W/4)\n",
    "\n",
    "        # Upsample logits to original image size\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1], # (height, width)\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        pred_mask_id = upsampled_logits.argmax(dim=1).squeeze().numpy()\n",
    "\n",
    "        # Convert predicted IDs back to RGB\n",
    "        pred_mask_rgb = np.zeros((*pred_mask_id.shape, 3), dtype=np.uint8)\n",
    "        for class_id, color in id_to_rgb.items():\n",
    "            pred_mask_rgb[pred_mask_id == class_id] = color\n",
    "            \n",
    "        # Create color legend patches\n",
    "        legend_patches = [plt.Rectangle((0,0),1,1, fc=np.array(color)/255.0) for color in id_to_rgb.values()]\n",
    "        legend_labels = [f\"{idx}: {name}\" for idx, name in id2label.items()]\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f\"Sample {i}: {os.path.basename(image_path)}\")\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(\"Input Image\")\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(true_mask_rgb)\n",
    "        axes[1].set_title(\"True Mask (RGB)\")\n",
    "        axes[1].axis('off')\n",
    "        axes[2].imshow(pred_mask_rgb)\n",
    "        axes[2].set_title(\"Predicted Mask (RGB)\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Add legend to the figure\n",
    "        fig.legend(legend_patches, legend_labels, loc='lower center', ncol=len(id_to_rgb), bbox_to_anchor=(0.5, -0.05))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust layout to make space for legend\n",
    "        plt.show()\n",
    "        \n",
    "    # Move the original model back to GPU after visualization if needed\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbf155",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis (P100 GPU)\n",
    "\n",
    "Analyze the training performance on P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs for performance analysis if available\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_training_performance():\n",
    "    log_files = glob.glob(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"No training logs found.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        with open(log_files[0], 'r') as f:\n",
    "            logs = json.load(f)\n",
    "            \n",
    "        # Extract relevant metrics\n",
    "        steps = [log['step'] for log in logs['log_history'] if 'loss' in log]\n",
    "        losses = [log['loss'] for log in logs['log_history'] if 'loss' in log]\n",
    "        learning_rates = [log['learning_rate'] for log in logs['log_history'] if 'learning_rate' in log and 'loss' in log]\n",
    "        \n",
    "        # Calculate training time and performance metrics\n",
    "        if 'created_at' in logs and logs['created_at'] and 'last_update' in logs and logs['last_update']:\n",
    "            created_at = datetime.fromisoformat(logs['created_at'].replace('Z', '+00:00'))\n",
    "            last_update = datetime.fromisoformat(logs['last_update'].replace('Z', '+00:00'))\n",
    "            training_duration = (last_update - created_at).total_seconds() / 60  # in minutes\n",
    "            \n",
    "            total_train_steps = logs['global_step']\n",
    "            steps_per_second = total_train_steps / (training_duration * 60)\n",
    "            \n",
    "            print(f\"\\nP100 GPU Performance Analysis:\")\n",
    "            print(f\"- Total training time: {training_duration:.2f} minutes\")\n",
    "            print(f\"- Steps per second: {steps_per_second:.2f}\")\n",
    "            print(f\"- Initial loss: {losses[0]:.4f}\")\n",
    "            print(f\"- Final loss: {losses[-1]:.4f}\")\n",
    "            \n",
    "        # Plot training progress\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        ax1.plot(steps, losses, 'b-', label='Training Loss')\n",
    "        ax1.set_title('Training Loss on P100 GPU')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        \n",
    "        if learning_rates:\n",
    "            ax2.plot(steps[:len(learning_rates)], learning_rates, 'g-', label='Learning Rate')\n",
    "            ax2.set_title('Learning Rate Schedule')\n",
    "            ax2.set_xlabel('Training Steps')\n",
    "            ax2.set_ylabel('Learning Rate')\n",
    "            ax2.grid(True)\n",
    "            ax2.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing training logs: {e}\")\n",
    "\n",
    "# Run performance analysis if training was completed\n",
    "if trainer is not None:\n",
    "    analyze_training_performance()\n",
    "else:\n",
    "    print(\"No training was performed, skipping performance analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor system usage\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def print_system_usage():\n",
    "    \"\"\"Print current system resource usage\"\"\"\n",
    "    # CPU usage\n",
    "    cpu_usage = psutil.cpu_percent(interval=0.1)\n",
    "    \n",
    "    # Memory usage\n",
    "    mem = psutil.virtual_memory()\n",
    "    mem_usage = mem.percent\n",
    "    mem_used_gb = mem.used / (1024 ** 3)\n",
    "    mem_total_gb = mem.total / (1024 ** 3)\n",
    "    \n",
    "    # Disk usage\n",
    "    disk = psutil.disk_usage('/')\n",
    "    disk_usage = disk.percent\n",
    "    \n",
    "    print(f\"System Resource Usage:\")\n",
    "    print(f\"- CPU: {cpu_usage}% used across {psutil.cpu_count()} cores\")\n",
    "    print(f\"- RAM: {mem_usage}% used ({mem_used_gb:.1f}GB / {mem_total_gb:.1f}GB)\")\n",
    "    print(f\"- Disk: {disk_usage}% used\")\n",
    "    \n",
    "    # GPU usage if available\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                gpu_usage = torch.cuda.memory_allocated(i) / torch.cuda.get_device_properties(i).total_memory * 100\n",
    "                print(f\"- GPU {i}: {gpu_usage:.1f}% allocated\")\n",
    "                print(f\"  Memory: {torch.cuda.memory_allocated(i) / 1e9:.2f}GB / {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f}GB\")\n",
    "        except:\n",
    "            try:\n",
    "                # Alternative using GPUtil\n",
    "                gpus = GPUtil.getGPUs()\n",
    "                for i, gpu in enumerate(gpus):\n",
    "                    print(f\"- GPU {i}: {gpu.memoryUtil*100:.1f}% used\")\n",
    "                    print(f\"  Memory: {gpu.memoryUsed:.2f}GB / {gpu.memoryTotal:.2f}GB\")\n",
    "            except:\n",
    "                print(\"- GPU: Info not available\")\n",
    "\n",
    "# Print current system usage\n",
    "print_system_usage()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 966962,
     "sourceId": 1635643,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31013,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
