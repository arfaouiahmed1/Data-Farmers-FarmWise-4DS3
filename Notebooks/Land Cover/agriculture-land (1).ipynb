{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7e21fc",
   "metadata": {
    "papermill": {
     "duration": 0.003762,
     "end_time": "2025-04-07T10:04:06.039919",
     "exception": false,
     "start_time": "2025-04-07T10:04:06.036157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Farm Field Instance Segmentation using MMDetection\n",
    "\n",
    "**Goal:** To develop an instance segmentation model capable of identifying and delineating individual farm fields from aerial/satellite imagery. The secondary goal is to enable classification based on field size using post-processing techniques after successful segmentation.\n",
    "\n",
    "**Methodology:** Utilize the MMDetection framework with a pre-trained model (e.g., Mask R-CNN) fine-tuned on a custom dataset of farm fields.\n",
    "\n",
    "**Environment:** Kaggle Notebook with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45250d55",
   "metadata": {
    "papermill": {
     "duration": 0.002803,
     "end_time": "2025-04-07T10:04:06.046236",
     "exception": false,
     "start_time": "2025-04-07T10:04:06.043433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Acquisition & Understanding\n",
    "\n",
    "This section handles acquiring the dataset from Roboflow and performing an initial check. We will use the COCO format, as it's well-suited for MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bb51b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:04:06.053601Z",
     "iopub.status.busy": "2025-04-07T10:04:06.053216Z",
     "iopub.status.idle": "2025-04-07T10:04:14.627669Z",
     "shell.execute_reply": "2025-04-07T10:04:14.626219Z"
    },
    "papermill": {
     "duration": 8.579963,
     "end_time": "2025-04-07T10:04:14.629153",
     "exception": true,
     "start_time": "2025-04-07T10:04:06.049190",
     "status": "failed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mWARNING: Using placeholder API Key. Please replace 'YOUR_ROBOFLOW_API_KEY' or set up Kaggle Secrets.\n",
      "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=undefined&ref=undefined\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "ename": "RoboflowError",
     "evalue": "{\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"YOUR_ROBOFLOW_API_KEY\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRoboflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ca27a8f7d57>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Download dataset in COCO format suitable for MMDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sid-mp92l\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final-detectron-2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"coco\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use standard COCO export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36mworkspace\u001b[0;34m(self, the_workspace)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if api_key was passed during __init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mlist_projects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_workspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_workspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_projects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/roboflow/adapters/rfapi.py\u001b[0m in \u001b[0;36mget_workspace\u001b[0;34m(api_key, workspace_url)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRoboflowError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRoboflowError\u001b[0m: {\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"YOUR_ROBOFLOW_API_KEY\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Install Roboflow client\n",
    "!pip install -q roboflow\n",
    "\n",
    "# Import and authenticate\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- IMPORTANT: Replace with your actual Roboflow API Key ---\n",
    "# It's highly recommended to store API keys securely, e.g., using Kaggle Secrets\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"HE9CEH5JxJ3U0vXrQTOy\") \n",
    "    print(\"Using Roboflow API Key from Kaggle Secrets.\")\n",
    "except:\n",
    "    api_key = \"YOUR_ROBOFLOW_API_KEY\" # Fallback: Replace this placeholder directly\n",
    "    if api_key == \"YOUR_ROBOFLOW_API_KEY\":\n",
    "         print(\"WARNING: Using placeholder API Key. Please replace 'YOUR_ROBOFLOW_API_KEY' or set up Kaggle Secrets.\")\n",
    "    else:\n",
    "         print(\"Using placeholder API Key from code variable.\")\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "rf = Roboflow(api_key=api_key)\n",
    "\n",
    "# Download dataset in COCO format suitable for MMDetection\n",
    "project = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"coco\") # Use standard COCO export\n",
    "\n",
    "# Store the location of the downloaded dataset\n",
    "# Roboflow downloads to a folder based on project name and version\n",
    "# Check the output of the download cell to confirm the exact path\n",
    "# It will likely be something like './final-detectron-2-1/'\n",
    "# Let's determine it dynamically\n",
    "dataset_location = dataset.location\n",
    "print(f\"Dataset downloaded to: {dataset_location}\")\n",
    "\n",
    "# Define paths to annotation files and image directories\n",
    "# COCO format typically has train/valid/test splits\n",
    "train_annot_path = os.path.join(dataset_location, \"train\", \"_annotations.coco.json\")\n",
    "train_img_dir = os.path.join(dataset_location, \"train\")\n",
    "\n",
    "valid_annot_path = os.path.join(dataset_location, \"valid\", \"_annotations.coco.json\")\n",
    "valid_img_dir = os.path.join(dataset_location, \"valid\")\n",
    "\n",
    "# (Optional) Test set paths if they exist\n",
    "test_annot_path = os.path.join(dataset_location, \"test\", \"_annotations.coco.json\")\n",
    "test_img_dir = os.path.join(dataset_location, \"test\")\n",
    "\n",
    "# Basic Data Understanding: Check if files exist and load annotations\n",
    "print(\"\\nChecking downloaded files:\")\n",
    "print(f\"Train annotations exist: {os.path.exists(train_annot_path)}\")\n",
    "print(f\"Train images folder exists: {os.path.exists(train_img_dir)}\")\n",
    "print(f\"Validation annotations exist: {os.path.exists(valid_annot_path)}\")\n",
    "print(f\"Validation images folder exists: {os.path.exists(valid_img_dir)}\")\n",
    "\n",
    "# Load train annotations to inspect structure (optional)\n",
    "if os.path.exists(train_annot_path):\n",
    "    with open(train_annot_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    print(f\"\\nNumber of training images: {len(train_data.get('images', []))}\")\n",
    "    print(f\"Number of training annotations: {len(train_data.get('annotations', []))}\")\n",
    "    categories = train_data.get('categories', [])\n",
    "    print(f\"Categories: {categories}\")\n",
    "    if categories:\n",
    "        num_classes = len(categories)\n",
    "        print(f\"Number of classes found: {num_classes}\")\n",
    "    else:\n",
    "        num_classes = 0 # Handle case where categories might be missing\n",
    "        print(\"Warning: Could not determine number of classes from annotations.\")\n",
    "else:\n",
    "    print(\"\\nCould not load training annotations for inspection.\")\n",
    "    num_classes = 1 # Assume 1 class ('farm') if annotations can't be read\n",
    "\n",
    "# Assert that we have only one class ('farm') based on our prior discussion\n",
    "# MMDetection needs num_classes = number of foreground classes\n",
    "if categories:\n",
    "     actual_num_classes = len(categories)\n",
    "     if actual_num_classes == 1:\n",
    "         print(f\"Confirmed: Dataset has {actual_num_classes} class ('{categories[0]['name']}').\")\n",
    "         num_classes = 1 # Set for MMDetection config\n",
    "     else:\n",
    "         print(f\"WARNING: Expected 1 class but found {actual_num_classes}. Check Roboflow export/annotations.\")\n",
    "         # Decide how to proceed - maybe default to 1 or raise an error\n",
    "         num_classes = actual_num_classes # Or keep as 1 if you're sure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298bc7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Modeling - Environment Setup\n",
    "\n",
    "Install MMDetection and its dependencies (`mmcv`, `mmengine`). We use `mim` (OpenMMLab's tool) for smoother installation, especially for `mmcv` which often needs compilation matching the PyTorch/CUDA version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52085e30",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T09:56:29.057388Z",
     "iopub.status.idle": "2025-04-07T09:56:29.057734Z",
     "shell.execute_reply": "2025-04-07T09:56:29.057603Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install mim\n",
    "!pip install -q openmim\n",
    "\n",
    "# Use mim to install mmengine and mmcv\n",
    "# This command automatically selects the correct mmcv binary for Kaggle's environment\n",
    "!mim install mmengine mmcv\n",
    "\n",
    "# Clone the MMDetection repository\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "# Install MMDetection from source\n",
    "!pip install -e .\n",
    "\n",
    "# Verify installation (optional)\n",
    "import mmcv\n",
    "import mmdet\n",
    "import mmengine\n",
    "print(f\"MMCV version: {mmcv.__version__}\")\n",
    "print(f\"MMDetection version: {mmdet.__version__}\")\n",
    "print(f\"MMEngine version: {mmengine.__version__}\")\n",
    "\n",
    "# Go back to the working directory root for clarity\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc2d38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Modeling - Configuration\n",
    "\n",
    "MMDetection uses Python configuration files (`.py`). We'll select a pre-trained instance segmentation model config (e.g., Mask R-CNN with ResNet-50 backbone) and modify it for our custom dataset.\n",
    "\n",
    "**Key Modifications:**\n",
    "1.  Update dataset paths (train/val annotations and image directories).\n",
    "2.  Set the number of classes (`num_classes`) in the model's classification head.\n",
    "3.  Specify the path to the pre-trained model weights (`load_from`).\n",
    "4.  Adjust training parameters (e.g., epochs, learning rate) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05c22d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T09:56:29.058656Z",
     "iopub.status.idle": "2025-04-07T09:56:29.059041Z",
     "shell.execute_reply": "2025-04-07T09:56:29.058861Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mmengine import Config\n",
    "\n",
    "# --- Choose a base config ---\n",
    "# Example: Mask R-CNN with ResNet-50 backbone, FPN neck, trained 1x schedule on COCO\n",
    "# You can browse other configs in './mmdetection/configs/'\n",
    "base_config_name = 'mask-rcnn_r50_fpn_1x_coco.py'\n",
    "base_config_path = os.path.join('mmdetection', 'configs', 'mask_rcnn', base_config_name)\n",
    "\n",
    "# --- Define path for our custom config ---\n",
    "custom_config_name = 'mask-rcnn_r50_fpn_1x_farm.py'\n",
    "custom_config_path = os.path.join('.', custom_config_name) # Save in working dir\n",
    "\n",
    "# --- Load the base configuration ---\n",
    "cfg = Config.fromfile(base_config_path)\n",
    "\n",
    "# --- Modify the configuration ---\n",
    "\n",
    "# 1. Dataset Paths & Type\n",
    "cfg.data_root = dataset_location # Root directory of the dataset\n",
    "cfg.dataset_type = 'CocoDataset' # Explicitly set dataset type\n",
    "\n",
    "cfg.train_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.train_dataloader.dataset.ann_file = os.path.join('train', '_annotations.coco.json') # Relative to data_root\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img=os.path.join('train', '')) # Relative to data_root\n",
    "\n",
    "cfg.val_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.ann_file = os.path.join('valid', '_annotations.coco.json') # Relative to data_root\n",
    "cfg.val_dataloader.dataset.data_prefix = dict(img=os.path.join('valid', '')) # Relative to data_root\n",
    "\n",
    "# Modify validation evaluator if necessary (usually fine)\n",
    "cfg.val_evaluator.ann_file = os.path.join(cfg.data_root, 'valid', '_annotations.coco.json') # Needs full path or path relative to runtime\n",
    "\n",
    "# If a test set exists and you want to configure it:\n",
    "# cfg.test_dataloader.dataset.type = cfg.dataset_type\n",
    "# cfg.test_dataloader.dataset.data_root = cfg.data_root\n",
    "# cfg.test_dataloader.dataset.ann_file = os.path.join('test', '_annotations.coco.json')\n",
    "# cfg.test_dataloader.dataset.data_prefix = dict(img=os.path.join('test', ''))\n",
    "# cfg.test_evaluator.ann_file = os.path.join(cfg.data_root, 'test', '_annotations.coco.json')\n",
    "\n",
    "# 2. Number of Classes\n",
    "# Important: Needs to be changed in the model's head definition\n",
    "# Find the roi_head section and update num_classes for bbox_head and mask_head\n",
    "# The number of classes should be the number of foreground classes (1 for 'farm')\n",
    "cfg.model.roi_head.bbox_head.num_classes = num_classes\n",
    "cfg.model.roi_head.mask_head.num_classes = num_classes\n",
    "\n",
    "# 3. Pre-trained Weights\n",
    "# MMDetection automatically downloads weights if not found locally,\n",
    "# using the URL specified in the base config's `load_from`.\n",
    "# You can explicitly define it or rely on the default from the base config.\n",
    "# Example of finding the default URL:\n",
    "print(f\"Attempting to load pre-trained weights from: {cfg.load_from}\")\n",
    "# No change needed here unless you want to use a different checkpoint.\n",
    "\n",
    "# 4. Training Parameters (Optional Adjustments)\n",
    "# Example: Reduce epochs for faster initial run\n",
    "# cfg.train_cfg.max_epochs = 12 # Default is often 12 ('1x' schedule)\n",
    "cfg.train_cfg.val_interval = 1 # Validate every epoch\n",
    "\n",
    "# Set evaluation interval - useful for long training runs\n",
    "# cfg.default_hooks.checkpoint.interval = 1 # Save checkpoint every epoch\n",
    "\n",
    "# Modify learning rate (optional, defaults might be okay for fine-tuning)\n",
    "# cfg.optim_wrapper.optimizer.lr = 0.02 / 8 # Example: Scale LR based on batch size if changed\n",
    "\n",
    "# Set batch size (adjust based on GPU memory in Kaggle)\n",
    "# Default might be 2 images per GPU. Check cfg.train_dataloader.batch_size\n",
    "# cfg.train_dataloader.batch_size = 2\n",
    "# cfg.train_dataloader.num_workers = 2 # Adjust based on Kaggle CPU resources\n",
    "\n",
    "# Set a work directory for logs and checkpoints\n",
    "cfg.work_dir = './work_dirs/farm_mask_rcnn'\n",
    "\n",
    "# Create work_dir if it doesn't exist\n",
    "os.makedirs(cfg.work_dir, exist_ok=True)\n",
    "\n",
    "# --- Save the modified configuration ---\n",
    "cfg.dump(custom_config_path)\n",
    "print(f\"Modified configuration saved to: {custom_config_path}\")\n",
    "print(f\"\\nKey changes applied:\")\n",
    "print(f\"  - Dataset root: {cfg.data_root}\")\n",
    "print(f\"  - Train annotations: {cfg.train_dataloader.dataset.ann_file}\")\n",
    "print(f\"  - Val annotations: {cfg.val_dataloader.dataset.ann_file}\")\n",
    "print(f\"  - Num classes (head): {cfg.model.roi_head.bbox_head.num_classes}\")\n",
    "print(f\"  - Work directory: {cfg.work_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d632e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Modeling - Training\n",
    "\n",
    "Use the MMDetection training script with our custom configuration file. Monitor the output for training loss and validation metrics (like mAP for segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68babdaf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T09:56:29.060052Z",
     "iopub.status.idle": "2025-04-07T09:56:29.060435Z",
     "shell.execute_reply": "2025-04-07T09:56:29.060282Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change directory to where the train script is located\n",
    "%cd mmdetection\n",
    "\n",
    "# Construct the training command\n",
    "# --work-dir specifies where logs/models are saved, overriding the config file if needed\n",
    "# We already set it in the config, so it's slightly redundant but ensures it's used.\n",
    "train_command = f\"python tools/train.py {os.path.join('..', custom_config_path)} --work-dir {os.path.join('..', cfg.work_dir)}\"\n",
    "\n",
    "print(f\"Running training command:\\n{train_command}\")\n",
    "\n",
    "# Execute the training command\n",
    "!{train_command}\n",
    "\n",
    "# Change back to the root working directory after training\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a571f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Modeling - Evaluation & Prediction (Initial)\n",
    "\n",
    "After training, you can evaluate the best performing checkpoint on the validation set or run inference on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c538222",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-07T09:56:29.061362Z",
     "iopub.status.idle": "2025-04-07T09:56:29.061679Z",
     "shell.execute_reply": "2025-04-07T09:56:29.061530Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Evaluation (Optional) ---\n",
    "# Usually done automatically during training if configured.\n",
    "# You can re-run evaluation using test.py if needed:\n",
    "# !python ./mmdetection/tools/test.py {custom_config_path} {path_to_best_checkpoint.pth} --show # For visualization\n",
    "\n",
    "# --- Inference on a Sample Image ---\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "import mmcv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find the latest checkpoint file in the work directory\n",
    "latest_checkpoint = max([os.path.join(cfg.work_dir, f) for f in os.listdir(cfg.work_dir) if f.endswith('.pth')], key=os.path.getctime)\n",
    "print(f\"Using checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# Build the model from config and checkpoint\n",
    "model = init_detector(custom_config_path, latest_checkpoint, device='cuda:0') # Use GPU\n",
    "\n",
    "# Select a sample image for inference (e.g., the first validation image)\n",
    "sample_image_path = os.path.join(valid_img_dir, os.listdir(valid_img_dir)[0]) # Get first image in valid dir\n",
    "print(f\"Running inference on: {sample_image_path}\")\n",
    "\n",
    "# Run inference\n",
    "result = inference_detector(model, sample_image_path)\n",
    "\n",
    "# Visualize the results (optional)\n",
    "# The 'show_result_pyplot' function can draw bboxes and masks\n",
    "# It might need the class names from your dataset\n",
    "# Let's try to get class names if possible\n",
    "class_names = []\n",
    "if os.path.exists(valid_annot_path):\n",
    "    with open(valid_annot_path, 'r') as f:\n",
    "        valid_data = json.load(f)\n",
    "    categories = valid_data.get('categories', [])\n",
    "    class_names = [cat['name'] for cat in categories]\n",
    "else: # Fallback if annotation file fails\n",
    "     class_names = ['farm'] * num_classes # Assume based on previous logic\n",
    "\n",
    "print(f\"Using class names for visualization: {class_names}\")\n",
    "\n",
    "# Create a Matplotlib figure context\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Visualize detection results\n",
    "model.show_result(\n",
    "    sample_image_path,\n",
    "    result,\n",
    "    score_thr=0.3, # Threshold to filter results\n",
    "    show=True, # Show in the notebook output\n",
    "    wait_time=0,\n",
    "    # class_names=class_names # Pass class names if needed by the specific show_result implementation\n",
    "    out_file=None # Don't save to file, display directly\n",
    ")\n",
    "# Due to interactive nature, sometimes direct plt.show() might be needed if image doesn't appear\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5afd5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Next Steps (Deployment & Size Classification)\n",
    "\n",
    "1.  **Refine Training:** Adjust hyperparameters (learning rate, epochs, augmentations) in the config file based on initial results to improve model performance (mAP).\n",
    "2.  **Size Calculation:** Develop a post-processing script that takes the inference results (masks from `result` object), calculates the pixel area of each mask, and (if GSD is available) converts it to real-world area.\n",
    "3.  **Size Classification:** Apply thresholds to the calculated areas to assign size categories (e.g., Small, Medium, Large Plot).\n",
    "4.  **Deployment:** Package the trained model (`.pth` file) and the inference/post-processing script for deployment in the target environment (e.g., web service using Flask/FastAPI, integrated with satellite imagery API). Ensure the deployment environment handles fetching GSD information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c823899",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.925885,
   "end_time": "2025-04-07T10:04:15.151305",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T10:04:03.225420",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
