{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdb79df",
   "metadata": {},
   "source": [
    "# FarmWise: Farmland Segmentation and Size Classification with YOLOv8\n",
    "\n",
    "**Date**: April 14, 2025\n",
    "\n",
    "This notebook implements a farm segmentation system using the YOLOv8 architecture to identify agricultural fields from satellite imagery, calculate their sizes, and classify them for targeted recommendations.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal**: Create a system that can:\n",
    "1. Detect and segment farmlands from satellite imagery using YOLOv8\n",
    "2. Calculate the size/area of each identified farm\n",
    "3. Classify farms by size (small, medium, large)\n",
    "4. Enable a recommendation system based on farm size classification\n",
    "\n",
    "**Approach**: YOLOv8 architecture for instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32e0b7",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Agricultural recommendations are most effective when tailored to the specific context of a farm, with farm size being a crucial factor. Large farms may benefit from different techniques, equipment, and crop selections compared to small ones. This project aims to automatically classify farms by size from satellite imagery using YOLOv8 segmentation to enable targeted recommendations.\n",
    "\n",
    "### 1.2 Success Criteria\n",
    "\n",
    "- **Technical Success**: Achieve high accuracy in farmland segmentation (e.g., mAP50-95 for segmentation > 0.5)\n",
    "- **Business Success**: Enable accurate size-based classification of farms for targeted recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0f576",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Understanding\n",
    "\n",
    "### 2.1 Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4dd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages: ultralytics for YOLOv8, roboflow for data download, and others\n",
    "!pip install torch torchvision torchaudio ultralytics roboflow opencv-python matplotlib numpy pillow scikit-learn scikit-image tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from tqdm.notebook import tqdm\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO # Import YOLO\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility (less critical for YOLO training itself, but good practice)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability and set up CUDA device\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device('cuda:0') # YOLO typically uses device 0 by default, but can be specified\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU. Training will be significantly slower.\")\n",
    "\n",
    "# Display CUDA version if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a57f6",
   "metadata": {},
   "source": [
    "### 2.2 Data Acquisition from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64caafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Roboflow and load dataset\n",
    "# Note: You will need to provide your Roboflow API key\n",
    "try:\n",
    "    rf = Roboflow(api_key=\"HE9CEH5JxJ3U0vXrQTOy\")  # Replace with your actual API key\n",
    "    project = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\n",
    "    # Ensure you download the 'yolov8' format if available, or a compatible segmentation format\n",
    "    dataset = project.version(1).download(\"yolov8\")\n",
    "    dataset_path = dataset.location\n",
    "    data_yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    print(f\"Data YAML path: {data_yaml_path}\")\n",
    "    # Verify data.yaml exists\n",
    "    if not os.path.exists(data_yaml_path):\n",
    "        print(\"\\nERROR: data.yaml not found in the downloaded dataset location!\")\n",
    "        print(\"YOLOv8 training requires this file. Please check the download format and location.\")\n",
    "        data_yaml_path = None # Prevent use of non-existent path\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset from Roboflow: {e}\")\n",
    "    print(\"Please check your API key and project details.\")\n",
    "    dataset_path = None\n",
    "    data_yaml_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec85ec",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Exploration (Optional)\n",
    "YOLOv8 handles data loading internally, but we can still explore the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "def explore_directory(path, level=0):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory not found: {path}\")\n",
    "        return\n",
    "    print('  ' * level + f\"|-- {os.path.basename(path)}\")\n",
    "    if os.path.isdir(path):\n",
    "        items = os.listdir(path)\n",
    "        count = 0\n",
    "        for item in items:\n",
    "            if count >= 10 and level > 0: # Limit display depth for subdirs\n",
    "                 break\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                explore_directory(item_path, level + 1)\n",
    "            else:\n",
    "                print('  ' * (level + 1) + f\"|-- {item}\")\n",
    "            count += 1\n",
    "        if len(items) > count:\n",
    "            print('  ' * (level + 1) + f\"|-- ... ({len(items) - count} more items)\")\n",
    "\n",
    "if dataset_path:\n",
    "    print(\"Dataset Structure:\")\n",
    "    explore_directory(dataset_path)\n",
    "else:\n",
    "    print(\"Dataset path not defined. Skipping exploration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90d870",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize some sample images and masks (Enhanced for Segmentation Focus)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m \u001b[38;5;66;03m# Ensure yaml is imported if not already\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "# Visualize some sample images with their annotations (using YOLO format)\n",
    "import random\n",
    "\n",
    "def visualize_yolo_samples(data_yaml_path, num_samples=3):\n",
    "    if not data_yaml_path or not os.path.exists(data_yaml_path):\n",
    "        print(\"Cannot visualize samples: data.yaml path is invalid.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_cfg = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data.yaml: {e}\")\n",
    "        return\n",
    "\n",
    "    # Construct paths relative to the YAML file location\n",
    "    base_dir = os.path.dirname(data_yaml_path)\n",
    "    train_img_dir = os.path.join(base_dir, data_cfg.get('train', 'train/images'))\n",
    "    train_label_dir = os.path.join(base_dir, data_cfg.get('train', '').replace('images', 'labels')) # Heuristic\n",
    "\n",
    "    if not os.path.isdir(train_img_dir) or not os.path.isdir(train_label_dir):\n",
    "        print(f\"Error: Training image or label directory not found.\")\n",
    "        print(f\"Checked img: {train_img_dir}\")\n",
    "        print(f\"Checked label: {train_label_dir}\")\n",
    "        # Try alternative common structures if the first guess failed\n",
    "        train_img_dir = os.path.join(base_dir, 'train', 'images')\n",
    "        train_label_dir = os.path.join(base_dir, 'train', 'labels')\n",
    "        if not os.path.isdir(train_img_dir) or not os.path.isdir(train_label_dir):\n",
    "             print(\"Alternative paths also failed. Cannot visualize.\")\n",
    "             return\n",
    "        else:\n",
    "             print(\"Using alternative paths: train/images and train/labels\")\n",
    "\n",
    "    class_names = data_cfg.get('names', ['Unknown'])\n",
    "    print(f\"Class names: {class_names}\")\n",
    "\n",
    "    img_files = [f for f in os.listdir(train_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not img_files:\n",
    "        print(f\"No images found in {train_img_dir}\")\n",
    "        return\n",
    "\n",
    "    sample_files = random.sample(img_files, min(num_samples, len(img_files)))\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        img_path = os.path.join(train_img_dir, img_file)\n",
    "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        label_path = os.path.join(train_label_dir, label_file)\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            h, w, _ = img.shape\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        img_draw = img.copy() # Create a copy to draw on\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "\n",
    "                class_id = int(parts[0])\n",
    "                class_name = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n",
    "                color = plt.cm.get_cmap('tab10')(class_id % 10)[:3] # Get a color\n",
    "                color = tuple(int(c * 255) for c in color)\n",
    "\n",
    "                # Check for segmentation format (class_id x1 y1 x2 y2 ... xN yN)\n",
    "                if len(parts) > 5 and len(parts) % 2 == 1:\n",
    "                    points_norm = np.array([float(p) for p in parts[1:]]).reshape(-1, 2)\n",
    "                    points_pixel = (points_norm * np.array([w, h])).astype(np.int32)\n",
    "                    cv2.polylines(img_draw, [points_pixel], isClosed=True, color=color, thickness=2)\n",
    "                    # Optionally fill polygon\n",
    "                    # overlay = img_draw.copy()\n",
    "                    # cv2.fillPoly(overlay, [points_pixel], color)\n",
    "                    # alpha = 0.4\n",
    "                    # img_draw = cv2.addWeighted(overlay, alpha, img_draw, 1 - alpha, 0)\n",
    "                    label_pos = points_pixel.min(axis=0)\n",
    "                    cv2.putText(img_draw, class_name, (label_pos[0], max(0, label_pos[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                # Check for detection format (class_id cx cy w h)\n",
    "                elif len(parts) == 5:\n",
    "                    cx, cy, bw, bh = map(float, parts[1:])\n",
    "                    x1 = int((cx - bw / 2) * w)\n",
    "                    y1 = int((cy - bh / 2) * h)\n",
    "                    x2 = int((cx + bw / 2) * w)\n",
    "                    y2 = int((cy + bh / 2) * h)\n",
    "                    cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(img_draw, class_name, (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        plt.imshow(img_draw)\n",
    "        plt.title(f\"Image: {img_file}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "visualize_yolo_samples(data_yaml_path, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecd8c6",
   "metadata": {},
   "source": [
    "### 2.4 Data Preparation (Handled by YOLOv8)\n",
    "\n",
    "YOLOv8 handles data loading, transformations, and augmentation internally based on the `data.yaml` file and training arguments. We don't need a custom `Dataset` or `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e34e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained the FarmlandDataset class and visualization.\n",
    "# It is no longer needed as YOLOv8 handles data loading.\n",
    "print(\"Data preparation is handled internally by YOLOv8 based on the data.yaml file.\")\n",
    "print(\"Ensure the paths in data.yaml are correct relative to its location.\")\n",
    "\n",
    "# Verify the content of data.yaml\n",
    "if data_yaml_path and os.path.exists(data_yaml_path):\n",
    "    print(\"\\n--- Contents of data.yaml ---\")\n",
    "    try:\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            print(f.read())\n",
    "        print(\"---------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data.yaml: {e}\")\n",
    "else:\n",
    "    print(\"\\nWarning: data.yaml not found or path is invalid. Cannot verify contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ced9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained DataLoader setup and transformations.\n",
    "# It is no longer needed for YOLOv8 training.\n",
    "print(\"DataLoader setup and transformations are managed by YOLOv8 during training.\")\n",
    "print(\"Augmentations can be configured via arguments in the model.train() call.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e03722",
   "metadata": {},
   "source": [
    "## 3. Modeling with YOLOv8\n",
    "\n",
    "### 3.1 Initialize YOLOv8 Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a YOLOv8 segmentation model\n",
    "# We can start with a pretrained model like 'yolov8n-seg.pt'\n",
    "# Other options: yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt\n",
    "try:\n",
    "    model = YOLO('yolov8n-seg.pt')  # Load a pretrained segmentation model\n",
    "    print(\"YOLOv8 segmentation model loaded successfully.\")\n",
    "    # Move model to the appropriate device (though YOLO often handles this internally)\n",
    "    model.to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLO model: {e}\")\n",
    "    print(\"Ensure 'yolov8n-seg.pt' is downloadable or provide a local path.\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c16ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously initialized the U-Net model and optimizer.\n",
    "# It is no longer needed for YOLOv8.\n",
    "print(\"YOLOv8 model initialization done in the previous cell.\")\n",
    "print(\"Optimizer and loss are handled internally by YOLOv8's train method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d17c6",
   "metadata": {},
   "source": [
    "### 3.2 Training the YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the YOLOv8 model\n",
    "\n",
    "if model and data_yaml_path:\n",
    "    print(f\"Starting YOLOv8 training with data: {data_yaml_path}\")\n",
    "    try:\n",
    "        # Define training parameters\n",
    "        epochs = 30 # Adjust as needed\n",
    "        img_size = 640 # YOLOv8 default, adjust based on dataset/GPU memory\n",
    "        batch_size = 16 # Adjust based on GPU memory (e.g., 8, 16, 32). Use -1 for auto-batch.\n",
    "\n",
    "        # Start training\n",
    "        results = model.train(\n",
    "            data=data_yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=img_size,\n",
    "            batch=batch_size,\n",
    "            device=0 if torch.cuda.is_available() else 'cpu', # Specify device\n",
    "            project='FarmWise_YOLOv8_Training', # Project folder for results\n",
    "            name='exp', # Experiment name (subfolder)\n",
    "            exist_ok=True, # Overwrite existing experiment folder\n",
    "            # Add other augmentations/parameters as needed:\n",
    "            # degrees=10, # Random rotation\n",
    "            # flipud=0.5, # Random vertical flip\n",
    "            # mosaic=1.0, # Mosaic augmentation (usually enabled by default)\n",
    "            # patience=10 # Early stopping patience\n",
    "        )\n",
    "        print(\"YOLOv8 training completed.\")\n",
    "        print(f\"Training results saved in: {results.save_dir}\")\n",
    "        # The best model weights are typically saved as 'best.pt' in the experiment folder\n",
    "        best_model_path = os.path.join(results.save_dir, 'weights', 'best.pt')\n",
    "        print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "        # Load the best model for subsequent steps\n",
    "        model = YOLO(best_model_path)\n",
    "        model.to(device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during YOLOv8 training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        best_model_path = None\n",
    "else:\n",
    "    print(\"Skipping training: Model or data.yaml path not available.\")\n",
    "    best_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a111f",
   "metadata": {},
   "source": [
    "### 3.3 Validation (Optional)\n",
    "YOLOv8 automatically validates during training. We can also run validation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the trained model (optional, as validation happens during training)\n",
    "if model and data_yaml_path and best_model_path:\n",
    "    print(\"\\nRunning validation on the best model...\")\n",
    "    try:\n",
    "        metrics = model.val(\n",
    "            data=data_yaml_path,\n",
    "            imgsz=640, # Use the same image size as training\n",
    "            split='val' # Specify the validation split\n",
    "        )\n",
    "        print(\"Validation Metrics:\")\n",
    "        # Access specific metrics, e.g., segmentation mAP\n",
    "        print(f\"  mAP50-95(Seg): {metrics.seg.map:.4f}\")\n",
    "        print(f\"  mAP50(Seg): {metrics.seg.map50:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during validation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping validation: Model or data.yaml path not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eca62d",
   "metadata": {},
   "source": [
    "### 3.4 Visualize Training Results\n",
    "YOLOv8 saves training plots (like loss curves, metrics) in the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbe1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results plots saved by YOLOv8\n",
    "results_dir = None\n",
    "if 'results' in locals() and hasattr(results, 'save_dir'):\n",
    "    results_dir = results.save_dir\n",
    "elif best_model_path:\n",
    "    # Try to infer results dir from best_model_path\n",
    "    results_dir = os.path.dirname(os.path.dirname(best_model_path))\n",
    "\n",
    "if results_dir and os.path.isdir(results_dir):\n",
    "    print(f\"Displaying results from: {results_dir}\")\n",
    "    results_png_path = os.path.join(results_dir, 'results.png')\n",
    "    confusion_matrix_path = os.path.join(results_dir, 'confusion_matrix.png')\n",
    "\n",
    "    if os.path.exists(results_png_path):\n",
    "        print(\"\\n--- Training Metrics Plot ---\")\n",
    "        display(Image.open(results_png_path))\n",
    "    else:\n",
    "        print(f\"results.png not found in {results_dir}\")\n",
    "\n",
    "    if os.path.exists(confusion_matrix_path):\n",
    "        print(\"\\n--- Confusion Matrix ---\")\n",
    "        display(Image.open(confusion_matrix_path))\n",
    "    # else:\n",
    "        # print(f\"confusion_matrix.png not found in {results_dir}\")\n",
    "else:\n",
    "    print(\"Could not find YOLOv8 results directory. Cannot display plots.\")\n",
    "    print(\"Check the 'FarmWise_YOLOv8_Training/exp*' folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448580ef",
   "metadata": {},
   "source": [
    "## 4. Farm Size Calculation and Classification\n",
    "\n",
    "Now we'll use our trained YOLOv8 model to segment farms and calculate their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f254cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the best model is loaded\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    model = YOLO(best_model_path)\n",
    "    model.to(device)\n",
    "    print(f\"Loaded best model from {best_model_path}\")\n",
    "elif 'model' not in locals() or model is None:\n",
    "    print(\"Error: No trained YOLOv8 model available.\")\n",
    "    # Attempt to load a default if needed, but this likely won't be trained on the specific task\n",
    "    # model = YOLO('yolov8n-seg.pt')\n",
    "    # model.to(device)\n",
    "else:\n",
    "    print(\"Using the model currently in memory (might not be the best one if training failed).\")\n",
    "\n",
    "# Function to segment farms in an image using YOLOv8\n",
    "def segment_farms_yolo(model, image_path, confidence_threshold=0.25):\n",
    "    if model is None:\n",
    "        print(\"Model not available for segmentation.\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        # Load image using PIL for original size info\n",
    "        img_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        original_size = img_pil.size # (width, height)\n",
    "\n",
    "        # Perform prediction\n",
    "        results = model.predict(image_path, conf=confidence_threshold, device=device)\n",
    "\n",
    "        # Check if results were obtained\n",
    "        if not results or len(results) == 0:\n",
    "            print(\"No results returned from model prediction.\")\n",
    "            return None, np.array(img_pil), None\n",
    "\n",
    "        # Assuming results[0] contains the prediction for the single image\n",
    "        pred_result = results[0]\n",
    "\n",
    "        # Combine masks from all detected instances into a single binary mask\n",
    "        combined_mask = np.zeros(pred_result.orig_shape, dtype=np.uint8)\n",
    "\n",
    "        if pred_result.masks is not None:\n",
    "            print(f\"Found {len(pred_result.masks)} potential farm segments.\")\n",
    "            for i, mask_data in enumerate(pred_result.masks):\n",
    "                # The mask data might need resizing if prediction was done at a different size\n",
    "                # Access the mask array (assuming it's directly available or via .data)\n",
    "                mask_tensor = mask_data.data.squeeze() # Get the mask tensor [H, W]\n",
    "                mask_np = mask_tensor.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                # Resize mask to original image size if necessary\n",
    "                if mask_np.shape != pred_result.orig_shape:\n",
    "                     mask_np_resized = cv2.resize(mask_np, (original_size[0], original_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "                else:\n",
    "                     mask_np_resized = mask_np\n",
    "\n",
    "                # Add this mask to the combined mask (use bitwise OR)\n",
    "                combined_mask = cv2.bitwise_or(combined_mask, mask_np_resized * 255) # Multiply by 255 if mask is 0/1\n",
    "        else:\n",
    "            print(\"No segmentation masks found in the prediction results.\")\n",
    "\n",
    "        return combined_mask, np.array(img_pil), pred_result # Return raw results too\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO segmentation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Try to load image anyway for context\n",
    "        try:\n",
    "            img_pil = Image.open(image_path).convert(\"RGB\")\n",
    "            return None, np.array(img_pil), None\n",
    "        except:\n",
    "            return None, None, None\n",
    "\n",
    "# Function to calculate farm sizes and classify them (remains largely the same)\n",
    "# Takes the combined binary mask as input\n",
    "def calculate_farm_sizes(binary_mask, pixels_per_meter=None):\n",
    "    if binary_mask is None:\n",
    "        return [], None\n",
    "    # Use connected component analysis to identify individual farms\n",
    "    # Ensure mask is binary (0 or 255)\n",
    "    binary_mask_01 = (binary_mask > 128).astype(np.uint8)\n",
    "    labeled_mask, num_farms = measure.label(binary_mask_01, connectivity=2, return_num=True)\n",
    "\n",
    "    print(f\"Found {num_farms} connected components.\")\n",
    "\n",
    "    # Calculate properties of each labeled region\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "\n",
    "    # Store farm areas\n",
    "    farm_areas = []\n",
    "\n",
    "    for region in regions:\n",
    "        # Skip very small regions (likely noise)\n",
    "        if region.area < 100:  # Adjust threshold as needed\n",
    "            continue\n",
    "\n",
    "        # Calculate area in pixels\n",
    "        area_pixels = region.area\n",
    "\n",
    "        # Convert to real-world units if pixels_per_meter is provided\n",
    "        if pixels_per_meter is not None:\n",
    "            area_sq_meters = area_pixels / (pixels_per_meter ** 2)\n",
    "            # Convert to hectares (1 hectare = 10,000 sq meters)\n",
    "            area_hectares = area_sq_meters / 10000\n",
    "            farm_areas.append(area_hectares)\n",
    "        else:\n",
    "            farm_areas.append(area_pixels)\n",
    "\n",
    "    print(f\"Calculated areas for {len(farm_areas)} farms (after filtering small regions).\")\n",
    "    return farm_areas, labeled_mask\n",
    "\n",
    "# Function to classify farms by size (remains the same)\n",
    "def classify_farms(farm_areas, unit='pixels'):\n",
    "    # Define size thresholds (adjust based on your specific context)\n",
    "    if unit == 'hectares':\n",
    "        # Real-world thresholds (in hectares)\n",
    "        small_threshold = 10     # 0-10 hectares = small farm\n",
    "        medium_threshold = 50    # 10-50 hectares = medium farm\n",
    "        # > 50 hectares = large farm\n",
    "    else:\n",
    "        # Pixel-based thresholds (adjust based on your image resolution)\n",
    "        small_threshold = 5000     # 0-5000 pixels = small farm\n",
    "        medium_threshold = 20000   # 5000-20000 pixels = medium farm\n",
    "        # > 20000 pixels = large farm\n",
    "\n",
    "    # Classify each farm\n",
    "    farm_classes = []\n",
    "    for area in farm_areas:\n",
    "        if area < small_threshold:\n",
    "            farm_classes.append('Small')\n",
    "        elif area < medium_threshold:\n",
    "            farm_classes.append('Medium')\n",
    "        else:\n",
    "            farm_classes.append('Large')\n",
    "\n",
    "    # Count farms in each category\n",
    "    class_counts = {\n",
    "        'Small': farm_classes.count('Small'),\n",
    "        'Medium': farm_classes.count('Medium'),\n",
    "        'Large': farm_classes.count('Large')\n",
    "    }\n",
    "\n",
    "    return farm_classes, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize farm segmentation and classification (using labeled mask)\n",
    "def visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes):\n",
    "    if image is None or labeled_mask is None:\n",
    "        print(\"Cannot visualize: Image or labeled mask is missing.\")\n",
    "        return\n",
    "\n",
    "    # Create a colormap for visualization\n",
    "    # Use specific colors for classes\n",
    "    colors = {\n",
    "        'Small': [0, 255, 0, 128], # Green\n",
    "        'Medium': [255, 255, 0, 128], # Yellow\n",
    "        'Large': [255, 0, 0, 128] # Red\n",
    "    }\n",
    "    legend_colors = {'Small': 'green', 'Medium': 'yellow', 'Large': 'red'}\n",
    "\n",
    "    # Create a colored overlay based on farm classification\n",
    "    overlay = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
    "\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "    valid_region_indices = [i for i, r in enumerate(regions) if r.area >= 100] # Indices of farms that passed area threshold\n",
    "\n",
    "    if len(valid_region_indices) != len(farm_classes):\n",
    "        print(f\"Warning: Mismatch between number of classified farms ({len(farm_classes)}) and valid regions ({len(valid_region_indices)}). Visualization might be incomplete.\")\n",
    "        # Attempt to proceed, assuming farm_classes corresponds to the filtered regions\n",
    "\n",
    "    class_idx = 0\n",
    "    for i, region in enumerate(regions):\n",
    "        if region.area < 100:\n",
    "            continue\n",
    "\n",
    "        if class_idx < len(farm_classes):\n",
    "            farm_class = farm_classes[class_idx]\n",
    "            color_value = colors.get(farm_class, [128, 128, 128, 128]) # Default gray\n",
    "            # Fill region with corresponding color\n",
    "            coords = region.coords\n",
    "            overlay[coords[:, 0], coords[:, 1]] = color_value\n",
    "            class_idx += 1\n",
    "        else:\n",
    "            # Handle mismatch if necessary\n",
    "            print(f\"Warning: No class found for region {i+1}. Skipping color.\")\n",
    "\n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Original image with segmentation overlay\n",
    "    ax1.imshow(image)\n",
    "    ax1.imshow(overlay)\n",
    "    ax1.set_title('Farm Segmentation and Classification')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=legend_colors['Small'], alpha=0.5, label='Small Farms'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=legend_colors['Medium'], alpha=0.5, label='Medium Farms'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color=legend_colors['Large'], alpha=0.5, label='Large Farms')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    # Pie chart of farm size distribution\n",
    "    class_counts = {\n",
    "        'Small': farm_classes.count('Small'),\n",
    "        'Medium': farm_classes.count('Medium'),\n",
    "        'Large': farm_classes.count('Large')\n",
    "    }\n",
    "\n",
    "    if sum(class_counts.values()) > 0:  # Check if we have any farms\n",
    "        labels = list(class_counts.keys())\n",
    "        sizes = list(class_counts.values())\n",
    "        pie_colors = [legend_colors[l] for l in labels]\n",
    "\n",
    "        ax2.pie(sizes, labels=labels, colors=pie_colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        ax2.set_title('Farm Size Distribution')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No farms detected or classified', horizontalalignment='center', verticalalignment='center')\n",
    "        ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample image using YOLOv8\n",
    "def process_sample_image_yolo(image_path, pixel_scale=None):\n",
    "    if not model:\n",
    "        print(\"Model not available. Cannot process image.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Segment farms using YOLOv8\n",
    "    # Use a lower confidence if needed to capture more potential segments\n",
    "    binary_mask, image, raw_results = segment_farms_yolo(model, image_path, confidence_threshold=0.25)\n",
    "\n",
    "    if image is None:\n",
    "        print(\"Failed to load or process image.\")\n",
    "        return None, None, None\n",
    "\n",
    "    if binary_mask is None:\n",
    "        print(\"Segmentation failed or produced no mask.\")\n",
    "        # Show the original image anyway\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Original Image (Segmentation Failed)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return [], [], {'Small': 0, 'Medium': 0, 'Large': 0}\n",
    "\n",
    "    # Calculate farm sizes\n",
    "    unit = 'hectares' if pixel_scale else 'pixels'\n",
    "    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask, pixel_scale)\n",
    "\n",
    "    # Classify farms\n",
    "    farm_classes, class_counts = classify_farms(farm_areas, unit=unit)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n--- Analysis for {os.path.basename(image_path)} ---\")\n",
    "    print(f\"Number of farms detected and classified: {len(farm_areas)}\")\n",
    "    print(f\"Farm size classification ({unit}): {class_counts}\")\n",
    "\n",
    "    # Visualize results using the labeled mask from connected components\n",
    "    visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n",
    "\n",
    "    # Optional: Visualize raw YOLO predictions\n",
    "    if raw_results:\n",
    "        print(\"\\n--- Raw YOLOv8 Prediction Visualization ---\")\n",
    "        try:\n",
    "            # Use YOLO's built-in plotting\n",
    "            pred_plot = raw_results.plot() # Returns numpy array (BGR)\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(cv2.cvtColor(pred_plot, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Raw YOLOv8 Detections/Segmentations\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        except Exception as plot_e:\n",
    "            print(f\"Could not plot raw YOLO results: {plot_e}\")\n",
    "\n",
    "    return farm_areas, farm_classes, class_counts\n",
    "\n",
    "# Try with a test image from the validation set\n",
    "test_img_dir = None\n",
    "if dataset_path:\n",
    "    # Construct path relative to dataset location\n",
    "    base_dir = dataset_path\n",
    "    # Try finding validation images path from data.yaml if possible\n",
    "    val_img_path_rel = 'valid/images' # Default guess\n",
    "    if data_yaml_path and os.path.exists(data_yaml_path):\n",
    "        try:\n",
    "            with open(data_yaml_path, 'r') as f:\n",
    "                data_cfg = yaml.safe_load(f)\n",
    "                val_img_path_rel = data_cfg.get('val', val_img_path_rel)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read val path from data.yaml: {e}\")\n",
    "    test_img_dir = os.path.join(base_dir, val_img_path_rel)\n",
    "\n",
    "if test_img_dir and os.path.isdir(test_img_dir):\n",
    "    test_img_files = [f for f in os.listdir(test_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if test_img_files:\n",
    "        # Select a random image or the first one\n",
    "        # test_img_path = os.path.join(test_img_dir, random.choice(test_img_files))\n",
    "        test_img_path = os.path.join(test_img_dir, test_img_files[0])\n",
    "        print(f\"\\nProcessing test image: {test_img_path}\")\n",
    "        # Note: Provide pixel_scale if known, e.g., pixel_scale=0.5 (meters per pixel)\n",
    "        farm_areas, farm_classes, class_counts = process_sample_image_yolo(test_img_path, pixel_scale=None)\n",
    "    else:\n",
    "        print(f\"No image files found in validation directory: {test_img_dir}\")\n",
    "else:\n",
    "    print(\"Validation image directory not found or dataset path not set. Cannot process test image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6babe",
   "metadata": {},
   "source": [
    "## 5. Recommendation System Based on Farm Size\n",
    "(This section remains the same as it depends on the classification output, not the model type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recommendations based on farm size\n",
    "def get_recommendations_by_size(farm_size):\n",
    "    recommendations = {\n",
    "        'Small': {\n",
    "            'Crop Selection': [\n",
    "                'Focus on high-value crops (e.g., specialty vegetables, herbs, berries)',\n",
    "                'Consider intercropping to maximize land use',\n",
    "                'Explore vertical farming techniques for space optimization'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in versatile, small-scale equipment',\n",
    "                'Consider equipment sharing programs or cooperatives',\n",
    "                'Focus on precision hand tools for specialized tasks'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Direct-to-consumer sales (farmers markets, CSA)',\n",
    "                'Develop value-added products',\n",
    "                'Leverage organic or specialty certifications'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement intensive organic practices',\n",
    "                'Consider agroecological approaches',\n",
    "                'Explore permaculture design principles'\n",
    "            ]\n",
    "        },\n",
    "        'Medium': {\n",
    "            'Crop Selection': [\n",
    "                'Balance between specialty and commodity crops',\n",
    "                'Consider crop rotation systems',\n",
    "                'Explore diversification strategies'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in mid-sized tractors and implements',\n",
    "                'Consider precision agriculture technology',\n",
    "                'Develop efficient irrigation systems'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Develop relationships with local wholesalers and restaurants',\n",
    "                'Consider cooperative marketing',\n",
    "                'Explore agritourism opportunities'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement integrated pest management',\n",
    "                'Consider conservation tillage practices',\n",
    "                'Develop soil health management plans'\n",
    "            ]\n",
    "        },\n",
    "        'Large': {\n",
    "            'Crop Selection': [\n",
    "                'Focus on efficient production of commodity crops',\n",
    "                'Consider dedicating portions to specialty high-value crops',\n",
    "                'Implement strategic crop rotation systems'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in large-scale, efficient machinery',\n",
    "                'Implement precision agriculture and automation',\n",
    "                'Consider GPS guidance systems and variable rate technology'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Develop contracts with processors and distributors',\n",
    "                'Consider futures markets and hedging strategies',\n",
    "                'Explore export opportunities'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement conservation agriculture practices at scale',\n",
    "                'Consider renewable energy investments',\n",
    "                'Develop comprehensive nutrient management plans'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return recommendations.get(farm_size, {})\n",
    "\n",
    "# Function to display recommendations for a specific farm\n",
    "def display_farm_recommendations(farm_class):\n",
    "    recommendations = get_recommendations_by_size(farm_class)\n",
    "\n",
    "    if not recommendations:\n",
    "        print(f\"No recommendations available for {farm_class} farms.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Recommendations for {farm_class} Farms ===\\n\")\n",
    "\n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recommendations for each farm size category\n",
    "for size in ['Small', 'Medium', 'Large']:\n",
    "    display_farm_recommendations(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705153ea",
   "metadata": {},
   "source": [
    "## 6. End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end pipeline function using YOLOv8\n",
    "def process_farm_image_pipeline(image_path, pixel_scale=None):\n",
    "    \"\"\"\n",
    "    Process a satellite image using YOLOv8 to detect farms, classify them by size,\n",
    "    and provide recommendations.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the satellite image\n",
    "        pixel_scale (float, optional): Scale factor in meters per pixel (if available)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the analysis\n",
    "    \"\"\"\n",
    "    if not model:\n",
    "        print(\"Model not available. Cannot run pipeline.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n=== Running End-to-End Pipeline for: {image_path} ===\")\n",
    "\n",
    "    # Step 1: Segment farms using the trained YOLOv8 model\n",
    "    binary_mask, image, raw_results = segment_farms_yolo(model, image_path, confidence_threshold=0.25)\n",
    "\n",
    "    if image is None:\n",
    "        print(\"Pipeline failed: Could not load or process image.\")\n",
    "        return None\n",
    "    if binary_mask is None:\n",
    "        print(\"Pipeline failed: Segmentation did not produce a mask.\")\n",
    "        # Optionally show the original image\n",
    "        plt.imshow(image); plt.title(\"Original Image (Segmentation Failed)\"); plt.axis('off'); plt.show()\n",
    "        return {'num_farms': 0, 'farm_areas': [], 'farm_classes': [], 'class_counts': {'Small': 0, 'Medium': 0, 'Large': 0}, 'predominant_size': None}\n",
    "\n",
    "    # Step 2: Calculate farm sizes\n",
    "    unit = 'hectares' if pixel_scale else 'pixels'\n",
    "    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask, pixel_scale)\n",
    "\n",
    "    # Step 3: Classify farms by size\n",
    "    farm_classes, class_counts = classify_farms(farm_areas, unit)\n",
    "\n",
    "    # Step 4: Print summary\n",
    "    print(f\"\\nFarm Analysis Summary:\")\n",
    "    print(f\"Total farms detected and classified: {len(farm_areas)}\")\n",
    "    print(f\"Farm size distribution ({unit}): {class_counts}\")\n",
    "\n",
    "    # Step 5: Calculate predominant farm size\n",
    "    predominant_size = None\n",
    "    if len(farm_areas) > 0:\n",
    "        # Find the class with the highest count\n",
    "        predominant_size = max(class_counts, key=class_counts.get)\n",
    "        # Handle cases where counts might be zero or equal\n",
    "        if class_counts[predominant_size] == 0:\n",
    "             predominant_size = None # No farms actually classified\n",
    "        else:\n",
    "             print(f\"Predominant farm size: {predominant_size}\")\n",
    "             # Step 6: Provide recommendations based on predominant farm size\n",
    "             display_farm_recommendations(predominant_size)\n",
    "    else:\n",
    "        print(\"No farms detected or classified in the image.\")\n",
    "\n",
    "    # Step 7: Visualize results\n",
    "    if len(farm_areas) > 0:\n",
    "        visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n",
    "    else:\n",
    "        # If no farms, show the original image and the (likely empty) binary mask\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(image); axes[0].set_title(\"Original Image\"); axes[0].axis('off')\n",
    "        axes[1].imshow(binary_mask, cmap='gray'); axes[1].set_title(\"Segmentation Mask (No Farms Found)\"); axes[1].axis('off')\n",
    "        plt.suptitle(\"No Farms Detected or Classified\")\n",
    "        plt.show()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    results_dict = {\n",
    "        'num_farms': len(farm_areas),\n",
    "        'farm_areas': farm_areas,\n",
    "        'farm_classes': farm_classes,\n",
    "        'class_counts': class_counts,\n",
    "        'predominant_size': predominant_size\n",
    "    }\n",
    "\n",
    "    # Save results to JSON\n",
    "    try:\n",
    "        results_filename = f\"farm_analysis_{os.path.splitext(os.path.basename(image_path))[0]}.json\"\n",
    "        with open(results_filename, 'w') as f:\n",
    "            # Convert numpy types if necessary\n",
    "            serializable_results = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in results_dict.items()}\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "        print(f\"\\nAnalysis results saved to {results_filename}\")\n",
    "    except Exception as json_e:\n",
    "        print(f\"\\nError saving results to JSON: {json_e}\")\n",
    "\n",
    "    print(f\"=== Pipeline Finished for: {image_path} ===\")\n",
    "    return results_dict\n",
    "\n",
    "# Test the end-to-end pipeline on a sample image (if available)\n",
    "if test_img_dir and os.path.isdir(test_img_dir):\n",
    "    test_img_files = [f for f in os.listdir(test_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if test_img_files:\n",
    "        # Process the first validation image\n",
    "        test_img_path_pipeline = os.path.join(test_img_dir, test_img_files[0])\n",
    "        pipeline_results = process_farm_image_pipeline(test_img_path_pipeline, pixel_scale=None)\n",
    "        # Process another image if available\n",
    "        if len(test_img_files) > 1:\n",
    "             test_img_path_pipeline_2 = os.path.join(test_img_dir, test_img_files[1])\n",
    "             pipeline_results_2 = process_farm_image_pipeline(test_img_path_pipeline_2, pixel_scale=None)\n",
    "    else:\n",
    "        print(\"No validation images found to test the pipeline.\")\n",
    "else:\n",
    "    print(\"Validation image directory not found. Cannot test pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346b6a0",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "### 7.1 Summary\n",
    "\n",
    "In this project, we have:\n",
    "1. Loaded and processed a dataset of satellite imagery with farm annotations in YOLO format.\n",
    "2. Initialized and trained a YOLOv8 segmentation model for farmland detection.\n",
    "3. Developed methods to extract segmentation masks from YOLOv8 predictions and calculate farm sizes using connected components.\n",
    "4. Created a classification system to categorize farms by size (Small, Medium, Large).\n",
    "5. Built a recommendation system providing tailored advice based on farm size.\n",
    "6. Integrated all components into an end-to-end pipeline for processing new images.\n",
    "\n",
    "### 7.2 Limitations\n",
    "\n",
    "Current limitations of the system include:\n",
    "- Segmentation accuracy depends heavily on the quality and diversity of the training data and YOLOv8 model choice/hyperparameters.\n",
    "- Size classification thresholds are heuristic and may need adjustment for different regions or image resolutions.\n",
    "- Lacks real-world unit calibration (meters/hectares) without accurate image scale information (pixel_scale).\n",
    "- Recommendations are general and do not account for specific climate, soil, or market conditions.\n",
    "- Connected components might merge adjacent farms if segmentation is imperfect or farms are very close.\n",
    "\n",
    "### 7.3 Future Improvements\n",
    "\n",
    "Potential next steps for improving the system:\n",
    "1. **Model Enhancements**:\n",
    "   - Experiment with larger YOLOv8 models (e.g., yolov8m-seg, yolov8l-seg) if compute resources allow.\n",
    "   - Fine-tune hyperparameters (learning rate, augmentations, epochs) using tools like hyperparameter sweeping.\n",
    "   - Train on a larger, more diverse dataset covering various farm types, lighting conditions, and geographical areas.\n",
    "   - Explore techniques to better separate adjacent fields if merging is an issue.\n",
    "\n",
    "2. **Size Calculation**:\n",
    "   - Integrate with GIS systems or use image metadata (e.g., EXIF, GeoTIFF tags) to obtain accurate geospatial coordinates and determine image scale (pixel_scale) automatically.\n",
    "   - Account for terrain variations using Digital Elevation Models (DEMs) for more accurate area calculations.\n",
    "\n",
    "3. **Recommendation System**:\n",
    "   - Incorporate climate data (temperature, rainfall), soil type data, and market prices for more targeted recommendations.\n",
    "   - Develop region-specific recommendation models.\n",
    "   - Create a more interactive interface allowing users to input additional farm context.\n",
    "\n",
    "4. **Deployment & User Interface**:\n",
    "   - Export the trained YOLOv8 model to formats like ONNX or TensorRT for optimized inference.\n",
    "   - Develop a web or mobile application for easier access, allowing users to upload imagery or select areas on a map.\n",
    "   - Provide visualization tools for farmers to explore segmentation results and recommendations.\n",
    "\n",
    "5. **Validation**:\n",
    "   - Conduct field validation comparing calculated areas with actual farm measurements.\n",
    "   - Collect feedback from agricultural experts and farmers on the accuracy and usefulness of the segmentations and recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca07339",
   "metadata": {},
   "source": [
    "## 8. Model Export (YOLOv8)\n",
    "\n",
    "YOLOv8 provides a simple method to export the trained model to various formats like ONNX, TensorRT, CoreML, etc., for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained YOLOv8 model\n",
    "\n",
    "if model and best_model_path:\n",
    "    print(f\"Exporting model from: {best_model_path}\")\n",
    "    try:\n",
    "        # Export to ONNX format (recommended for cross-platform compatibility)\n",
    "        onnx_path = model.export(format='onnx', imgsz=640) # Use the same imgsz as training/validation\n",
    "        print(f\"Model exported to ONNX format at: {onnx_path}\")\n",
    "\n",
    "        # Optional: Export to other formats if needed\n",
    "        # torchscript_path = model.export(format='torchscript')\n",
    "        # print(f\"Model exported to TorchScript format at: {torchscript_path}\")\n",
    "\n",
    "        # For TensorRT export (requires TensorRT installation and GPU)\n",
    "        # if torch.cuda.is_available():\n",
    "        #     try:\n",
    "        #         tensorrt_path = model.export(format='engine', device=0) # Specify GPU device\n",
    "        #         print(f\"Model exported to TensorRT engine at: {tensorrt_path}\")\n",
    "        #     except Exception as trt_e:\n",
    "        #         print(f\"Could not export to TensorRT: {trt_e}\")\n",
    "        # else:\n",
    "        #     print(\"Skipping TensorRT export: No GPU available or TensorRT not installed.\")\n",
    "\n",
    "    except Exception as export_e:\n",
    "        print(f\"An error occurred during model export: {export_e}\")\n",
    "else:\n",
    "    print(\"Skipping model export: No trained model available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained the U-Net transfer learning training loop.\n",
    "# It is removed as we are using YOLOv8.\n",
    "print(\"U-Net transfer learning section removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530e36a",
   "metadata": {},
   "source": [
    "## 9. Visualization of Predictions (YOLOv8)\n",
    "\n",
    "Let's visualize the predictions of the final YOLOv8 model on a few validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2badbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize YOLOv8 predictions on an image\n",
    "def visualize_yolo_predictions(model, image_path, conf_threshold=0.25):\n",
    "    if not model:\n",
    "        print(\"Model not available.\")\n",
    "        return\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Visualizing predictions for: {os.path.basename(image_path)}\")\n",
    "    try:\n",
    "        # Run prediction\n",
    "        results = model.predict(image_path, conf=conf_threshold, device=device)\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            # Use YOLO's built-in plot function\n",
    "            pred_plot = results[0].plot() # Returns a BGR numpy array\n",
    "\n",
    "            # Display the plot\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            plt.imshow(cv2.cvtColor(pred_plot, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"YOLOv8 Predictions (Conf: {conf_threshold})\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No predictions found for this image.\")\n",
    "            # Show original image if no predictions\n",
    "            img = Image.open(image_path)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Original Image (No Predictions)\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Visualize predictions on a few validation images\n",
    "if test_img_dir and os.path.isdir(test_img_dir):\n",
    "    test_img_files = [f for f in os.listdir(test_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    num_viz = min(3, len(test_img_files)) # Visualize up to 3 images\n",
    "    if num_viz > 0:\n",
    "        print(f\"\\n--- Visualizing Predictions on {num_viz} Validation Images ---\")\n",
    "        selected_files = random.sample(test_img_files, num_viz)\n",
    "        for img_file in selected_files:\n",
    "            viz_path = os.path.join(test_img_dir, img_file)\n",
    "            visualize_yolo_predictions(model, viz_path, conf_threshold=0.25)\n",
    "    else:\n",
    "        print(\"No validation images found for visualization.\")\n",
    "else:\n",
    "    print(\"Validation image directory not found. Cannot visualize predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29bd0d",
   "metadata": {},
   "source": [
    "### End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained ONNX export for U-Net.\n",
    "# YOLOv8 export is handled in Section 8.\n",
    "print(\"U-Net ONNX export section removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained a U-Net specific visualization function.\n",
    "# YOLOv8 prediction visualization is handled in Section 9.\n",
    "print(\"U-Net visualization function removed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
