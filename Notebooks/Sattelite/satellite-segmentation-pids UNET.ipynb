{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FarmWise: Farmland Segmentation and Size Classification with U-Net\n",
    "\n",
    "**Date**: April 14, 2025\n",
    "\n",
    "This notebook implements a farm segmentation system using U-Net architecture to identify agricultural fields from satellite imagery, calculate their sizes, and classify them for targeted recommendations.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal**: Create a system that can:\n",
    "1. Detect and segment farmlands from satellite imagery\n",
    "2. Calculate the size/area of each identified farm\n",
    "3. Classify farms by size (small, medium, large)\n",
    "4. Enable a recommendation system based on farm size classification\n",
    "\n",
    "**Approach**: U-Net architecture for semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Agricultural recommendations are most effective when tailored to the specific context of a farm, with farm size being a crucial factor. Large farms may benefit from different techniques, equipment, and crop selections compared to small ones. This project aims to automatically classify farms by size from satellite imagery to enable targeted recommendations.\n",
    "\n",
    "### 1.2 Success Criteria\n",
    "\n",
    "- **Technical Success**: Achieve high accuracy in farmland segmentation (IoU > 0.75)\n",
    "- **Business Success**: Enable accurate size-based classification of farms for targeted recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Understanding\n",
    "\n",
    "### 2.1 Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:48.962300Z",
     "iopub.status.busy": "2025-04-14T21:52:48.961899Z",
     "iopub.status.idle": "2025-04-14T21:52:52.514395Z",
     "shell.execute_reply": "2025-04-14T21:52:52.513293Z",
     "shell.execute_reply.started": "2025-04-14T21:52:48.962265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for Kaggle environment and set up dependencies for GPU acceleration\n",
    "!pip install torch torchvision matplotlib numpy pillow scikit-learn scikit-image opencv-python roboflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:52.516529Z",
     "iopub.status.busy": "2025-04-14T21:52:52.516176Z",
     "iopub.status.idle": "2025-04-14T21:52:52.527355Z",
     "shell.execute_reply": "2025-04-14T21:52:52.526499Z",
     "shell.execute_reply.started": "2025-04-14T21:52:52.516499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from tqdm.notebook import tqdm\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability and set up CUDA device\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Use all available GPUs if there are multiple\n",
    "    if num_gpus > 1:\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using {num_gpus} GPUs for data parallel training\")\n",
    "    else:\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"Using single GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU. This will be slower.\")\n",
    "\n",
    "# Display CUDA version if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Acquisition from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:52.529348Z",
     "iopub.status.busy": "2025-04-14T21:52:52.529142Z",
     "iopub.status.idle": "2025-04-14T21:52:55.439820Z",
     "shell.execute_reply": "2025-04-14T21:52:55.439067Z",
     "shell.execute_reply.started": "2025-04-14T21:52:52.529329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Roboflow and load dataset\n",
    "# Note: You will need to provide your Roboflow API key\n",
    "rf = Roboflow(api_key=\"HE9CEH5JxJ3U0vXrQTOy\")  # Replace with your actual API key\n",
    "project = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "\n",
    "# Print dataset path\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Exploration\n",
    "\n",
    "Let's examine the structure of our dataset and visualize some sample images with their masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:55.441460Z",
     "iopub.status.busy": "2025-04-14T21:52:55.441225Z",
     "iopub.status.idle": "2025-04-14T21:52:55.492202Z",
     "shell.execute_reply": "2025-04-14T21:52:55.491537Z",
     "shell.execute_reply.started": "2025-04-14T21:52:55.441440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "def explore_directory(path, level=0):\n",
    "    print('  ' * level + f\"|-- {os.path.basename(path)}\")\n",
    "    if os.path.isdir(path):\n",
    "        for item in os.listdir(path)[:10]:  # Limit to first 10 items\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                explore_directory(item_path, level + 1)\n",
    "            else:\n",
    "                print('  ' * (level + 1) + f\"|-- {item}\")\n",
    "        if len(os.listdir(path)) > 10:\n",
    "            print('  ' * (level + 1) + f\"|-- ... ({len(os.listdir(path)) - 10} more items)\")\n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "explore_directory(dataset.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:55.493232Z",
     "iopub.status.busy": "2025-04-14T21:52:55.492959Z",
     "iopub.status.idle": "2025-04-14T21:52:57.267324Z",
     "shell.execute_reply": "2025-04-14T21:52:57.266421Z",
     "shell.execute_reply.started": "2025-04-14T21:52:55.493200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# YAML needed for reading class information\n",
    "import yaml\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_samples(data_dir, num_samples=3):\n",
    "    \"\"\"Visualizes original images, generated masks from YOLO files, and raw annotations for a random sample.\"\"\"\n",
    "\n",
    "    print(f\"\\n--- Starting Random Sample Visualization ({num_samples} samples) ---\")\n",
    "    \n",
    "    # Define image and mask directories\n",
    "    train_img_dir = os.path.join(data_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(data_dir, 'train', 'labels')\n",
    "    \n",
    "    # Basic path checks\n",
    "    if not os.path.isdir(train_img_dir): \n",
    "        print(f\"Error: Train image directory not found: {train_img_dir}\")\n",
    "        return\n",
    "    if not os.path.isdir(train_mask_dir): \n",
    "        print(f\"Error: Train mask directory not found: {train_mask_dir}\")\n",
    "        return\n",
    "\n",
    "    # Get image files\n",
    "    img_files_all = [f for f in os.listdir(train_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not img_files_all:\n",
    "        print(f\"No image files found in {train_img_dir}\")\n",
    "        return\n",
    "\n",
    "    # Sample random images\n",
    "    num_available = len(img_files_all)\n",
    "    num_samples = min(num_samples, num_available)\n",
    "    if num_samples == 0:\n",
    "        print(\"No samples requested or available.\")\n",
    "        return\n",
    "    img_files = random.sample(img_files_all, num_samples)\n",
    "\n",
    "    # Load class names from data.yaml\n",
    "    yaml_path = os.path.join(data_dir, 'data.yaml')\n",
    "    class_names = ['Unknown']\n",
    "    farm_class_id = None\n",
    "    if os.path.exists(yaml_path):\n",
    "        try:\n",
    "            with open(yaml_path, 'r') as f: \n",
    "                data_yaml = yaml.safe_load(f)\n",
    "            if 'names' in data_yaml:\n",
    "                class_names = data_yaml['names']\n",
    "                for i, name in enumerate(class_names):\n",
    "                    if \"farm\" in name.lower():\n",
    "                        farm_class_id = i\n",
    "                        break\n",
    "        except Exception as e: \n",
    "            print(f\"Warning: Error reading class names from data.yaml: {e}\")\n",
    "    else: \n",
    "        print(f\"Warning: data.yaml not found at {yaml_path}.\")\n",
    "\n",
    "    # Setup plot\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        fig.suptitle(\"Random Sample Image, Generated Mask, and Raw Annotations\", fontsize=16)\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        plt.suptitle(f\"Random Sample Images, Generated Masks, and Raw Annotations (n={num_samples})\", fontsize=16)\n",
    "\n",
    "    # Process each image\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        ax_img, ax_mask, ax_raw = axes[i]\n",
    "        img_path = os.path.join(train_img_dir, img_file)\n",
    "        try: # Load image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_np = np.array(img)\n",
    "            img_height, img_width = img_np.shape[:2]\n",
    "        except Exception as e: # Handle image loading error\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "            ax_img.set_title(f\"Error loading {img_file}\")\n",
    "            ax_img.axis('off')\n",
    "            ax_mask.set_title(\"Mask N/A\")\n",
    "            ax_mask.axis('off')\n",
    "            ax_raw.set_title(\"Raw Annotations N/A\")\n",
    "            ax_raw.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Prepare for drawing\n",
    "        mask_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        mask_path = os.path.join(train_mask_dir, mask_file)\n",
    "        filled_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "        img_np_raw = img_np.copy()\n",
    "        ax_img.imshow(img_np)\n",
    "        ax_img.set_title(f\"Image: {img_file}\")\n",
    "        ax_img.axis('off')\n",
    "        ax_raw.set_title(f\"Raw Annots (File: {mask_file})\")\n",
    "        ax_raw.axis('off')\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try: # Read annotations\n",
    "                with open(mask_path, 'r') as f: \n",
    "                    lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    try: # Process single annotation line\n",
    "                        parts = line.strip().split(' ')\n",
    "                        if len(parts) < 5: \n",
    "                            continue\n",
    "                        class_id = int(parts[0])\n",
    "                        if farm_class_id is not None and class_id != farm_class_id: \n",
    "                            continue # Filter class\n",
    "                        if len(parts) > 5 and len(parts) % 2 == 1: \n",
    "                            is_polygon=True # Polygon format: class x1 y1 x2 y2 ...\n",
    "                        elif len(parts) == 5: \n",
    "                            is_polygon=False # Bbox format: class xc yc w h\n",
    "                        else: \n",
    "                            continue # Skip ambiguous format\n",
    "                        coords = parts[1:]\n",
    "                        if is_polygon:\n",
    "                             polygon_points_pixels = [(int(float(coords[j])*img_width), int(float(coords[j+1])*img_height)) for j in range(0, len(coords), 2)]\n",
    "                             if len(polygon_points_pixels) >= 3:\n",
    "                                 pts = np.array(polygon_points_pixels, np.int32).reshape((-1, 1, 2))\n",
    "                                 cv2.fillPoly(filled_mask, [pts], 255) # Fill mask white\n",
    "                                 outline_color_bgr=(0, 255, 0)\n",
    "                                 vertex_color_bgr=(0, 0, 255)\n",
    "                                 cv2.polylines(img_np_raw, [pts], True, outline_color_bgr, 1) # Draw raw outline\n",
    "                                 for px, py in polygon_points_pixels: \n",
    "                                     cv2.circle(img_np_raw, (px, py), 2, vertex_color_bgr, -1) # Draw raw vertices\n",
    "                        else: # Bbox\n",
    "                            x_center, y_center, width, height = map(float, coords)\n",
    "                            x1=max(0, int((x_center-width/2)*img_width))\n",
    "                            y1=max(0, int((y_center-height/2)*img_height))\n",
    "                            x2=min(img_width-1, int((x_center+width/2)*img_width))\n",
    "                            y2=min(img_height-1, int((y_center+height/2)*img_height))\n",
    "                            cv2.rectangle(filled_mask, (x1, y1), (x2, y2), 128, -1) # Fill mask gray for bbox\n",
    "                            bbox_color_bgr = (255, 0, 0)\n",
    "                            cv2.rectangle(img_np_raw, (x1, y1), (x2, y2), bbox_color_bgr, 1) # Draw raw bbox outline\n",
    "                    except (ValueError, IndexError, Exception): \n",
    "                        pass # Silently ignore line processing errors\n",
    "            except Exception: \n",
    "                ax_mask.set_title(\"Error reading mask file\")\n",
    "                ax_raw.set_title(\"Error reading mask file\") # Handle file read error\n",
    "        else: \n",
    "            ax_mask.set_title(\"Mask file missing\")\n",
    "            ax_raw.set_title(\"Annotation file missing\") # Handle missing mask file\n",
    "\n",
    "        # Display final results for the row\n",
    "        ax_mask.imshow(filled_mask, cmap='gray', vmin=0, vmax=255)\n",
    "        ax_mask.set_title(\"Generated Mask (Poly=White, BBox=Gray)\")\n",
    "        ax_mask.axis('off')\n",
    "        ax_raw.imshow(img_np_raw)\n",
    "        ax_raw.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    print(\"--- Finished Random Sample Visualization ---\")\n",
    "\n",
    "\n",
    "# Run the visualization with 3 samples\n",
    "if 'dataset' in locals() and hasattr(dataset, 'location'):\n",
    "    try:\n",
    "        # Set to 3 to show 3 random samples\n",
    "        num_visual_samples = 3\n",
    "        visualize_samples(dataset.location, num_samples=num_visual_samples)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"An unexpected error occurred during visualization: {e}\")\n",
    "else:\n",
    "    print(\"\\nError: Dataset is not properly loaded. Cannot run visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Preparation\n",
    "\n",
    "We need to convert YOLOv8 format annotations to segmentation masks for U-Net training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.268596Z",
     "iopub.status.busy": "2025-04-14T21:52:57.268355Z",
     "iopub.status.idle": "2025-04-14T21:52:57.288655Z",
     "shell.execute_reply": "2025-04-14T21:52:57.287711Z",
     "shell.execute_reply.started": "2025-04-14T21:52:57.268577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Section 2.3: Dataset Exploration and Definition ===\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import yaml # Make sure yaml is imported\n",
    "import torch.nn.functional as F # Ensure F is imported for interpolate\n",
    "\n",
    "# Define paths after loading the dataset\n",
    "# Ensure the 'dataset' variable is available from the download cell\n",
    "if 'dataset' in locals() and hasattr(dataset, 'location'):\n",
    "    dataset_base_dir = dataset.location # For finding data.yaml and overall root\n",
    "    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n",
    "    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n",
    "    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n",
    "    print(f\"Dataset paths set using location: {dataset.location}\")\n",
    "else:\n",
    "    # Fallback or error if dataset location is not defined\n",
    "    print(\"ERROR: 'dataset' variable or 'dataset.location' not found.\")\n",
    "    print(\"Please run the Roboflow download cell first.\")\n",
    "    # Define dummy paths to avoid crashing subsequent cells, but processing will fail\n",
    "    dataset_base_dir = \".\"\n",
    "    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n",
    "    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n",
    "    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n",
    "\n",
    "\n",
    "# --- Custom Dataset Class (Less Verbose) ---\n",
    "class FarmlandDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading satellite images and generating segmentation masks\n",
    "    from YOLOv8 polygon annotation files. Less verbose version.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, mask_dir, dataset_root_dir, transform=None, farm_class_name=\"farm\", img_size=256):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.farm_class_name = farm_class_name\n",
    "\n",
    "        # Filter for valid image files only\n",
    "        try:\n",
    "            self.img_files = sorted([\n",
    "                f for f in os.listdir(img_dir)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ])\n",
    "            if not self.img_files:\n",
    "                 print(f\"Warning: No image files found in {self.img_dir}\") # Keep important warning\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: Image directory not found: {self.img_dir}. Dataset will be empty.\") # Keep critical error\n",
    "             self.img_files = []\n",
    "\n",
    "        self.class_names = ['Unknown']\n",
    "        self.farm_class_id = None\n",
    "        self._load_class_info(dataset_root_dir)\n",
    "\n",
    "\n",
    "    def _load_class_info(self, dataset_root_dir):\n",
    "        \"\"\"Loads class names and identifies the farm class ID from data.yaml.\"\"\"\n",
    "        yaml_path = os.path.join(dataset_root_dir, 'data.yaml')\n",
    "        if os.path.exists(yaml_path):\n",
    "            try:\n",
    "                with open(yaml_path, 'r') as f:\n",
    "                    data_yaml = yaml.safe_load(f)\n",
    "                    if 'names' in data_yaml:\n",
    "                        self.class_names = data_yaml['names']\n",
    "                        for i, name in enumerate(self.class_names):\n",
    "                            if self.farm_class_name.lower() in name.lower():\n",
    "                                self.farm_class_id = i; break\n",
    "                        if self.farm_class_id is None:\n",
    "                            print(f\"Warning: Target class name '{self.farm_class_name}' not found in data.yaml names: {self.class_names}\") # Keep warning\n",
    "                    else:\n",
    "                        print(f\"Warning: 'names' key not found in {yaml_path}\") # Keep warning\n",
    "            except ImportError:\n",
    "                 print(\"Warning: PyYAML not installed. Cannot read class names from data.yaml.\") # Keep warning\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading class names from data.yaml: {e}\") # Keep warning\n",
    "        else:\n",
    "             print(f\"Warning: data.yaml not found at {yaml_path}. Cannot determine farm class ID automatically.\") # Keep warning\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.img_files):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        img_filename = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            original_width, original_height = image.size\n",
    "        except Exception:\n",
    "            # Return dummy tensors silently if image loading fails\n",
    "            dummy_image = torch.zeros((3, self.img_size, self.img_size))\n",
    "            dummy_mask = torch.zeros((1, self.img_size, self.img_size))\n",
    "            return dummy_image, dummy_mask\n",
    "\n",
    "        mask_file = os.path.splitext(img_filename)[0] + '.txt'\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "        mask = np.zeros((original_height, original_width), dtype=np.float32)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try:\n",
    "                with open(mask_path, 'r') as f: lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    try:\n",
    "                        parts = line.strip().split(' ')\n",
    "                        if len(parts) < 5: continue\n",
    "                        class_id = int(parts[0])\n",
    "                        if self.farm_class_id is not None and class_id != self.farm_class_id: continue\n",
    "                        if len(parts) > 5 and len(parts) % 2 == 1: # Polygon: class x1 y1 x2 y2 ...\n",
    "                            polygon_points_normalized = []\n",
    "                            for j in range(1, len(parts), 2):\n",
    "                                 if j + 1 < len(parts):\n",
    "                                     x_norm = float(parts[j]); y_norm = float(parts[j+1])\n",
    "                                     polygon_points_normalized.append((x_norm, y_norm))\n",
    "                            polygon_points_pixels = [(int(x*original_width), int(y*original_height)) for x,y in polygon_points_normalized]\n",
    "                            if len(polygon_points_pixels) >= 3:\n",
    "                                pts = np.array(polygon_points_pixels, np.int32).reshape((-1, 1, 2))\n",
    "                                cv2.fillPoly(mask, [pts], 1.0)\n",
    "                        # Ignore bounding boxes (len(parts) == 5) for mask generation\n",
    "                    except (ValueError, IndexError): continue # Silently ignore errors in single line processing\n",
    "            except Exception: pass # Silently ignore file reading errors\n",
    "\n",
    "        # --- Transformations ---\n",
    "        if not self.transform:\n",
    "             resizer = transforms.Resize((self.img_size, self.img_size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "             to_tensor = transforms.ToTensor()\n",
    "             image_resized = resizer(image)\n",
    "             image_tensor = to_tensor(image_resized)\n",
    "             mask_resized = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "             mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n",
    "        else:\n",
    "             image_tensor = self.transform(image)\n",
    "             target_h, target_w = image_tensor.shape[-2:]\n",
    "             mask_resized = cv2.resize(mask, (target_w, target_h), interpolation=cv2.INTER_NEAREST)\n",
    "             mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n",
    "\n",
    "        if image_tensor.shape[-2:] != mask_tensor.shape[-2:]:\n",
    "              try: # Attempt silent correction\n",
    "                  mask_tensor = F.interpolate(mask_tensor.unsqueeze(0), size=image_tensor.shape[-2:], mode='nearest').squeeze(0)\n",
    "              except Exception: # Return dummy on correction failure\n",
    "                   dummy_image = torch.zeros((3, self.img_size, self.img_size)); dummy_mask = torch.zeros((1, self.img_size, self.img_size))\n",
    "                   return dummy_image, dummy_mask\n",
    "\n",
    "        mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.290228Z",
     "iopub.status.busy": "2025-04-14T21:52:57.289831Z",
     "iopub.status.idle": "2025-04-14T21:52:57.316101Z",
     "shell.execute_reply": "2025-04-14T21:52:57.315425Z",
     "shell.execute_reply.started": "2025-04-14T21:52:57.290191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up augmented data transformations to prevent overfitting\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flips\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Random vertical flips\n",
    "    transforms.RandomRotation(10),           # Random rotations up to 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Keep validation transform simple\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets with separate transforms\n",
    "train_dataset = FarmlandDataset(train_img_dir, train_mask_dir, dataset_base_dir, transform=train_transform)\n",
    "val_dataset = FarmlandDataset(val_img_dir, val_mask_dir, dataset_base_dir, transform=val_transform)\n",
    "\n",
    "# Optimize batch size based on available GPUs and model complexity\n",
    "if torch.cuda.device_count() > 1:\n",
    "    batch_size = 24  # Increased batch size for multiple GPUs\n",
    "else:\n",
    "    batch_size = 8   # Default batch size for single GPU\n",
    "\n",
    "# Determine optimal number of workers for data loading\n",
    "num_workers = min(os.cpu_count(), 16) if os.cpu_count() else 4\n",
    "\n",
    "# Configure DataLoader with more aggressive prefetching and optimized memory usage\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,        # More workers for faster data loading\n",
    "    pin_memory=True,                # Use pinned memory for faster CPU->GPU transfer\n",
    "    prefetch_factor=4,              # Prefetch more batches\n",
    "    persistent_workers=True,        # Keep worker processes alive between iterations\n",
    "    drop_last=True                  # Drop last incomplete batch for better performance\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Adjust CUDA settings for optimal performance\n",
    "torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner to find the best algorithm\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "torch.backends.cudnn.allow_tf32 = True        # Allow TF32 on Ampere GPUs\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Using batch size: {batch_size} with {torch.cuda.device_count()} GPUs\")\n",
    "print(f\"DataLoader configured with {num_workers} worker processes\")\n",
    "print(f\"Data augmentation enabled for training to prevent overfitting\")\n",
    "print(f\"CUDA optimizations enabled: benchmark={torch.backends.cudnn.benchmark}, TF32={torch.backends.cuda.matmul.allow_tf32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "### 3.1 U-Net Architecture\n",
    "\n",
    "We'll implement an efficient U-Net architecture with GroupNorm for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.318099Z",
     "iopub.status.busy": "2025-04-14T21:52:57.317847Z",
     "iopub.status.idle": "2025-04-14T21:52:57.334397Z",
     "shell.execute_reply": "2025-04-14T21:52:57.333517Z",
     "shell.execute_reply.started": "2025-04-14T21:52:57.318078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Efficient U-Net implementation with memory optimization\n",
    "class EfficientUNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(EfficientUNet, self).__init__()\n",
    "        \n",
    "        # Use GroupNorm instead of BatchNorm for better performance with large batch sizes\n",
    "        # and more stability across different batch sizes\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.outconv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights properly for faster convergence\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.GroupNorm):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        u4 = self.upconv4(b)\n",
    "        u4 = torch.cat([u4, e4], dim=1)  # Skip connection\n",
    "        d4 = self.dec4(u4)\n",
    "        \n",
    "        u3 = self.upconv3(d4)\n",
    "        u3 = torch.cat([u3, e3], dim=1)  # Skip connection\n",
    "        d3 = self.dec3(u3)\n",
    "        \n",
    "        u2 = self.upconv2(d3)\n",
    "        u2 = torch.cat([u2, e2], dim=1)  # Skip connection\n",
    "        d2 = self.dec2(u2)\n",
    "        \n",
    "        u1 = self.upconv1(d2)\n",
    "        u1 = torch.cat([u1, e1], dim=1)  # Skip connection\n",
    "        d1 = self.dec1(u1)\n",
    "        \n",
    "        # Final output - return logits directly (no sigmoid)\n",
    "        # This is crucial for compatibility with BCEWithLogitsLoss and AMP\n",
    "        out = self.outconv(d1)\n",
    "        return out  # Ensure no sigmoid activation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.335519Z",
     "iopub.status.busy": "2025-04-14T21:52:57.335314Z",
     "iopub.status.idle": "2025-04-14T21:52:57.828921Z",
     "shell.execute_reply": "2025-04-14T21:52:57.828002Z",
     "shell.execute_reply.started": "2025-04-14T21:52:57.335502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = EfficientUNet(n_channels=3, n_classes=1)\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Ensure BCEWithLogitsLoss is used, as it's safe for AMP and expects raw logits from the model\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW with weight decay to reduce overfitting\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model Architecture: EfficientUNet with GroupNorm\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "print(f\"Optimizer: AdamW with learning rate 0.001 and weight decay 1e-4\")\n",
    "print(f\"Loss Function: Binary Cross Entropy Loss with Logits (BCEWithLogitsLoss)\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Functions\n",
    "\n",
    "We'll define optimized functions for training, validation, and model evaluation with IoU (Intersection over Union) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.830223Z",
     "iopub.status.busy": "2025-04-14T21:52:57.829882Z",
     "iopub.status.idle": "2025-04-14T21:52:57.840316Z",
     "shell.execute_reply": "2025-04-14T21:52:57.839498Z",
     "shell.execute_reply.started": "2025-04-14T21:52:57.830189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training and validation functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, amp_scaler=None):\n",
    "    \"\"\"Trains the model for one epoch with optional mixed precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    processed_samples = 0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device, non_blocking=True)  # Use non_blocking for async transfer\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if amp_scaler is not None:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                # Calculate loss using BCEWithLogitsLoss (expects logits)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            amp_scaler.scale(loss).backward()\n",
    "            amp_scaler.step(optimizer)\n",
    "            amp_scaler.update()\n",
    "        else:\n",
    "            # Standard training\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_size\n",
    "        processed_samples += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_samples\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validates the model and calculates validation loss\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    processed_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item() * batch_size\n",
    "            processed_samples += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_samples\n",
    "    return epoch_loss\n",
    "\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculates Intersection over Union between prediction and target mask\"\"\"\n",
    "    # Apply sigmoid to logits before thresholding, as model outputs logits\n",
    "    pred_sigmoid = torch.sigmoid(pred)\n",
    "    pred_binary = (pred_sigmoid > threshold).float()\n",
    "    intersection = (pred_binary * target).sum()\n",
    "    union = pred_binary.sum() + target.sum() - intersection\n",
    "    \n",
    "    iou = (intersection + 1e-8) / (union + 1e-8)  # Small epsilon to avoid division by zero\n",
    "    return iou.item()\n",
    "\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"Evaluates model performance using IoU metric\"\"\"\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating IoU\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate IoU for each image in batch\n",
    "            for i in range(batch_size):\n",
    "                iou = calculate_iou(outputs[i], masks[i], threshold)\n",
    "                iou_scores.append(iou)\n",
    "            \n",
    "            total_samples += batch_size\n",
    "    \n",
    "    mean_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training Loop\n",
    "\n",
    "Now we'll run the training process with clear metrics reporting and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T21:52:57.841424Z",
     "iopub.status.busy": "2025-04-14T21:52:57.841113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies for mixed precision training\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "# Set up training parameters\n",
    "num_epochs = 30 # Adjust based on dataset size and complexity\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_unet_model.pth'\n",
    "patience = 10 # Early stopping patience\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# Set up mixed precision training for faster performance if CUDA is available\n",
    "scaler = None\n",
    "if torch.cuda.is_available():\n",
    "    scaler = amp.GradScaler()\n",
    "    print(\"Using mixed precision training for faster performance\")\n",
    "else:\n",
    "    print(\"CUDA not available, using standard precision (slower)\")\n",
    "\n",
    "# Set up learning rate scheduler to reduce LR when training plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Initialize lists to store metrics for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ious = []\n",
    "learning_rates = []\n",
    "\n",
    "# Print training configuration\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"- Epochs: {num_epochs} (early stopping patience: {patience})\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Learning rate: {optimizer.param_groups[0]['lr']} with ReduceLROnPlateau\")\n",
    "print(f\"- Mixed precision: {'Enabled' if scaler else 'Disabled'}\")\n",
    "print(f\"- Best model will be saved to: {best_model_path}\")\n",
    "\n",
    "# Training loop with clean, organized output\n",
    "print(\"\\nStarting Training...\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^12} | {'Val IoU':^10} | {'LR':^9} | {'Time (min)':^12} | {'Status':^20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, amp_scaler=scaler)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate IoU for monitoring performance\n",
    "    val_iou = evaluate_model(model, val_loader, device)\n",
    "    val_ious.append(val_iou)\n",
    "    \n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Time tracking\n",
    "    epoch_time = (time.time() - epoch_start) / 60  # convert to minutes\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        improvement = best_val_loss - val_loss\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        status = f\"Improved ({improvement:.4f})\"\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        status = f\"No improvement ({no_improve_epochs}/{patience})\"\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"{epoch+1:^7} | {train_loss:^12.4f} | {val_loss:^12.4f} | {val_iou:^10.4f} | {current_lr:^9.6f} | {epoch_time:^12.2f} | {status:^20}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "# Print training summary\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training completed in {total_time:.2f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize Training Results\n",
    "\n",
    "Let's visualize our training progress to better understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot IoU progress\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_ious, 'g-', label='Validation IoU')\n",
    "plt.axhline(y=0.75, color='r', linestyle='--', label='Target IoU (0.75)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('Validation IoU Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate changes\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(learning_rates, 'o-', color='purple')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final performance summary\n",
    "final_epoch = len(train_losses)\n",
    "best_iou_idx = val_ious.index(max(val_ious))\n",
    "best_loss_idx = val_losses.index(min(val_losses))\n",
    "\n",
    "print(f\"Training Summary:\")\n",
    "print(f\"- Total epochs trained: {final_epoch}\")\n",
    "print(f\"- Best validation IoU: {max(val_ious):.4f} (epoch {best_iou_idx+1})\")\n",
    "print(f\"- Best validation loss: {min(val_losses):.4f} (epoch {best_loss_idx+1})\")\n",
    "print(f\"- Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"- Final validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"- Final validation IoU: {val_ious[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Visualization\n",
    "\n",
    "Let's load our best model and evaluate its performance on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the best model for evaluation\n",
    "if os.path.exists(best_model_path):\n",
    "    # Load saved weights into model\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Loaded best model from {best_model_path}\")\n",
    "else:\n",
    "    print(\"Using current model weights (best model not found)\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to visualize model predictions\n",
    "def visualize_predictions(model, dataset, device, num_samples=3):\n",
    "    \"\"\"Visualize model predictions on random test samples\"\"\"\n",
    "    # Get loader for single images\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get a sample\n",
    "        image_tensor, mask_tensor = dataset[idx]\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        mask_tensor = mask_tensor.cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            output_sigmoid = torch.sigmoid(output)\n",
    "            predicted_mask = output_sigmoid.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Get original image (unnormalize)\n",
    "        image = image_tensor.squeeze().cpu().numpy()\n",
    "        image = np.transpose(image, (1, 2, 0))  # Change from CxHxW to HxWxC\n",
    "        # Unnormalize image for visualization\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Create binary predicted mask\n",
    "        predicted_binary = (predicted_mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Calculate IoU between prediction and ground truth\n",
    "        intersection = np.logical_and(predicted_binary, mask_tensor > 0.5).sum()\n",
    "        union = np.logical_or(predicted_binary, mask_tensor > 0.5).sum()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        \n",
    "        # Visualization - original image\n",
    "        plt.subplot(num_samples, 3, i*3 + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Visualization - ground truth mask\n",
    "        plt.subplot(num_samples, 3, i*3 + 2)\n",
    "        plt.imshow(mask_tensor, cmap='gray')\n",
    "        plt.title(f\"Ground Truth Mask\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Visualization - predicted mask\n",
    "        plt.subplot(num_samples, 3, i*3 + 3)\n",
    "        plt.imshow(predicted_binary, cmap='gray')\n",
    "        plt.title(f\"Predicted Mask (IoU: {iou:.4f})\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to create overlay visualization\n",
    "def visualize_segmentation_overlay(model, dataset, device, num_samples=3):\n",
    "    \"\"\"Create an overlay visualization of segmentation on the original image\"\"\"\n",
    "    # Get loader for single images\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get a sample\n",
    "        image_tensor, mask_tensor = dataset[idx]\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        mask_tensor = mask_tensor.cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            # Apply sigmoid to logits\n",
    "            output_sigmoid = torch.sigmoid(output)\n",
    "            predicted_mask = output_sigmoid.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Get original image (unnormalize)\n",
    "        image = image_tensor.squeeze().cpu().numpy()\n",
    "        image = np.transpose(image, (1, 2, 0))  # Change from CxHxW to HxWxC\n",
    "        # Unnormalize image for visualization\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Create binary predicted mask\n",
    "        predicted_binary = (predicted_mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Calculate IoU between prediction and ground truth\n",
    "        intersection = np.logical_and(predicted_binary, mask_tensor > 0.5).sum()\n",
    "        union = np.logical_or(predicted_binary, mask_tensor > 0.5).sum()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        \n",
    "        # Create overlay image\n",
    "        plt.subplot(num_samples, 2, i*2 + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(predicted_binary, alpha=0.4, cmap='Greens')\n",
    "        plt.title(f\"Prediction Overlay (IoU: {iou:.4f})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show side-by-side difference visualization\n",
    "        plt.subplot(num_samples, 2, i*2 + 2)\n",
    "        # Create RGB overlay: Green = correct prediction, Red = false positive, Blue = false negative\n",
    "        overlay = np.zeros((predicted_binary.shape[0], predicted_binary.shape[1], 3))\n",
    "        # True positive (where prediction and ground truth agree) - Green\n",
    "        overlay[np.logical_and(predicted_binary, mask_tensor > 0.5), 1] = 1.0\n",
    "        # False positive (prediction positive but ground truth negative) - Red\n",
    "        overlay[np.logical_and(predicted_binary, mask_tensor <= 0.5), 0] = 1.0\n",
    "        # False negative (prediction negative but ground truth positive) - Blue\n",
    "        overlay[np.logical_and(np.logical_not(predicted_binary), mask_tensor > 0.5), 2] = 1.0\n",
    "        \n",
    "        plt.imshow(image)\n",
    "        plt.imshow(overlay, alpha=0.5)\n",
    "        plt.title(f\"Error Analysis (Green=TP, Red=FP, Blue=FN)\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Visualize predictions on validation set\n",
    "print(\"Visualizing model predictions on validation data...\")\n",
    "visualize_predictions(model, val_dataset, device, num_samples=3)\n",
    "\n",
    "# Visualize overlay on validation set\n",
    "print(\"\\nVisualizing segmentation overlay and error analysis...\")\n",
    "visualize_segmentation_overlay(model, val_dataset, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quantitative Model Evaluation\n",
    "\n",
    "Let's evaluate the model's performance on the validation set with various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Expanded evaluation metrics for segmentation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_segmentation_metrics(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics:\n",
    "    - IoU (Intersection over Union)\n",
    "    - Dice Coefficient (F1 Score)\n",
    "    - Pixel Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'iou': [],\n",
    "        'dice': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating Metrics\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            batch_size = outputs.size(0)\n",
    "            \n",
    "            # Process each image in batch\n",
    "            for i in range(batch_size):\n",
    "                pred = outputs[i].squeeze().cpu().numpy()\n",
    "                target = masks[i].squeeze().cpu().numpy()\n",
    "                \n",
    "                # Convert to binary\n",
    "                pred_binary = (pred > threshold).astype(np.uint8)\n",
    "                target_binary = (target > threshold).astype(np.uint8)\n",
    "                \n",
    "                # Flatten for metric calculation\n",
    "                pred_flat = pred_binary.flatten()\n",
    "                target_flat = target_binary.flatten()\n",
    "                \n",
    "                # Calculate IoU\n",
    "                intersection = np.logical_and(pred_binary, target_binary).sum()\n",
    "                union = np.logical_or(pred_binary, target_binary).sum()\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                metrics['iou'].append(iou)\n",
    "                \n",
    "                # Calculate Dice coefficient (F1 score)\n",
    "                dice = 2 * intersection / (pred_binary.sum() + target_binary.sum()) if (pred_binary.sum() + target_binary.sum()) > 0 else 0\n",
    "                metrics['dice'].append(dice)\n",
    "                \n",
    "                # Pixel accuracy\n",
    "                accuracy = accuracy_score(target_flat, pred_flat)\n",
    "                metrics['accuracy'].append(accuracy)\n",
    "                \n",
    "                # Precision and recall\n",
    "                precision = precision_score(target_flat, pred_flat, zero_division=0)\n",
    "                recall = recall_score(target_flat, pred_flat, zero_division=0)\n",
    "                metrics['precision'].append(precision)\n",
    "                metrics['recall'].append(recall)\n",
    "    \n",
    "    # Calculate mean metrics\n",
    "    mean_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    std_metrics = {k: np.std(v) for k, v in metrics.items()}\n",
    "    \n",
    "    # Create DataFrame for nice display\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': list(mean_metrics.keys()),\n",
    "        'Mean': list(mean_metrics.values()),\n",
    "        'Std Dev': list(std_metrics.values())\n",
    "    })\n",
    "    \n",
    "    return metrics_df, metrics\n",
    "\n",
    "# Evaluate model on validation set\n",
    "metrics_df, detailed_metrics = evaluate_segmentation_metrics(model, val_loader, device)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(metrics_df.to_string(index=False, formatters={'Mean': '{:.4f}'.format, 'Std Dev': '{:.4f}'.format}))\n",
    "\n",
    "# Plot metrics distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# IoU distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(detailed_metrics['iou'], bins=20, alpha=0.7)\n",
    "plt.axvline(np.mean(detailed_metrics['iou']), color='r', linestyle='--', label=f\"Mean: {np.mean(detailed_metrics['iou']):.4f}\")\n",
    "plt.title('IoU Distribution')\n",
    "plt.xlabel('IoU')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Dice distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(detailed_metrics['dice'], bins=20, alpha=0.7)\n",
    "plt.axvline(np.mean(detailed_metrics['dice']), color='r', linestyle='--', label=f\"Mean: {np.mean(detailed_metrics['dice']):.4f}\")\n",
    "plt.title('Dice Coefficient Distribution')\n",
    "plt.xlabel('Dice')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Precision-Recall plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(detailed_metrics['precision'], detailed_metrics['recall'], alpha=0.5)\n",
    "plt.xlim(0, 1.05)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.title('Precision vs Recall')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if we achieved target IoU > 0.75\n",
    "mean_iou = np.mean(detailed_metrics['iou'])\n",
    "if mean_iou > 0.75:\n",
    "    print(f\"✅ Success! Achieved target IoU > 0.75 (Actual: {mean_iou:.4f})\")\n",
    "else:\n",
    "    print(f\"❌ Target IoU > 0.75 not achieved (Actual: {mean_iou:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
