{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9fdb79df","cell_type":"markdown","source":"# FarmWise: Farmland Segmentation and Size Classification with U-Net\n\n**Date**: April 14, 2025\n\nThis notebook implements a farm segmentation system using U-Net architecture to identify agricultural fields from satellite imagery, calculate their sizes, and classify them for targeted recommendations.\n\n## Project Overview\n\n**Goal**: Create a system that can:\n1. Detect and segment farmlands from satellite imagery\n2. Calculate the size/area of each identified farm\n3. Classify farms by size (small, medium, large)\n4. Enable a recommendation system based on farm size classification\n\n**Approach**: U-Net architecture for semantic segmentation","metadata":{}},{"id":"7c32e0b7","cell_type":"markdown","source":"## 1. Business Understanding\n\n### 1.1 Problem Statement\n\nAgricultural recommendations are most effective when tailored to the specific context of a farm, with farm size being a crucial factor. Large farms may benefit from different techniques, equipment, and crop selections compared to small ones. This project aims to automatically classify farms by size from satellite imagery to enable targeted recommendations.\n\n### 1.2 Success Criteria\n\n- **Technical Success**: Achieve high accuracy in farmland segmentation (IoU > 0.75)\n- **Business Success**: Enable accurate size-based classification of farms for targeted recommendations","metadata":{}},{"id":"f9c0f576","cell_type":"markdown","source":"## 2. Data Acquisition and Understanding\n\n### 2.1 Setup and Environment Preparation","metadata":{}},{"id":"5f4dd48b","cell_type":"code","source":"# Check for Kaggle environment and set up dependencies for GPU acceleration\n!pip install torch torchvision matplotlib numpy pillow scikit-learn scikit-image opencv-python roboflow tqdm","metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"execution_count":null},{"id":"371ef204","cell_type":"code","source":"# Import required libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom skimage import measure\nfrom tqdm.notebook import tqdm\nfrom roboflow import Roboflow\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA Available: False\n","No GPU available, using CPU. This will be slower.\n"]}],"execution_count":null},{"id":"006a57f6","cell_type":"markdown","source":"### 2.2 Data Acquisition from Roboflow","metadata":{}},{"id":"f64caafc","cell_type":"code","source":"# Initialize Roboflow and load dataset\n# Note: You will need to provide your Roboflow API key\nrf = Roboflow(api_key=\"HE9CEH5JxJ3U0vXrQTOy\")  # Replace with your actual API key\nproject = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\ndataset = project.version(1).download(\"yolov8\")\n\n# Print dataset path\nprint(f\"Dataset downloaded to: {dataset.location}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","loading Roboflow project...\n","Dataset downloaded to: /kaggle/working/Final-Detectron-2-1\n","Dataset downloaded to: /kaggle/working/Final-Detectron-2-1\n"]}],"execution_count":null},{"id":"6cec85ec","cell_type":"markdown","source":"### 2.3 Dataset Exploration","metadata":{}},{"id":"59b1ed2e","cell_type":"code","source":"# Explore the dataset structure\ndef explore_directory(path, level=0):\n    print('  ' * level + f\"|-- {os.path.basename(path)}\")\n    if os.path.isdir(path):\n        for item in os.listdir(path)[:10]:  # Limit to first 10 items\n            item_path = os.path.join(path, item)\n            if os.path.isdir(item_path):\n                explore_directory(item_path, level + 1)\n            else:\n                print('  ' * (level + 1) + f\"|-- {item}\")\n        if len(os.listdir(path)) > 10:\n            print('  ' * (level + 1) + f\"|-- ... ({len(os.listdir(path)) - 10} more items)\")\n\nprint(\"Dataset Structure:\")\nexplore_directory(dataset.location)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Structure:\n","|-- Final-Detectron-2-1\n","  |-- data.yaml\n","  |-- README.dataset.txt\n","  |-- valid\n","    |-- images\n","      |-- F_339_jpg.rf.51e2ccc5bf9110500c7c741dd02c850c.jpg\n","      |-- Image1302_jpg.rf.480c3275c79cf6f0fe5a12da7bc26de9.jpg\n","      |-- 0231133310001203_png_jpg.rf.8e459df1c3bb07a250e06f5466b4be06.jpg\n","      |-- tile_16_7_png_jpg.rf.2c2c4106ef0b72d1fe5c1c93f21628e3.jpg\n","      |-- F_231_jpg.rf.14d7b0a341204d5449770168be048bf6.jpg\n","      |-- Image1239_jpg.rf.f5e4174c2e4800c1cf34209fffe7c3ed.jpg\n","      |-- F_30002777_jpg.rf.4e10a4baa58cb9e36e45775e58e5ffeb.jpg\n","      |-- F_3000409_jpg.rf.0da484949be4a73e59e9a23d6af6a79c.jpg\n","      |-- F_30002029_jpg.rf.cd287ef949f58016c4af785cff991970.jpg\n","      |-- F_30002494_jpg.rf.17cf6a446e646d44fb036af064cd103e.jpg\n","      |-- ... (708 more items)\n","    |-- labels\n","      |-- F_30001820_jpg.rf.ba694da5df8a77c724abe5295c7dae02.txt\n","      |-- F_30002748_jpg.rf.2af9a581c54c84cd3eff25a73cbfb669.txt\n","      |-- 0303220111030101_png_jpg.rf.d2db1ac66b6db39fdc5674141b57fbad.txt\n","      |-- F_2421_jpg.rf.d6f1b5f8f7f9434dd958d4f539370024.txt\n","      |-- F_3000953_jpg.rf.759584d8100d4c3525c93efa48ac3758.txt\n","      |-- tile_16_1_png_jpg.rf.5f09ecc42e0c17c9647c7b439fe2b529.txt\n","      |-- F_1155_jpg.rf.0739f20a758dd93c650ebbc730fb114a.txt\n","      |-- F_30002854_jpg.rf.736c184a0ffab3f0f866ece78f511709.txt\n","      |-- F_30001382_jpg.rf.1d3b94ef62fb25dfec459d39b9950389.txt\n","      |-- Image1253_jpg.rf.d98ee7f28ccebae5ef9b9fe716d73435.txt\n","      |-- ... (708 more items)\n","  |-- README.roboflow.txt\n","  |-- test\n","    |-- images\n","      |-- 023010030301211_png_jpg.rf.8eb3fa75947585e9c8dbc2e09175ee17.jpg\n","      |-- 031331121201333_png_jpg.rf.26dbf681860b38adad0b2563d98f3ef8.jpg\n","      |-- 0320023010203102_png_jpg.rf.7584dbc61d34b227f9557895f896e250.jpg\n","      |-- F_2573_jpg.rf.ff1eee7c1c68d4bdb30729f4a912d976.jpg\n","      |-- F_30002333_jpg.rf.7487f482600d4a34eecb47c4691a38f6.jpg\n","      |-- F_30002736_jpg.rf.9f2df2d43406d6c5bb5ca42cfe02e027.jpg\n","      |-- F_30001429_jpg.rf.08c3007d481c57cca43660818509dc08.jpg\n","      |-- 031313332330013_png_jpg.rf.6aa2e39c60073dbd6a72c4490838a4ee.jpg\n","      |-- testnew72_jpg.rf.dd43d17d139e376f035dedad3d4a21a6.jpg\n","      |-- Image1350_jpg.rf.cedfbca968cbc8e02198eeb2ddda4daa.jpg\n","      |-- ... (820 more items)\n","    |-- labels\n","      |-- F_1670_jpg.rf.8d173ab0776a6f4c21db5568ca2f82b8.txt\n","      |-- F_30002943_jpg.rf.bfa75db45b93d83ef437c18fb631ad91.txt\n","      |-- F_30001966_jpg.rf.2e921043f87b52af46f6d3d6d5593fb4.txt\n","      |-- Image1361_jpg.rf.8d16893f406493ed43f322ba208c0121.txt\n","      |-- F_30001275_jpg.rf.b34b3b12aec50074ef4db867cb23e33a.txt\n","      |-- F_30001460_jpg.rf.703906d3cf51660235231677f8857e1c.txt\n","      |-- testnew54_jpg.rf.caecfee55f4479ad8d748456fdb44981.txt\n","      |-- testnew55_jpg.rf.eeed9bb3e1ae5810e22c557af48450f7.txt\n","      |-- F_30002665_jpg.rf.5088bc693ad7198c8f3bd7270eaf0466.txt\n","      |-- 032020113133110_png_jpg.rf.7d09ccc0f4b73e98f71cb01a2e60a36d.txt\n","      |-- ... (820 more items)\n","  |-- train\n","    |-- images\n","      |-- h525_jpg.rf.c43adc18ab10defccd721f1b03a1c55d.jpg\n","      |-- tile_13_8_png_jpg.rf.45bbcbcb29f8d69da43ddffc809b2099.jpg\n","      |-- 1212011000120_png_jpg.rf.ac7c4566ea4d12391b3289dc52af45f8.jpg\n","      |-- 023131120223212_png_jpg.rf.38eb6ca455b7e9332a16785761227399.jpg\n","      |-- Image1344_jpg.rf.f5a969d247fc87ee6e8e6242387e7a27.jpg\n","      |-- F_1966_jpg.rf.04b177fe7fe56049082a27caba4bcabf.jpg\n","      |-- h455_jpg.rf.60dda526fbed4d1c505cf41ee873b2ea.jpg\n","      |-- Image1256_jpg.rf.64064a488eeff3ccb00c8a16978e34f7.jpg\n","      |-- F_30002571_jpg.rf.27cd177e827d98bdba78331ba081b454.jpg\n","      |-- F_388_jpg.rf.12465b935ff9553c49a1130deb9648a0.jpg\n","      |-- ... (5948 more items)\n","    |-- labels\n","      |-- 311230012231123_png_jpg.rf.37837a9e8e7f3d6160e4d9439a1963cf.txt\n","      |-- tile_15_1_png_jpg.rf.08e7226edf9140fdadd36d62ebafaf3e.txt\n","      |-- F_1705_jpg.rf.cffb3f4a8eb15f56e0e5cf66e57bf296.txt\n","      |-- 120230010323013_png_jpg.rf.9f96d7829b9e4b057d83f8ef3c4bbfd8.txt\n","      |-- F_30002939_jpg.rf.af714d370be4f1735b145d25c3007f8a.txt\n","      |-- F_30002560_jpg.rf.db0b50461de4a0bd968d16ad40f11ed7.txt\n","      |-- F_30002653_jpg.rf.f59d210875b45f5169b768021bc33304.txt\n","      |-- h462_jpg.rf.083817f86f639750cf021b3afed92545.txt\n","      |-- 023113313232100_png_jpg.rf.bb05bd3acc83c86b6cbc50d6020f6aec.txt\n","      |-- F_30001429_jpg.rf.70078529b114512c071f1fe683af1fd1.txt\n","      |-- ... (5948 more items)\n"]}],"execution_count":null},{"id":"8f90d870","cell_type":"code","source":"# Visualize some sample images and masks\ndef visualize_samples(data_dir, num_samples=3):\n    # Paths for train images and labels\n    train_img_dir = os.path.join(data_dir, 'train', 'images')\n    train_mask_dir = os.path.join(data_dir, 'train', 'labels')\n    \n    img_files = os.listdir(train_img_dir)[:num_samples]\n    \n    # Read the data.yaml file to get class information\n    yaml_path = os.path.join(data_dir, 'data.yaml')\n    class_names = ['Unknown']\n    if os.path.exists(yaml_path):\n        try:\n            import yaml\n            with open(yaml_path, 'r') as f:\n                data_yaml = yaml.safe_load(f)\n                if 'names' in data_yaml:\n                    class_names = data_yaml['names']\n                    print(f\"Classes found in dataset: {class_names}\")\n        except Exception as e:\n            print(f\"Error reading class names from data.yaml: {e}\")\n    \n    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n    \n    for i, img_file in enumerate(img_files):\n        # Load image\n        img_path = os.path.join(train_img_dir, img_file)\n        img = Image.open(img_path)\n        \n        # Find corresponding mask (YOLOv8 format)\n        mask_file = os.path.splitext(img_file)[0] + '.txt'\n        mask_path = os.path.join(train_mask_dir, mask_file)\n        \n        # For visualization, we'll create a binary mask from YOLOv8 annotations\n        # This is simplified and will need to be adjusted based on actual format\n        mask = np.zeros(img.size[::-1], dtype=np.uint8)\n        \n        if os.path.exists(mask_path):\n            # Read YOLOv8 format annotations (class x_center y_center width height)\n            with open(mask_path, 'r') as f:\n                lines = f.readlines()\n                \n            img_width, img_height = img.size\n            for line in lines:\n                parts = line.strip().split(' ')\n                if len(parts) >= 5:  # Basic format check\n                    # Convert normalized coordinates to pixel values\n                    class_id = int(parts[0])\n                    class_name = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n                    \n                    # We're only interested in the \"Farms\" class (or all classes if we're not sure)\n                    if \"farm\" in class_name.lower() or True:  # Currently process all classes\n                        x_center = float(parts[1]) * img_width\n                        y_center = float(parts[2]) * img_height\n                        width = float(parts[3]) * img_width\n                        height = float(parts[4]) * img_height\n                        \n                        # Check if we have polygon points (instance segmentation)\n                        if len(parts) > 5:\n                            # Extract polygon points\n                            for j in range(5, len(parts), 2):\n                                if j+1 < len(parts):\n                                    x = float(parts[j]) * img_width\n                                    y = float(parts[j+1]) * img_height\n                                    polygon_points.append((x, y))\n                            \n                            if polygon_points:\n                                # Convert to numpy array for OpenCV\n                                pts = np.array(polygon_points, np.int32)\n                                pts = pts.reshape((-1, 1, 2))\n                                # Fill polygon with ones\n                                cv2.fillPoly(mask, [pts], 255)\n                        else:\n                            # Calculate bounding box coordinates\n                            x1 = int(x_center - width / 2)\n                            y1 = int(y_center - height / 2)\n                            x2 = int(x_center + width / 2)\n                            y2 = int(y_center + height / 2)\n                            \n                            # Draw rectangle on mask\n                            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)  # Fill rectangle\n        \n        # Display image and mask\n        if num_samples == 1:\n            axes[0].imshow(np.array(img))\n            axes[0].set_title(f\"Image: {img_file}\")\n            axes[0].axis('off')\n            \n            axes[1].imshow(mask, cmap='gray')\n            axes[1].set_title(f\"Mask (from YOLO annotations)\")\n            axes[1].axis('off')\n        else:\n            axes[i, 0].imshow(np.array(img))\n            axes[i, 0].set_title(f\"Image: {img_file}\")\n            axes[i, 0].axis('off')\n            \n            axes[i, 1].imshow(mask, cmap='gray')\n            axes[i, 1].set_title(f\"Mask (from YOLO annotations)\")\n            axes[i, 1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ntry:\n    visualize_samples(dataset.location)\nexcept Exception as e:\n    print(f\"Error visualizing samples: {e}\")\n    print(\"Note: Adjust the visualization code based on the actual data format you received.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"9aecd8c6","cell_type":"markdown","source":"### 2.4 Data Preparation\n\nWe need to convert YOLOv8 format annotations to segmentation masks for U-Net training.","metadata":{}},{"id":"f2e34e4a","cell_type":"code","source":"# Create a custom dataset class to load images and generate masks\nclass FarmlandDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, transform=None, farm_class_id=None, img_size=256):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.img_size = img_size\n        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n        \n        # Try to load class names from data.yaml\n        self.class_names = ['Unknown']\n        self.farm_class_id = farm_class_id  # If specified, only use this class ID for farm segmentation\n        yaml_path = os.path.join(os.path.dirname(os.path.dirname(img_dir)), 'data.yaml')\n        if os.path.exists(yaml_path):\n            try:\n                import yaml\n                with open(yaml_path, 'r') as f:\n                    data_yaml = yaml.safe_load(f)\n                    if 'names' in data_yaml:\n                        self.class_names = data_yaml['names']\n                        # If farm_class_id is not provided, try to find \"Farms\" class automatically\n                        if self.farm_class_id is None:\n                            for i, name in enumerate(self.class_names):\n                                if \"farm\" in name.lower():\n                                    print(f\"Found Farm class: {name} with ID {i}\")\n                                    self.farm_class_id = i\n                                    break\n            except Exception as e:\n                print(f\"Error reading class names from data.yaml: {e}\")\n    \n    def __len__(self):\n        return len(self.img_files)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.img_dir, self.img_files[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Generate mask from YOLOv8 annotation\n        mask_file = os.path.splitext(self.img_files[idx])[0] + '.txt'\n        mask_path = os.path.join(self.mask_dir, mask_file)\n        \n        # Create empty mask with the same size as the image\n        original_size = image.size[::-1]  # (height, width)\n        mask = np.zeros(original_size, dtype=np.float32)\n        \n        if os.path.exists(mask_path):\n            # Read YOLOv8 format annotations\n            with open(mask_path, 'r') as f:\n                lines = f.readlines()\n            \n            img_width, img_height = image.size\n            for line in lines:\n                parts = line.strip().split(' ')\n                if len(parts) >= 5:  # Basic format check for bounding box\n                    # Extract class ID\n                    class_id = int(parts[0])\n                    \n                    # Only process the farm class if specified, otherwise process all classes\n                    if self.farm_class_id is None or class_id == self.farm_class_id:\n                        # Check if we have polygon points (instance segmentation)\n                        if len(parts) > 5:\n                            # Extract polygon points\n                            polygon_points = []\n                            for i in range(5, len(parts), 2):\n                                if i+1 < len(parts):\n                                    x = float(parts[i]) * img_width\n                                    y = float(parts[i+1]) * img_height\n                                    polygon_points.append((x, y))\n                            \n                            if polygon_points:\n                                # Convert to numpy array for OpenCV\n                                pts = np.array(polygon_points, np.int32)\n                                pts = pts.reshape((-1, 1, 2))\n                                # Fill polygon with ones\n                                cv2.fillPoly(mask, [pts], 1)\n                        else:\n                            # Use bounding box if no polygon points\n                            x_center = float(parts[1]) * img_width\n                            y_center = float(parts[2]) * img_height\n                            width = float(parts[3]) * img_width\n                            height = float(parts[4]) * img_height\n                            \n                            x1 = int(x_center - width / 2)\n                            y1 = int(y_center - height / 2)\n                            x2 = int(x_center + width / 2)\n                            y2 = int(y_center + height / 2)\n                            \n                            cv2.rectangle(mask, (x1, y1), (x2, y2), 1, -1)\n        \n        # Create a custom transformation for the mask to ensure it's the same size as the transformed image\n        if self.transform:\n            # First resize the mask to the target size\n            resized_mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n            \n            # Apply transforms to image\n            image_tensor = self.transform(image)\n            \n            # Convert mask to tensor (without normalization)\n            mask_tensor = torch.from_numpy(resized_mask).float().unsqueeze(0)\n            \n            # Double-check that dimensions match\n            if image_tensor.shape[-2:] != mask_tensor.shape[-2:]:\n                # If dimensions still don't match, force resize the mask to match exactly\n                mask_tensor = F.interpolate(\n                    mask_tensor.unsqueeze(0), \n                    size=image_tensor.shape[-2:], \n                    mode='nearest'\n                ).squeeze(0)\n        else:\n            # If no transform is applied, resize both to a fixed size and convert to tensor\n            image = transforms.Resize((self.img_size, self.img_size))(image)\n            image_tensor = transforms.ToTensor()(image)\n            \n            resized_mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n            mask_tensor = torch.from_numpy(resized_mask).float().unsqueeze(0)\n        \n        # Print shapes for debugging (only for the first few items)\n        if idx < 3:  \n            print(f\"Image shape: {image_tensor.shape}, Mask shape: {mask_tensor.shape}\")\n            \n        return image_tensor, mask_tensor","metadata":{},"outputs":[],"execution_count":null},{"id":"1ced46c3","cell_type":"markdown","source":"### 2.5 Dataset Organization and Augmentation","metadata":{}},{"id":"593e5929","cell_type":"code","source":"## 2.5 Dataset Organization and Augmentation\n\n# We'll create a more robust data pipeline with strong augmentation to improve model generalization\nprint(\"Setting up improved data organization and augmentation pipeline...\")\n\n# Import additional libraries for augmentation\n!pip install -q albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport random\nfrom sklearn.model_selection import KFold\n\n# Define a function to organize the dataset more systematically\ndef organize_dataset(dataset_location, val_split=0.2, test_split=0.1, seed=42):\n    \"\"\"\n    Organize dataset into properly stratified train/val/test splits\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    # Identify all image files\n    train_img_dir = os.path.join(dataset_location, 'train', 'images')\n    val_img_dir = os.path.join(dataset_location, 'valid', 'images')\n    \n    train_files = [f for f in os.listdir(train_img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n    val_files = [f for f in os.listdir(val_img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n    \n    print(f\"Found {len(train_files)} training images and {len(val_files)} validation images\")\n    \n    # If validation set already exists and is reasonable size, use it\n    if len(val_files) > 0 and len(val_files) >= len(train_files) * 0.1:\n        print(f\"Using existing validation split with {len(val_files)} images\")\n        return {\n            'train': train_files,\n            'val': val_files,\n            'test': val_files[:max(1, int(len(val_files) * 0.5))]  # Use part of validation as test set\n        }\n    \n    # Otherwise, create our own splits\n    all_files = train_files + val_files\n    np.random.shuffle(all_files)\n    \n    # Calculate split sizes\n    test_size = max(1, int(len(all_files) * test_split))\n    val_size = max(1, int(len(all_files) * val_split))\n    train_size = len(all_files) - val_size - test_size\n    \n    # Create splits\n    train_files = all_files[:train_size]\n    val_files = all_files[train_size:train_size+val_size]\n    test_files = all_files[train_size+val_size:]\n    \n    print(f\"Created new splits: {len(train_files)} training, {len(val_files)} validation, {len(test_files)} test images\")\n    \n    return {\n        'train': train_files,\n        'val': val_files,\n        'test': test_files\n    }\n\n# Define robust augmentation pipelines for training and validation\ndef get_training_augmentation(img_size=256):\n    \"\"\"\n    Returns a strong augmentation pipeline for training to improve generalization\n    \"\"\"\n    return A.Compose([\n        A.RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.0)),\n        A.Flip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n        A.OneOf([\n            A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n            A.GridDistortion(p=0.5),\n            A.OpticalDistortion(distort_limit=1.0, shift_limit=0.5, p=0.5),\n        ], p=0.3),\n        A.OneOf([\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        ], p=0.3),\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\ndef get_validation_augmentation(img_size=256):\n    \"\"\"\n    Returns minimal augmentation for validation (just resizing and normalization)\n    \"\"\"\n    return A.Compose([\n        A.Resize(height=img_size, width=img_size),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\n# Create an improved FarmlandDataset class with better augmentation handling\nclass ImprovedFarmlandDataset(Dataset):\n    def __init__(self, dataset_location, file_names, transform=None, farm_class_id=None, img_size=256, mode='train'):\n        self.dataset_location = dataset_location\n        self.file_names = file_names\n        self.transform = transform\n        self.img_size = img_size\n        self.mode = mode\n        \n        # Determine the image and mask directories based on mode\n        if mode == 'train':\n            self.img_dir = os.path.join(dataset_location, 'train', 'images')\n            self.mask_dir = os.path.join(dataset_location, 'train', 'labels')\n        else:  # val or test\n            self.img_dir = os.path.join(dataset_location, 'valid', 'images')\n            self.mask_dir = os.path.join(dataset_location, 'valid', 'labels')\n        \n        # Try to load class names from data.yaml\n        self.class_names = ['Unknown']\n        self.farm_class_id = farm_class_id\n        yaml_path = os.path.join(dataset_location, 'data.yaml')\n        \n        if os.path.exists(yaml_path):\n            try:\n                import yaml\n                with open(yaml_path, 'r') as f:\n                    data_yaml = yaml.safe_load(f)\n                    if 'names' in data_yaml:\n                        self.class_names = data_yaml['names']\n                        # If farm_class_id is not provided, try to find \"Farms\" class automatically\n                        if self.farm_class_id is None:\n                            for i, name in enumerate(self.class_names):\n                                if \"farm\" in name.lower():\n                                    print(f\"Found Farm class: {name} with ID {i}\")\n                                    self.farm_class_id = i\n                                    break\n            except Exception as e:\n                print(f\"Error reading class names from data.yaml: {e}\")\n    \n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, idx):\n        # Get file name\n        img_file = self.file_names[idx]\n        \n        # Ensure the file exists in the directory\n        img_path = os.path.join(self.img_dir, img_file)\n        if not os.path.exists(img_path):\n            # If not, it might be in the other directory\n            alt_img_dir = os.path.join(self.dataset_location, 'valid' if self.mode == 'train' else 'train', 'images')\n            img_path = os.path.join(alt_img_dir, img_file)\n            \n            # Update mask directory too\n            alt_mask_dir = os.path.join(self.dataset_location, 'valid' if self.mode == 'train' else 'train', 'labels')\n            self.mask_dir = alt_mask_dir\n        \n        # Load image\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Generate mask from YOLOv8 annotation\n        mask_file = os.path.splitext(img_file)[0] + '.txt'\n        mask_path = os.path.join(self.mask_dir, mask_file)\n        \n        # Create empty mask with the same size as the image\n        mask = np.zeros(image.shape[:2], dtype=np.float32)\n        \n        if os.path.exists(mask_path):\n            # Read YOLOv8 format annotations\n            with open(mask_path, 'r') as f:\n                lines = f.readlines()\n            \n            img_height, img_width = image.shape[:2]\n            for line in lines:\n                parts = line.strip().split(' ')\n                if len(parts) >= 5:  # Basic format check for bounding box\n                    # Extract class ID\n                    class_id = int(parts[0])\n                    \n                    # Only process the farm class if specified, otherwise process all classes\n                    if self.farm_class_id is None or class_id == self.farm_class_id:\n                        # Check if we have polygon points (instance segmentation)\n                        if len(parts) > 5:\n                            # Extract polygon points\n                            polygon_points = []\n                            for i in range(5, len(parts), 2):\n                                if i+1 < len(parts):\n                                    x = float(parts[i]) * img_width\n                                    y = float(parts[i+1]) * img_height\n                                    polygon_points.append((x, y))\n                            \n                            if polygon_points:\n                                # Convert to numpy array for OpenCV\n                                pts = np.array(polygon_points, np.int32)\n                                pts = pts.reshape((-1, 1, 2))\n                                # Fill polygon with ones\n                                cv2.fillPoly(mask, [pts], 1)\n                        else:\n                            # Use bounding box if no polygon points\n                            x_center = float(parts[1]) * img_width\n                            y_center = float(parts[2]) * img_height\n                            width = float(parts[3]) * img_width\n                            height = float(parts[4]) * img_height\n                            \n                            x1 = int(x_center - width / 2)\n                            y1 = int(y_center - height / 2)\n                            x2 = int(x_center + width / 2)\n                            y2 = int(y_center + height / 2)\n                            \n                            cv2.rectangle(mask, (x1, y1), (x2, y2), 1, -1)\n        \n        # Apply transformations using albumentations\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image_tensor = augmented['image']\n            mask_tensor = augmented['mask']\n            \n            # Ensure mask is single channel\n            if len(mask_tensor.shape) == 2:\n                mask_tensor = mask_tensor.unsqueeze(0)\n        else:\n            # Fallback if no transform is provided\n            image = cv2.resize(image, (self.img_size, self.img_size))\n            mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n            \n            # Convert to tensor\n            image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n            mask_tensor = torch.from_numpy(mask).float().unsqueeze(0)\n        \n        # Verify shapes for debugging\n        if idx < 3 and self.mode == 'train':\n            print(f\"{self.mode} - Image shape: {image_tensor.shape}, Mask shape: {mask_tensor.shape}\")\n            \n        return image_tensor, mask_tensor\n\n# Organize dataset and create splits\nsplits = organize_dataset(dataset.location)\n\n# Create datasets with appropriate augmentations\ntrain_dataset = ImprovedFarmlandDataset(\n    dataset_location=dataset.location,\n    file_names=splits['train'],\n    transform=get_training_augmentation(img_size=256),\n    mode='train'\n)\n\nval_dataset = ImprovedFarmlandDataset(\n    dataset_location=dataset.location, \n    file_names=splits['val'],\n    transform=get_validation_augmentation(img_size=256),\n    mode='val'\n)\n\ntest_dataset = ImprovedFarmlandDataset(\n    dataset_location=dataset.location,\n    file_names=splits['test'],\n    transform=get_validation_augmentation(img_size=256),\n    mode='val'\n)\n\nprint(f\"Created augmented datasets:\")\nprint(f\"  - Training set: {len(train_dataset)} images\")\nprint(f\"  - Validation set: {len(val_dataset)} images\") \nprint(f\"  - Test set: {len(test_dataset)} images\")\n\n# Visualize some augmented samples to verify the pipeline\ndef show_augmented_samples(dataset, num_samples=3, figsize=(18, 12)):\n    \"\"\"Show augmented samples from the dataset\"\"\"\n    fig, axes = plt.subplots(num_samples, 2, figsize=figsize)\n    \n    for i in range(num_samples):\n        idx = np.random.randint(0, len(dataset))\n        image, mask = dataset[idx]\n        \n        # Convert tensors to numpy arrays for visualization\n        image_np = image.permute(1, 2, 0).cpu().numpy()\n        mask_np = mask.squeeze().cpu().numpy()\n        \n        # De-normalize image for better visualization\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image_np = (image_np * std + mean)\n        image_np = np.clip(image_np, 0, 1)\n        \n        # Display\n        axes[i, 0].imshow(image_np)\n        axes[i, 0].set_title(f\"Augmented Image {i+1}\")\n        axes[i, 0].axis(\"off\")\n        \n        axes[i, 1].imshow(mask_np, cmap='gray')\n        axes[i, 1].set_title(f\"Corresponding Mask {i+1}\")\n        axes[i, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize some augmented training samples\nprint(\"Visualizing augmented training samples:\")\nshow_augmented_samples(train_dataset, num_samples=3)\n\n# Update data loaders with the improved datasets\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=min(os.cpu_count(), 8),\n    pin_memory=True,\n    prefetch_factor=2,\n    persistent_workers=True,\n    drop_last=True  # Drop last incomplete batch for more stable training\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=min(os.cpu_count(), 4),\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=min(os.cpu_count(), 4),\n    pin_memory=True\n)","metadata":{},"outputs":[],"execution_count":null},{"id":"a050d223","cell_type":"markdown","source":"## 3.2 Hyperparameter Tuning and Anti-Overfitting Techniques","metadata":{}},{"id":"359cd06d","cell_type":"code","source":"# Implement hyperparameter tuning and regularization to optimize the model\nprint(\"Setting up hyperparameter tuning and anti-overfitting techniques...\")\n\n# Import additional libraries for hyperparameter tuning\n!pip install -q optuna\nimport optuna\nfrom functools import partial\nimport copy\n\n# Define regularization techniques to prevent overfitting\nclass UNetWithRegularization(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, dropout_rate=0.2, use_dropout=True):\n        super(UNetWithRegularization, self).__init__()\n        self.use_dropout = use_dropout\n        self.dropout_rate = dropout_rate\n        \n        # Encoder (downsampling)\n        self.enc1 = DoubleConv(n_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Dropout for bottleneck (most important for preventing overfitting)\n        self.dropout = nn.Dropout2d(p=self.dropout_rate) if use_dropout else nn.Identity()\n        \n        # Bottleneck\n        self.bottleneck = DoubleConv(512, 1024)\n        \n        # Decoder (upsampling)\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec4 = DoubleConv(1024, 512)  # 512 + 512 input channels from skip connection\n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = DoubleConv(512, 256)   # 256 + 256 input channels from skip connection\n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = DoubleConv(256, 128)   # 128 + 128 input channels from skip connection\n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = DoubleConv(128, 64)    # 64 + 64 input channels from skip connection\n        \n        # Final output layer\n        self.outconv = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        p1 = self.pool1(e1)\n        \n        e2 = self.enc2(p1)\n        p2 = self.pool2(e2)\n        \n        e3 = self.enc3(p2)\n        p3 = self.pool3(e3)\n        \n        e4 = self.enc4(p3)\n        p4 = self.pool4(e4)\n        \n        # Bottleneck with dropout\n        p4 = self.dropout(p4)\n        b = self.bottleneck(p4)\n        \n        # Decoder with skip connections\n        u4 = self.upconv4(b)\n        u4 = torch.cat([u4, e4], dim=1)  # Skip connection\n        d4 = self.dec4(u4)\n        \n        u3 = self.upconv3(d4)\n        u3 = torch.cat([u3, e3], dim=1)  # Skip connection\n        d3 = self.dec3(u3)\n        \n        u2 = self.upconv2(d3)\n        u2 = torch.cat([u2, e2], dim=1)  # Skip connection\n        d2 = self.dec2(u2)\n        \n        u1 = self.upconv1(d2)\n        u1 = torch.cat([u1, e1], dim=1)  # Skip connection\n        d1 = self.dec1(u1)\n        \n        # Final output\n        out = self.outconv(d1)\n        return torch.sigmoid(out)  # Apply sigmoid for binary segmentation\n\n# Define weighted loss function to handle class imbalance\nclass WeightedBCELoss(nn.Module):\n    def __init__(self, pos_weight=2.0):\n        super(WeightedBCELoss, self).__init__()\n        self.pos_weight = pos_weight\n        \n    def forward(self, outputs, targets):\n        # Apply weighted loss - higher weight for positive class (farmland)\n        loss = F.binary_cross_entropy(outputs, targets, reduction='none')\n        weights = targets * (self.pos_weight - 1.0) + 1.0\n        weighted_loss = (loss * weights).mean()\n        return weighted_loss\n\n# Define combo loss (BCE + Dice loss) for better segmentation\nclass DiceBCELoss(nn.Module):\n    def __init__(self, dice_weight=1.0, bce_weight=1.0):\n        super(DiceBCELoss, self).__init__()\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n        self.bce_loss = nn.BCELoss()\n        \n    def forward(self, outputs, targets):\n        # BCE Loss\n        bce_loss = self.bce_loss(outputs, targets)\n        \n        # Dice Loss\n        intersection = (outputs * targets).sum()\n        dice_loss = 1 - (2. * intersection + 1.0) / (outputs.sum() + targets.sum() + 1.0)\n        \n        # Combined loss\n        combo_loss = self.bce_weight * bce_loss + self.dice_weight * dice_loss\n        return combo_loss\n\n# Define the objective function for Optuna hyperparameter tuning\ndef objective(trial, max_epochs=10, device=device):\n    # Define hyperparameters to tune\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2),\n        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),\n        'loss_type': trial.suggest_categorical('loss_type', ['bce', 'weighted_bce', 'dice_bce']),\n        'pos_weight': trial.suggest_float('pos_weight', 1.0, 5.0) if trial.params.get('loss_type', '') == 'weighted_bce' else 2.0,\n        'dice_weight': trial.suggest_float('dice_weight', 0.5, 2.0) if trial.params.get('loss_type', '') == 'dice_bce' else 1.0,\n    }\n    \n    # Create mini train/val datasets for faster tuning\n    mini_train_size = min(len(train_dataset), 100)  # Use at most 100 samples for tuning\n    mini_val_size = min(len(val_dataset), 50)       # Use at most 50 samples for validation\n    \n    indices_train = np.random.choice(len(train_dataset), mini_train_size, replace=False)\n    indices_val = np.random.choice(len(val_dataset), mini_val_size, replace=False)\n    \n    mini_train_dataset = torch.utils.data.Subset(train_dataset, indices_train)\n    mini_val_dataset = torch.utils.data.Subset(val_dataset, indices_val)\n    \n    # Create dataloaders with the selected batch size\n    mini_train_loader = DataLoader(\n        mini_train_dataset, \n        batch_size=params['batch_size'], \n        shuffle=True, \n        num_workers=4,\n        pin_memory=True\n    )\n    \n    mini_val_loader = DataLoader(\n        mini_val_dataset, \n        batch_size=params['batch_size'], \n        shuffle=False, \n        num_workers=4,\n        pin_memory=True\n    )\n    \n    # Initialize model with regularization\n    model = UNetWithRegularization(\n        n_channels=3, \n        n_classes=1, \n        dropout_rate=params['dropout_rate'],\n        use_dropout=True\n    ).to(device)\n    \n    # Initialize optimizer with weight decay (L2 regularization)\n    optimizer = optim.Adam(\n        model.parameters(), \n        lr=params['learning_rate'],\n        weight_decay=params['weight_decay']  # L2 regularization\n    )\n    \n    # Initialize learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=3,\n        verbose=True\n    )\n    \n    # Initialize loss function based on hyperparameters\n    if params['loss_type'] == 'weighted_bce':\n        criterion = WeightedBCELoss(pos_weight=params['pos_weight'])\n    elif params['loss_type'] == 'dice_bce':\n        criterion = DiceBCELoss(dice_weight=params['dice_weight'], bce_weight=1.0)\n    else:  # default: 'bce'\n        criterion = nn.BCELoss()\n    \n    # Train for a few epochs\n    best_val_loss = float('inf')\n    \n    for epoch in range(max_epochs):\n        # Train\n        model.train()\n        train_loss = 0\n        \n        for images, masks in mini_train_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * images.size(0)\n        \n        train_loss /= len(mini_train_loader.dataset)\n        \n        # Validate\n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for images, masks in mini_val_loader:\n                images = images.to(device)\n                masks = masks.to(device)\n                \n                # Forward pass\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                \n                val_loss += loss.item() * images.size(0)\n        \n        val_loss /= len(mini_val_loader.dataset)\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Update best validation loss\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n        \n        # Report intermediate objective value\n        trial.report(val_loss, epoch)\n        \n        # Handle pruning based on the intermediate value\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    \n    return best_val_loss\n\n# Function to run hyperparameter tuning\ndef run_hyperparameter_tuning(n_trials=20, max_epochs=10):\n    print(f\"Starting hyperparameter tuning with {n_trials} trials...\")\n    \n    # Create a study object and optimize the objective function\n    study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n    study.optimize(partial(objective, max_epochs=max_epochs, device=device), n_trials=n_trials)\n    \n    # Get the best hyperparameters\n    best_params = study.best_params\n    print(f\"Best hyperparameters: {best_params}\")\n    print(f\"Best validation loss: {study.best_value:.4f}\")\n    \n    # Visualize the hyperparameter tuning results\n    try:\n        # Plot optimization history\n        plt.figure(figsize=(10, 6))\n        optuna.visualization.matplotlib.plot_optimization_history(study)\n        plt.tight_layout()\n        plt.show()\n        \n        # Plot parameter importances\n        plt.figure(figsize=(10, 6))\n        optuna.visualization.matplotlib.plot_param_importances(study)\n        plt.tight_layout()\n        plt.show()\n    except:\n        print(\"Could not generate Optuna visualization plots\")\n    \n    return best_params\n\n# Run a quick hyperparameter search with fewer trials for demonstration\nbest_params = run_hyperparameter_tuning(n_trials=5, max_epochs=5)\n\n# Create the optimized model with the best hyperparameters\noptimized_model = UNetWithRegularization(\n    n_channels=3, \n    n_classes=1,\n    dropout_rate=best_params.get('dropout_rate', 0.2),\n    use_dropout=True\n).to(device)\n\n# Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training of optimized model\")\n    optimized_model = nn.DataParallel(optimized_model)\n\n# Create the optimized loss function\nif best_params.get('loss_type') == 'weighted_bce':\n    criterion = WeightedBCELoss(pos_weight=best_params.get('pos_weight', 2.0))\nelif best_params.get('loss_type') == 'dice_bce':\n    criterion = DiceBCELoss(\n        dice_weight=best_params.get('dice_weight', 1.0), \n        bce_weight=1.0\n    )\nelse:\n    criterion = nn.BCELoss()\n\n# Create the optimized optimizer with weight decay for regularization\noptimizer = optim.Adam(\n    optimized_model.parameters(),\n    lr=best_params.get('learning_rate', 0.001),\n    weight_decay=best_params.get('weight_decay', 1e-4)  # L2 regularization\n)\n\n# Create learning rate scheduler for better convergence\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=5,\n    verbose=True\n)\n\n# Print model and optimization summary\nprint(\"\\nOptimized Model Configuration:\")\nprint(f\"- Dropout Rate: {best_params.get('dropout_rate', 0.2)}\")\nprint(f\"- Learning Rate: {best_params.get('learning_rate', 0.001)}\")\nprint(f\"- Weight Decay: {best_params.get('weight_decay', 1e-4)}\")\nprint(f\"- Loss Function: {best_params.get('loss_type', 'bce')}\")\nif best_params.get('loss_type') == 'weighted_bce':\n    print(f\"- Positive Class Weight: {best_params.get('pos_weight', 2.0)}\")\nelif best_params.get('loss_type') == 'dice_bce':\n    print(f\"- Dice Weight: {best_params.get('dice_weight', 1.0)}\")\nprint(f\"- Batch Size: {best_params.get('batch_size', 16)}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"29f62063","cell_type":"markdown","source":"## 3.2.1 Evaluation Metrics","metadata":{}},{"id":"1b686382","cell_type":"code","source":"# Define evaluation metrics and functions\ndef calculate_iou(outputs, targets, threshold=0.5):\n    \"\"\"\n    Calculate IoU (Intersection over Union) for segmentation masks\n    Args:\n        outputs: Model predictions (B, 1, H, W)\n        targets: Ground truth masks (B, 1, H, W)\n        threshold: Threshold to binarize predictions\n    Returns:\n        Average IoU across batch\n    \"\"\"\n    # Binarize outputs at the given threshold\n    outputs = (outputs > threshold).float()\n    \n    # Calculate intersection and union\n    intersection = (outputs * targets).sum(dim=(1, 2, 3))  # Sum over HWC dimensions for each sample\n    union = outputs.sum(dim=(1, 2, 3)) + targets.sum(dim=(1, 2, 3)) - intersection\n    \n    # Handle empty masks - set IoU to 1 if both prediction and target are empty\n    iou = torch.zeros_like(intersection)\n    valid = union > 0\n    iou[valid] = intersection[valid] / union[valid]\n    iou[~valid] = 1.0  # If both prediction and target are empty, IoU is 1\n    \n    # Return batch average\n    return iou.mean().item()\n\ndef evaluate_model(model, dataloader, device, threshold=0.5):\n    \"\"\"Evaluate model on dataloader and return average IoU\"\"\"\n    model.eval()\n    iou_sum = 0.0\n    count = 0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Calculate IoU\n            batch_iou = calculate_iou(outputs, masks, threshold)\n            iou_sum += batch_iou * images.size(0)\n            count += images.size(0)\n    \n    return iou_sum / count if count > 0 else 0.0\n\ndef visualize_predictions(model, dataloader, device, num_samples=3, threshold=0.5):\n    \"\"\"Visualize model predictions alongside ground truth masks\"\"\"\n    model.eval()\n    \n    # Get a batch of images and masks\n    images, masks = next(iter(dataloader))\n    images = images.to(device)\n    masks = masks.to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        outputs = model(images)\n        binary_outputs = (outputs > threshold).float()\n    \n    # Visualize only a subset of the batch\n    n = min(num_samples, images.size(0))\n    fig, axes = plt.subplots(n, 3, figsize=(15, 5*n))\n    \n    for i in range(n):\n        # Get image, mask, and prediction for this sample\n        img = images[i].cpu()\n        mask = masks[i].cpu()\n        pred = outputs[i].cpu()\n        binary_pred = binary_outputs[i].cpu()\n        \n        # Denormalize the image for visualization\n        img = img.permute(1, 2, 0).numpy()\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img = (img * std + mean)\n        img = np.clip(img, 0, 1)\n        \n        # Convert masks to numpy\n        mask = mask.squeeze().numpy()\n        pred = pred.squeeze().numpy()\n        binary_pred = binary_pred.squeeze().numpy()\n        \n        # Plot\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title(\"Input Image\")\n        axes[i, 0].axis(\"off\")\n        \n        axes[i, 1].imshow(mask, cmap='gray')\n        axes[i, 1].set_title(\"Ground Truth Mask\")\n        axes[i, 1].axis(\"off\")\n        \n        axes[i, 2].imshow(binary_pred, cmap='gray')\n        iou = calculate_iou(\n            torch.tensor(pred).unsqueeze(0).unsqueeze(0), \n            torch.tensor(mask).unsqueeze(0).unsqueeze(0), \n            threshold\n        )\n        axes[i, 2].set_title(f\"Prediction (IoU: {iou:.4f})\")\n        axes[i, 2].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return binary_outputs","metadata":{},"outputs":[],"execution_count":null},{"id":"2824bc27","cell_type":"markdown","source":"## 3.3 Improved Training Loop with Anti-Overfitting Techniques","metadata":{}},{"id":"c28e44fe","cell_type":"code","source":"# Train the model with the optimized hyperparameters and anti-overfitting measures\nprint(\"Starting training with optimized model and anti-overfitting techniques...\")\n\n# Define improved training functions with early stopping\ndef train_epoch_with_regularization(model, dataloader, criterion, optimizer, device, grad_clip=1.0):\n    \"\"\"Train for one epoch with gradient clipping to prevent exploding gradients\"\"\"\n    model.train()\n    running_loss = 0.0\n    \n    for images, masks in tqdm(dataloader, desc=\"Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        \n        # Backward pass and optimize with gradient clipping\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Apply gradient clipping to prevent exploding gradients (another anti-overfitting technique)\n        if grad_clip > 0:\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n        \n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\n# Early stopping implementation\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, verbose=True):\n        \"\"\"\n        Early stopping to stop training when validation loss doesn't improve\n        Args:\n            patience (int): How many epochs to wait after last improvement\n            min_delta (float): Minimum change to qualify as improvement\n            verbose (bool): If True, prints a message for each improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.early_stop = False\n        \n    def __call__(self, val_loss, model, path):\n        if val_loss < self.best_loss - self.min_delta:\n            if self.verbose:\n                print(f\"Validation loss improved from {self.best_loss:.4f} to {val_loss:.4f}. Saving model...\")\n            self.best_loss = val_loss\n            self.counter = 0\n            torch.save(model.state_dict(), path)\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"Validation loss did not improve. Counter: {self.counter}/{self.patience}\")\n            if self.counter >= self.patience:\n                if self.verbose:\n                    print(\"Early stopping triggered\")\n                self.early_stop = True\n        \n        return self.early_stop\n\n# Define validation function\ndef validate_epoch(model, dataloader, criterion, device):\n    \"\"\"Validate the model for one epoch\"\"\"\n    model.eval()\n    running_loss = 0.0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Validating\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\n# Set up training parameters\nnum_epochs = 50  # Maximum number of epochs\nbest_model_path = 'best_unet_regularized_model.pth'\n\n# Set up early stopping\nearly_stopping = EarlyStopping(patience=10, verbose=True)\n\n# Lists to store training history\ntrain_losses = []\nval_losses = []\nval_ious = []\nlearning_rates = []\n\n# Training loop with early stopping and regularization\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    learning_rates.append(current_lr)\n    print(f\"Current learning rate: {current_lr:.7f}\")\n    \n    # Train with gradient clipping\n    train_loss = train_epoch_with_regularization(\n        optimized_model, \n        train_loader, \n        criterion, \n        optimizer, \n        device,\n        grad_clip=1.0  # Apply gradient clipping to prevent exploding gradients\n    )\n    train_losses.append(train_loss)\n    \n    # Validate\n    val_loss = validate_epoch(optimized_model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    \n    # Calculate IoU\n    val_iou = evaluate_model(optimized_model, val_loader, device)\n    val_ious.append(val_iou)\n    \n    # Print epoch results\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    print(f\"Validation IoU: {val_iou:.4f}\")\n    \n    # Update learning rate based on validation loss\n    scheduler.step(val_loss)\n    \n    # Check early stopping\n    if early_stopping(val_loss, optimized_model, best_model_path):\n        print(\"Early stopping triggered\")\n        break\n    \n    print(\"-\" * 50)\n\n# Load the best model for evaluation\noptimized_model.load_state_dict(torch.load(best_model_path))\nprint(\"Training completed with early stopping!\")\n\n# Final evaluation on test set\ntest_loss = validate_epoch(optimized_model, test_loader, criterion, device)\ntest_iou = evaluate_model(optimized_model, test_loader, device)\n\nprint(f\"Final Test Loss: {test_loss:.4f}\")\nprint(f\"Final Test IoU: {test_iou:.4f}\")\n\n# Check if the model achieved the success criteria\nif test_iou > 0.75:\n    print(\"✅ Success! Model achieved the target IoU (> 0.75)\")\nelse:\n    print(f\"Model achieved {test_iou:.4f} IoU, which is below the target of 0.75\")","metadata":{},"outputs":[],"execution_count":null}]}