{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9fdb79df","cell_type":"markdown","source":"# FarmWise: Farmland Segmentation and Size Classification with U-Net\n\n**Date**: April 14, 2025\n\nThis notebook implements a farm segmentation system using U-Net architecture to identify agricultural fields from satellite imagery, calculate their sizes, and classify them for targeted recommendations.\n\n## Project Overview\n\n**Goal**: Create a system that can:\n1. Detect and segment farmlands from satellite imagery\n2. Calculate the size/area of each identified farm\n3. Classify farms by size (small, medium, large)\n4. Enable a recommendation system based on farm size classification\n\n**Approach**: U-Net architecture for semantic segmentation","metadata":{}},{"id":"7c32e0b7","cell_type":"markdown","source":"## 1. Business Understanding\n\n### 1.1 Problem Statement\n\nAgricultural recommendations are most effective when tailored to the specific context of a farm, with farm size being a crucial factor. Large farms may benefit from different techniques, equipment, and crop selections compared to small ones. This project aims to automatically classify farms by size from satellite imagery to enable targeted recommendations.\n\n### 1.2 Success Criteria\n\n- **Technical Success**: Achieve high accuracy in farmland segmentation (IoU > 0.75)\n- **Business Success**: Enable accurate size-based classification of farms for targeted recommendations","metadata":{}},{"id":"f9c0f576","cell_type":"markdown","source":"## 2. Data Acquisition and Understanding\n\n### 2.1 Setup and Environment Preparation","metadata":{}},{"id":"5f4dd48b","cell_type":"code","source":"# Check for Kaggle environment and set up dependencies for GPU acceleration\n!pip install torch torchvision matplotlib numpy pillow scikit-learn scikit-image opencv-python roboflow tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:12.528483Z","iopub.execute_input":"2025-04-14T19:50:12.528848Z","iopub.status.idle":"2025-04-14T19:50:18.781048Z","shell.execute_reply.started":"2025-04-14T19:50:12.528818Z","shell.execute_reply":"2025-04-14T19:50:18.780213Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.25.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nCollecting roboflow\n  Downloading roboflow-1.1.61-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.12.12)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2025.1.31)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\nCollecting pillow-heif>=0.18.0 (from roboflow)\n  Downloading pillow_heif-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting python-dotenv (from roboflow)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.3.0)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nDownloading roboflow-1.1.61-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow_heif-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: filetype, python-dotenv, pillow-heif, idna, roboflow\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.61\n","output_type":"stream"}],"execution_count":1},{"id":"371ef204","cell_type":"code","source":"# Import required libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom skimage import measure\nfrom tqdm.notebook import tqdm\nfrom roboflow import Roboflow\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Check for GPU availability and set up CUDA device\nprint(\"CUDA Available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    \n    # Use all available GPUs if there are multiple\n    if num_gpus > 1:\n        device = torch.device('cuda')\n        print(f\"Using {num_gpus} GPUs for data parallel training\")\n    else:\n        device = torch.device('cuda:0')\n        print(\"Using single GPU\")\nelse:\n    device = torch.device('cpu')\n    print(\"No GPU available, using CPU. This will be slower.\")\n\n# Display CUDA version if available\nif torch.cuda.is_available():\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"Current CUDA device: {torch.cuda.current_device()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:18.782238Z","iopub.execute_input":"2025-04-14T19:50:18.782489Z","iopub.status.idle":"2025-04-14T19:50:24.327976Z","shell.execute_reply.started":"2025-04-14T19:50:18.782467Z","shell.execute_reply":"2025-04-14T19:50:24.327002Z"}},"outputs":[{"name":"stdout","text":"CUDA Available: True\nNumber of GPUs available: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\nUsing 2 GPUs for data parallel training\nCUDA Version: 12.1\nCurrent CUDA device: 0\n","output_type":"stream"}],"execution_count":2},{"id":"006a57f6","cell_type":"markdown","source":"### 2.2 Data Acquisition from Roboflow","metadata":{}},{"id":"f64caafc","cell_type":"code","source":"# Initialize Roboflow and load dataset\n# Note: You will need to provide your Roboflow API key\nrf = Roboflow(api_key=\"HE9CEH5JxJ3U0vXrQTOy\")  # Replace with your actual API key\nproject = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\ndataset = project.version(1).download(\"yolov8\")\n\n# Print dataset path\nprint(f\"Dataset downloaded to: {dataset.location}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:24.329843Z","iopub.execute_input":"2025-04-14T19:50:24.330227Z","iopub.status.idle":"2025-04-14T19:50:33.739479Z","shell.execute_reply.started":"2025-04-14T19:50:24.330205Z","shell.execute_reply":"2025-04-14T19:50:33.738528Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Final-Detectron-2-1 to yolov8:: 100%|██████████| 417917/417917 [00:05<00:00, 78149.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Final-Detectron-2-1 in yolov8:: 100%|██████████| 15024/15024 [00:02<00:00, 6347.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset downloaded to: /kaggle/working/Final-Detectron-2-1\n","output_type":"stream"}],"execution_count":3},{"id":"6cec85ec","cell_type":"markdown","source":"### 2.3 Dataset Exploration","metadata":{}},{"id":"59b1ed2e","cell_type":"code","source":"# Explore the dataset structure\ndef explore_directory(path, level=0):\n    print('  ' * level + f\"|-- {os.path.basename(path)}\")\n    if os.path.isdir(path):\n        for item in os.listdir(path)[:10]:  # Limit to first 10 items\n            item_path = os.path.join(path, item)\n            if os.path.isdir(item_path):\n                explore_directory(item_path, level + 1)\n            else:\n                print('  ' * (level + 1) + f\"|-- {item}\")\n        if len(os.listdir(path)) > 10:\n            print('  ' * (level + 1) + f\"|-- ... ({len(os.listdir(path)) - 10} more items)\")\n\nprint(\"Dataset Structure:\")\nexplore_directory(dataset.location)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:33.741003Z","iopub.execute_input":"2025-04-14T19:50:33.741351Z","iopub.status.idle":"2025-04-14T19:50:33.790973Z","shell.execute_reply.started":"2025-04-14T19:50:33.741326Z","shell.execute_reply":"2025-04-14T19:50:33.790209Z"}},"outputs":[{"name":"stdout","text":"Dataset Structure:\n|-- Final-Detectron-2-1\n  |-- data.yaml\n  |-- valid\n    |-- labels\n      |-- F_30002908_jpg.rf.c1e99899283f37a5362add00c9bf06f1.txt\n      |-- h478_jpg.rf.902c82369dce960aed862f90cc5229cf.txt\n      |-- F_30001532_jpg.rf.e3d561e4cea6cce5406404309e2423ed.txt\n      |-- tile_13_12_png_jpg.rf.3802c2919ab91788ba2372ec9d01718f.txt\n      |-- h512_jpg.rf.e40e01492dc79c5992a8303087144827.txt\n      |-- F_2286_jpg.rf.74058111d31d48e36434e7b490689794.txt\n      |-- 021323333332230_png_jpg.rf.49a6557421a9d7b065ac407bcb9b0dfc.txt\n      |-- h492_jpg.rf.37204b4ba72bc151ded3d1023acfed07.txt\n      |-- h472_jpg.rf.72ca09b7e700d2e52ba8e8db2ac3f221.txt\n      |-- F_30001685_jpg.rf.c1ef08269d69c63a5044377085908085.txt\n      |-- ... (708 more items)\n    |-- images\n      |-- F_30001623_jpg.rf.3d9fc42d0589cba0315ddecfb1b0b0d8.jpg\n      |-- F_30001723_jpg.rf.337780f30d1cbe933a7b244755a4b757.jpg\n      |-- F_3000241_jpg.rf.ec4d03431504cd165dd4eb2bf57ae400.jpg\n      |-- F_30002428_jpg.rf.687391f822991aebc43e5b6dd9fa8a60.jpg\n      |-- F_361_jpg.rf.ab9fdeddbe59e4abfe47d6444c7d5ab3.jpg\n      |-- F_30002312_jpg.rf.72f79621e3491c1907cca1a945acb150.jpg\n      |-- F_30001670_jpg.rf.ce273ecb8703f821883cdca2e9bdc4c1.jpg\n      |-- 313110231233100_png_jpg.rf.684454baff6a624fb8eae32b79dd7fd6.jpg\n      |-- F_30001685_jpg.rf.c1ef08269d69c63a5044377085908085.jpg\n      |-- F_30002997_jpg.rf.099bfb9f8aec3a869d7b2f031fe2ebc8.jpg\n      |-- ... (708 more items)\n  |-- test\n    |-- labels\n      |-- 120220012012210_png_jpg.rf.6181187d7a567d62e37c7e54f2e1a80c.txt\n      |-- 0231311230201003_png_jpg.rf.39eadfe714b0e3bccbc2435e38c59088.txt\n      |-- F_30002065_jpg.rf.b3ad3f04c18076df414cdc9f986fbb24.txt\n      |-- testnew3_jpg.rf.45a9bf354b9986e724783777e6181328.txt\n      |-- 032010003021100_jpg.rf.4dbc2752c19b112ad1ceed4da8cfaef0.txt\n      |-- 0320013333103231_png_jpg.rf.2c850594fb5f2cff300180dcc8b3fda7.txt\n      |-- testnew9_jpg.rf.cdb729640b496efc54078ba3720333e2.txt\n      |-- F_30001494_jpg.rf.735079957ae9f03c7c4ea0404d5db145.txt\n      |-- F_30002374_jpg.rf.02c121468273922c81b735aa0bb4e82b.txt\n      |-- F_30002500_jpg.rf.f0e66e62ef6d0220b1632dcb806cb061.txt\n      |-- ... (820 more items)\n    |-- images\n      |-- F_1598_jpg.rf.67cdefe0a8c62742f1f9e531c402b9a9.jpg\n      |-- F_315_jpg.rf.657a702220823f75202cd10f6f1251a1.jpg\n      |-- Image1325_jpg.rf.6dee2a3e7c38c3109113774d1e025927.jpg\n      |-- F_30002901_jpg.rf.b8a8c37dd0127c364cdb67153556360d.jpg\n      |-- F_30001764_jpg.rf.8dca1b449a4cae214cf92cd7f75f3ef4.jpg\n      |-- F_30002587_jpg.rf.7adfe1a8c76527dfdb785229f10b8deb.jpg\n      |-- F_30002964_jpg.rf.9e4c119b7c67e532799a50ca3798a2bc.jpg\n      |-- F_30002697_jpg.rf.14fe59a1bd98012c1fca8430d1a6f837.jpg\n      |-- 0302320121312132_png_jpg.rf.4ac0d1b8cd10a89f648c82da30af8ef0.jpg\n      |-- F_30001495_jpg.rf.679c3c0e1e27c818c21585884249ae41.jpg\n      |-- ... (820 more items)\n  |-- train\n    |-- labels\n      |-- F_2349_jpg.rf.74169f6af34a27f25fd4815735460b96.txt\n      |-- 1202203231111320_png_jpg.rf.7c696b285fd9a13e3871f904b6b15929.txt\n      |-- Image1322_jpg.rf.cb0903ee1953955cc0b1ca8d14a5e47c.txt\n      |-- Image1292_jpg.rf.f04d373d5e7f8c433635e1f48d150a6a.txt\n      |-- tile_14_5_png_jpg.rf.5c029591b94a8c9a55f1600e4e7a558d.txt\n      |-- Image1334_jpg.rf.9e5a78c9fb33f7e6e22cb759f0f3317c.txt\n      |-- F_30001556_jpg.rf.52f1e372234cb12f3e181ca43dc9d2bd.txt\n      |-- 023120203030120_png_jpg.rf.3d5fa5c48d2d0e95c6a2ecfb843691c2.txt\n      |-- Image1365_jpg.rf.a97b9266a9ca196faace2a686b09b58e.txt\n      |-- 032002230012033_png_jpg.rf.db2151565ef40cb8792850f848618436.txt\n      |-- ... (5948 more items)\n    |-- images\n      |-- Image1310_jpg.rf.ff9b077e9e79d7ca4ff76965e7363b3e.jpg\n      |-- Image1255_jpg.rf.1031c31e8051752ea17d528badeb91de.jpg\n      |-- F_2729_jpg.rf.af937ea54325dadff4bcad61860f2ed9.jpg\n      |-- F_30002334_jpg.rf.78f308a3f1260dc8452bd1db71328a19.jpg\n      |-- ec737d97-image_1065_jpg.rf.03427ba83ec4b74cf8473557753e3bcc.jpg\n      |-- 02131221112101_png_jpg.rf.ae11faa5b9b0c5b70c5415349491335f.jpg\n      |-- F_30001719_jpg.rf.84685b462991245ed575db8edbf555ae.jpg\n      |-- 5cc10679-image_2914_jpg.rf.75bbb4fcdc5c742e5c9bdd91d83fa8f7.jpg\n      |-- F_30002670_jpg.rf.6baf0f36177b8c3993a522ed43080e6b.jpg\n      |-- 92852_53900_png_jpg.rf.3701bda17d664b9832f95a14a7f809a3.jpg\n      |-- ... (5948 more items)\n  |-- README.dataset.txt\n  |-- README.roboflow.txt\n","output_type":"stream"}],"execution_count":4},{"id":"8f90d870","cell_type":"code","source":"# --- Visualization Function (Less Verbose) ---\nimport matplotlib.pyplot as plt # Ensure matplotlib is imported\nimport random # Ensure random is imported\n\ndef visualize_samples(data_dir, num_samples=10):\n    \"\"\"Visualizes original images, generated masks from YOLO files, and raw annotations for a random sample. Less verbose.\"\"\"\n    print(f\"\\n--- Starting Random Sample Visualization ({num_samples} samples) ---\")\n        train_img_dir = FarmlandDataset(train_img_dir, train_mask_dir, dataset_base_dir, transform=train_transform)\n        train_mask_dir = FarmlandDataset(val_img_dir, val_mask_dir, dataset_base_dir, transform=val_transform)\n\n    # Basic path checks\n    if not os.path.isdir(train_img_dir): print(f\"Error: Train image directory not found: {train_img_dir}\"); return\n    if not os.path.isdir(train_mask_dir): print(f\"Error: Train mask directory not found: {train_mask_dir}\"); return\n\n    img_files_all = [f for f in os.listdir(train_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    if not img_files_all: print(f\"No image files found in {train_img_dir}\"); return\n\n    num_available = len(img_files_all)\n    num_samples = min(num_samples, num_available)\n    if num_samples == 0: print(\"No samples requested or available.\"); return\n    img_files = random.sample(img_files_all, num_samples)\n\n    # Load class names (keep essential warnings)\n    yaml_path = os.path.join(data_dir, 'data.yaml')\n    class_names = ['Unknown']; farm_class_id = None\n    if os.path.exists(yaml_path):\n        try:\n            with open(yaml_path, 'r') as f: data_yaml = yaml.safe_load(f)\n            if 'names' in data_yaml:\n                class_names = data_yaml['names']\n                for i, name in enumerate(class_names):\n                    if \"farm\" in name.lower(): farm_class_id = i; break\n        except Exception as e: print(f\"Warning: Error reading class names from data.yaml: {e}\")\n    else: print(f\"Warning: data.yaml not found at {yaml_path}.\")\n\n    fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n    if num_samples == 1: fig.suptitle(\"Random Sample Image, Generated Mask, and Raw Annotations\", fontsize=16); axes = np.array([axes])\n    else: plt.suptitle(f\"Random Sample Images, Generated Masks, and Raw Annotations (n={num_samples})\", fontsize=16)\n\n    for i, img_file in enumerate(img_files):\n        ax_img, ax_mask, ax_raw = axes[i]\n        img_path = os.path.join(train_img_dir, img_file)\n        try: # Load image\n            img = Image.open(img_path).convert(\"RGB\"); img_np = np.array(img); img_height, img_width = img_np.shape[:2]\n        except Exception as e: # Handle image loading error\n            print(f\"Error loading image {img_file}: {e}\"); ax_img.set_title(f\"Error loading {img_file}\"); ax_img.axis('off'); ax_mask.set_title(\"Mask N/A\"); ax_mask.axis('off'); ax_raw.set_title(\"Raw Annotations N/A\"); ax_raw.axis('off'); continue\n\n        # Prepare for drawing\n        mask_file = os.path.splitext(img_file)[0] + '.txt'; mask_path = os.path.join(train_mask_dir, mask_file)\n        filled_mask = np.zeros((img_height, img_width), dtype=np.uint8); img_np_raw = img_np.copy()\n        ax_img.imshow(img_np); ax_img.set_title(f\"Image: {img_file}\"); ax_img.axis('off')\n        ax_raw.set_title(f\"Raw Annots (File: {mask_file})\"); ax_raw.axis('off')\n\n        if os.path.exists(mask_path):\n            try: # Read annotations\n                with open(mask_path, 'r') as f: lines = f.readlines()\n                for line in lines:\n                    try: # Process single annotation line\n                        parts = line.strip().split(' ');\n                        if len(parts) < 5: continue\n                        class_id = int(parts[0])\n                        if farm_class_id is not None and class_id != farm_class_id: continue # Filter class\n                        if len(parts) > 5 and len(parts) % 2 == 1: is_polygon=True # Polygon format: class x1 y1 x2 y2 ...\n                        elif len(parts) == 5: is_polygon=False # Bbox format: class xc yc w h\n                        else: continue # Skip ambiguous format\n                        coords = parts[1:]\n                        if is_polygon:\n                             polygon_points_pixels = [(int(float(coords[j])*img_width), int(float(coords[j+1])*img_height)) for j in range(0, len(coords), 2)]\n                             if len(polygon_points_pixels) >= 3:\n                                 pts = np.array(polygon_points_pixels, np.int32).reshape((-1, 1, 2)); cv2.fillPoly(filled_mask, [pts], 255) # Fill mask white\n                                 outline_color_bgr=(0, 255, 0); vertex_color_bgr=(0, 0, 255); cv2.polylines(img_np_raw, [pts], True, outline_color_bgr, 1) # Draw raw outline\n                                 for px, py in polygon_points_pixels: cv2.circle(img_np_raw, (px, py), 2, vertex_color_bgr, -1) # Draw raw vertices\n                        else: # Bbox\n                            x_center, y_center, width, height = map(float, coords)\n                            x1=max(0, int((x_center-width/2)*img_width)); y1=max(0, int((y_center-height/2)*img_height))\n                            x2=min(img_width-1, int((x_center+width/2)*img_width)); y2=min(img_height-1, int((y_center+height/2)*img_height))\n                            cv2.rectangle(filled_mask, (x1, y1), (x2, y2), 128, -1) # Fill mask gray for bbox\n                            bbox_color_bgr = (255, 0, 0); cv2.rectangle(img_np_raw, (x1, y1), (x2, y2), bbox_color_bgr, 1) # Draw raw bbox outline\n                    except (ValueError, IndexError, Exception): pass # Silently ignore line processing errors\n            except Exception: ax_mask.set_title(\"Error reading mask file\"); ax_raw.set_title(\"Error reading mask file\") # Handle file read error\n        else: ax_mask.set_title(\"Mask file missing\"); ax_raw.set_title(\"Annotation file missing\") # Handle missing mask file\n\n        # Display final results for the row\n        ax_mask.imshow(filled_mask, cmap='gray', vmin=0, vmax=255); ax_mask.set_title(\"Generated Mask (Poly=White, BBox=Gray)\"); ax_mask.axis('off')\n        ax_raw.imshow(img_np_raw); ax_raw.axis('off')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n    print(\"--- Finished Random Sample Visualization ---\")\n\n\n# --- Run the visualization (or skip) ---\n# Make sure 'dataset_base_dir' is defined and valid\nif 'dataset_base_dir' in locals() and os.path.isdir(dataset_base_dir):\n    try:\n        num_visual_samples = 0 # <<< SET TO 0 TO DISABLE VISUALIZATION, or e.g., 5 to show 5 random samples >>>\n        if num_visual_samples > 0:\n             visualize_samples(dataset_base_dir, num_samples=num_visual_samples)\n        else:\n             print(\"\\nVisualization skipped as num_visual_samples is 0.\")\n    except NameError as ne: print(f\"A NameError occurred during visualization: {ne}. Ensure variables/functions are defined.\")\n    except Exception as e: import traceback; traceback.print_exc(); print(f\"An unexpected error occurred during visualization: {e}\")\nelse:\n     print(\"\\nError: 'dataset_base_dir' is not defined or is not a valid directory. Cannot run visualization.\")\n     print(\"Please ensure the Roboflow download cell and path definition cell ran successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.471887Z","iopub.status.idle":"2025-04-14T19:54:03.472185Z","shell.execute_reply":"2025-04-14T19:54:03.472055Z"}},"outputs":[],"execution_count":null},{"id":"9aecd8c6","cell_type":"markdown","source":"### 2.4 Data Preparation\n\nWe need to convert YOLOv8 format annotations to segmentation masks for U-Net training.","metadata":{}},{"id":"f2e34e4a","cell_type":"code","source":"# === Section 2.3: Dataset Exploration and Definition ===\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport cv2\nimport yaml # Make sure yaml is imported\nimport torch.nn.functional as F # Ensure F is imported for interpolate\n\n# Define paths after loading the dataset\n# Ensure the 'dataset' variable is available from the download cell\nif 'dataset' in locals() and hasattr(dataset, 'location'):\n    dataset_base_dir = dataset.location # For finding data.yaml and overall root\n    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n    print(f\"Dataset paths set using location: {dataset.location}\")\nelse:\n    # Fallback or error if dataset location is not defined\n    print(\"ERROR: 'dataset' variable or 'dataset.location' not found.\")\n    print(\"Please run the Roboflow download cell first.\")\n    # Define dummy paths to avoid crashing subsequent cells, but processing will fail\n    dataset_base_dir = \".\"\n    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n\n\n# --- Custom Dataset Class (Less Verbose) ---\nclass FarmlandDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for loading satellite images and generating segmentation masks\n    from YOLOv8 polygon annotation files. Less verbose version.\n    \"\"\"\n    def __init__(self, img_dir, mask_dir, dataset_root_dir, transform=None, farm_class_name=\"farm\", img_size=256):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.img_size = img_size\n        self.farm_class_name = farm_class_name\n\n        # Filter for valid image files only\n        try:\n            self.img_files = sorted([\n                f for f in os.listdir(img_dir)\n                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n            ])\n            if not self.img_files:\n                 print(f\"Warning: No image files found in {self.img_dir}\") # Keep important warning\n        except FileNotFoundError:\n             print(f\"Error: Image directory not found: {self.img_dir}. Dataset will be empty.\") # Keep critical error\n             self.img_files = []\n\n        self.class_names = ['Unknown']\n        self.farm_class_id = None\n        self._load_class_info(dataset_root_dir)\n\n\n    def _load_class_info(self, dataset_root_dir):\n        \"\"\"Loads class names and identifies the farm class ID from data.yaml.\"\"\"\n        yaml_path = os.path.join(dataset_root_dir, 'data.yaml')\n        if os.path.exists(yaml_path):\n            try:\n                with open(yaml_path, 'r') as f:\n                    data_yaml = yaml.safe_load(f)\n                    if 'names' in data_yaml:\n                        self.class_names = data_yaml['names']\n                        for i, name in enumerate(self.class_names):\n                            if self.farm_class_name.lower() in name.lower():\n                                self.farm_class_id = i; break\n                        if self.farm_class_id is None:\n                            print(f\"Warning: Target class name '{self.farm_class_name}' not found in data.yaml names: {self.class_names}\") # Keep warning\n                    else:\n                        print(f\"Warning: 'names' key not found in {yaml_path}\") # Keep warning\n            except ImportError:\n                 print(\"Warning: PyYAML not installed. Cannot read class names from data.yaml.\") # Keep warning\n            except Exception as e:\n                print(f\"Warning: Error reading class names from data.yaml: {e}\") # Keep warning\n        else:\n             print(f\"Warning: data.yaml not found at {yaml_path}. Cannot determine farm class ID automatically.\") # Keep warning\n\n    def __len__(self):\n        return len(self.img_files)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.img_files):\n             raise IndexError(\"Index out of bounds\")\n\n        img_filename = self.img_files[idx]\n        img_path = os.path.join(self.img_dir, img_filename)\n\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n            original_width, original_height = image.size\n        except Exception:\n            # Return dummy tensors silently if image loading fails\n            dummy_image = torch.zeros((3, self.img_size, self.img_size))\n            dummy_mask = torch.zeros((1, self.img_size, self.img_size))\n            return dummy_image, dummy_mask\n\n        mask_file = os.path.splitext(img_filename)[0] + '.txt'\n        mask_path = os.path.join(self.mask_dir, mask_file)\n        mask = np.zeros((original_height, original_width), dtype=np.float32)\n\n        if os.path.exists(mask_path):\n            try:\n                with open(mask_path, 'r') as f: lines = f.readlines()\n                for line in lines:\n                    try:\n                        parts = line.strip().split(' ')\n                        if len(parts) < 5: continue\n                        class_id = int(parts[0])\n                        if self.farm_class_id is not None and class_id != self.farm_class_id: continue\n                        if len(parts) > 5 and len(parts) % 2 == 1: # Polygon: class x1 y1 x2 y2 ...\n                            polygon_points_normalized = []\n                            for j in range(1, len(parts), 2):\n                                 if j + 1 < len(parts):\n                                     x_norm = float(parts[j]); y_norm = float(parts[j+1])\n                                     polygon_points_normalized.append((x_norm, y_norm))\n                            polygon_points_pixels = [(int(x*original_width), int(y*original_height)) for x,y in polygon_points_normalized]\n                            if len(polygon_points_pixels) >= 3:\n                                pts = np.array(polygon_points_pixels, np.int32).reshape((-1, 1, 2))\n                                cv2.fillPoly(mask, [pts], 1.0)\n                        # Ignore bounding boxes (len(parts) == 5) for mask generation\n                    except (ValueError, IndexError): continue # Silently ignore errors in single line processing\n            except Exception: pass # Silently ignore file reading errors\n\n        # --- Transformations ---\n        if not self.transform:\n             resizer = transforms.Resize((self.img_size, self.img_size), interpolation=transforms.InterpolationMode.BILINEAR)\n             to_tensor = transforms.ToTensor()\n             image_resized = resizer(image)\n             image_tensor = to_tensor(image_resized)\n             mask_resized = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n             mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n        else:\n             image_tensor = self.transform(image)\n             target_h, target_w = image_tensor.shape[-2:]\n             mask_resized = cv2.resize(mask, (target_w, target_h), interpolation=cv2.INTER_NEAREST)\n             mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n\n        if image_tensor.shape[-2:] != mask_tensor.shape[-2:]:\n              try: # Attempt silent correction\n                  mask_tensor = F.interpolate(mask_tensor.unsqueeze(0), size=image_tensor.shape[-2:], mode='nearest').squeeze(0)\n              except Exception: # Return dummy on correction failure\n                   dummy_image = torch.zeros((3, self.img_size, self.img_size)); dummy_mask = torch.zeros((1, self.img_size, self.img_size))\n                   return dummy_image, dummy_mask\n\n        mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)\n        return image_tensor, mask_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:33.905180Z","iopub.execute_input":"2025-04-14T19:50:33.905470Z","iopub.status.idle":"2025-04-14T19:50:33.925671Z","shell.execute_reply.started":"2025-04-14T19:50:33.905450Z","shell.execute_reply":"2025-04-14T19:50:33.924887Z"}},"outputs":[{"name":"stdout","text":"Dataset paths set using location: /kaggle/working/Final-Detectron-2-1\n","output_type":"stream"}],"execution_count":6},{"id":"f0ced9d4","cell_type":"code","source":"# Set up augmented data transformations to prevent overfitting\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flips\n    transforms.RandomVerticalFlip(p=0.5),    # Random vertical flips\n    transforms.RandomRotation(10),           # Random rotations up to 10 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Keep validation transform simple\nval_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Create datasets with separate transforms\ntrain_dataset = FarmlandDataset(train_img_dir, train_mask_dir, dataset_base_dir, transform=train_transform)\nval_dataset = FarmlandDataset(val_img_dir, val_mask_dir, dataset_base_dir, transform=val_transform)\n\n# Optimize batch size based on available GPUs and model complexity\nif torch.cuda.device_count() > 1:\n    batch_size = 24  # Increased batch size for multiple GPUs\nelse:\n    batch_size = 8   # Default batch size for single GPU\n\n# Determine optimal number of workers for data loading\nnum_workers = min(os.cpu_count(), 16) if os.cpu_count() else 4\n\n# Configure DataLoader with more aggressive prefetching and optimized memory usage\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=num_workers,        # More workers for faster data loading\n    pin_memory=True,                # Use pinned memory for faster CPU->GPU transfer\n    prefetch_factor=4,              # Prefetch more batches\n    persistent_workers=True,        # Keep worker processes alive between iterations\n    drop_last=True                  # Drop last incomplete batch for better performance\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=num_workers,\n    pin_memory=True,\n    prefetch_factor=4,\n    persistent_workers=True\n)\n\n# Adjust CUDA settings for optimal performance\ntorch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner to find the best algorithm\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\ntorch.backends.cudnn.allow_tf32 = True        # Allow TF32 on Ampere GPUs\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Using batch size: {batch_size} with {torch.cuda.device_count()} GPUs\")\nprint(f\"DataLoader configured with {num_workers} worker processes\")\nprint(f\"Data augmentation enabled for training to prevent overfitting\")\nprint(f\"CUDA optimizations enabled: benchmark={torch.backends.cudnn.benchmark}, TF32={torch.backends.cuda.matmul.allow_tf32}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:33.926483Z","iopub.execute_input":"2025-04-14T19:50:33.926754Z","iopub.status.idle":"2025-04-14T19:50:33.953825Z","shell.execute_reply.started":"2025-04-14T19:50:33.926723Z","shell.execute_reply":"2025-04-14T19:50:33.953233Z"}},"outputs":[{"name":"stdout","text":"Train dataset size: 5958\nValidation dataset size: 718\nUsing batch size: 24 with 2 GPUs\nDataLoader configured with 4 worker processes\nData augmentation enabled for training to prevent overfitting\nCUDA optimizations enabled: benchmark=True, TF32=True\n","output_type":"stream"}],"execution_count":7},{"id":"39e03722","cell_type":"markdown","source":"## 3. Modeling\n\n### 3.1 U-Net Architecture","metadata":{}},{"id":"cda3fabb","cell_type":"code","source":"# Efficient Model implementation with optimized memory usage\nclass EfficientUNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1):\n        super(EfficientUNet, self).__init__()\n        \n        # Use GroupNorm instead of BatchNorm for better performance with large batch sizes\n        # and more efficiency on multiple GPUs\n        \n        # Encoder (downsampling)\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=8, num_channels=64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=8, num_channels=64),\n            nn.ReLU(inplace=True)\n        )\n        self.pool1 = nn.MaxPool2d(2)\n        \n        self.enc2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=16, num_channels=128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=16, num_channels=128),\n            nn.ReLU(inplace=True)\n        )\n        self.pool2 = nn.MaxPool2d(2)\n        \n        self.enc3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=256),\n            nn.ReLU(inplace=True)\n        )\n        self.pool3 = nn.MaxPool2d(2)\n        \n        self.enc4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=512),\n            nn.ReLU(inplace=True)\n        )\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=1024),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=1024),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Decoder with memory-efficient skip connections\n        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec4 = nn.Sequential(\n            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=512),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec3 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=256),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = nn.Sequential(\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=16, num_channels=128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=16, num_channels=128),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=8, num_channels=64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.GroupNorm(num_groups=8, num_channels=64),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Final output layer\n        self.outconv = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n        # Initialize weights properly for faster convergence\n        self._initialize_weights()\n        \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.GroupNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        p1 = self.pool1(e1)\n        \n        e2 = self.enc2(p1)\n        p2 = self.pool2(e2)\n        \n        e3 = self.enc3(p2)\n        p3 = self.pool3(e3)\n        \n        e4 = self.enc4(p3)\n        p4 = self.pool4(e4)\n        \n        # Bottleneck\n        b = self.bottleneck(p4)\n        \n        # Decoder with skip connections\n        u4 = self.upconv4(b)\n        u4 = torch.cat([u4, e4], dim=1)  # Skip connection\n        d4 = self.dec4(u4)\n        \n        u3 = self.upconv3(d4)\n        u3 = torch.cat([u3, e3], dim=1)  # Skip connection\n        d3 = self.dec3(u3)\n        \n        u2 = self.upconv2(d3)\n        u2 = torch.cat([u2, e2], dim=1)  # Skip connection\n        d2 = self.dec2(u2)\n        \n        u1 = self.upconv1(d2)\n        u1 = torch.cat([u1, e1], dim=1)  # Skip connection\n        d1 = self.dec1(u1)\n        \n        # Final output\n        out = self.outconv(d1)\n        return torch.sigmoid(out)  # Apply sigmoid for binary segmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:33.955759Z","iopub.execute_input":"2025-04-14T19:50:33.955975Z","iopub.status.idle":"2025-04-14T19:50:33.972094Z","shell.execute_reply.started":"2025-04-14T19:50:33.955958Z","shell.execute_reply":"2025-04-14T19:50:33.971354Z"}},"outputs":[],"execution_count":8},{"id":"74c16ebc","cell_type":"code","source":"# Initialize the efficient model and optimizer\nmodel = EfficientUNet(n_channels=3, n_classes=1)\n\n# Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\n# Define loss function and optimizer with weight decay for regularization\ncriterion = nn.BCEWithLogitsLoss()  # Combines sigmoid and BCE for better numerical stability\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW with weight decay to reduce overfitting\n\n# Print model summary\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:33.973819Z","iopub.execute_input":"2025-04-14T19:50:33.974072Z","iopub.status.idle":"2025-04-14T19:50:34.745850Z","shell.execute_reply.started":"2025-04-14T19:50:33.974052Z","shell.execute_reply":"2025-04-14T19:50:34.745121Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs for parallel training\nDataParallel(\n  (module): EfficientUNet(\n    (enc1): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (enc2): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(16, 128, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(16, 128, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (enc3): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (enc4): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (bottleneck): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(32, 1024, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (upconv4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n    (dec4): Sequential(\n      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(32, 512, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (dec3): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (dec2): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(16, 128, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(16, 128, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (dec1): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n      (5): ReLU(inplace=True)\n    )\n    (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n)\n","output_type":"stream"}],"execution_count":9},{"id":"291d17c6","cell_type":"markdown","source":"### 3.2 Training Functions","metadata":{}},{"id":"b03bad2c","cell_type":"code","source":"# Define training and validation functions\ndef train_epoch(model, dataloader, criterion, optimizer, device, amp_scaler=None):\n    model.train()\n    running_loss = 0.0\n    \n    # Use tqdm for progress tracking\n    for images, masks in tqdm(dataloader, desc=\"Training\"):\n        images = images.to(device, non_blocking=True)  # Use non_blocking for async transfer\n        masks = masks.to(device, non_blocking=True)\n        \n        # Clear gradients\n        optimizer.zero_grad()  # Removed flush=True which is not supported in this version\n        \n        # Mixed precision training - updated to use newer API\n        if amp_scaler is not None:\n            with torch.amp.autocast(device_type='cuda'):  # Updated from torch.cuda.amp.autocast()\n                # Forward pass\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n            \n            # Backward pass with gradient scaling\n            amp_scaler.scale(loss).backward()\n            amp_scaler.step(optimizer)\n            amp_scaler.update()\n        else:\n            # Standard training\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\ndef validate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Validation\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            running_loss += loss.item() * images.size(0)\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    return epoch_loss\n\n# Function to calculate IoU (Intersection over Union)\ndef calculate_iou(pred, target, threshold=0.5):\n    pred_binary = (pred > threshold).float()\n    intersection = (pred_binary * target).sum()\n    union = pred_binary.sum() + target.sum() - intersection\n    \n    iou = (intersection + 1e-8) / (union + 1e-8)  # Adding small epsilon to avoid division by zero\n    return iou.item()\n\n# Function to evaluate model on validation set\ndef evaluate_model(model, dataloader, device, threshold=0.5):\n    model.eval()\n    iou_scores = []\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Calculate IoU for each image in batch\n            for i in range(outputs.size(0)):\n                iou = calculate_iou(outputs[i], masks[i], threshold)\n                iou_scores.append(iou)\n    \n    mean_iou = sum(iou_scores) / len(iou_scores)\n    return mean_iou","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:34.746657Z","iopub.execute_input":"2025-04-14T19:50:34.746941Z","iopub.status.idle":"2025-04-14T19:50:34.757069Z","shell.execute_reply.started":"2025-04-14T19:50:34.746919Z","shell.execute_reply":"2025-04-14T19:50:34.756042Z"}},"outputs":[],"execution_count":10},{"id":"af8a111f","cell_type":"markdown","source":"### 3.3 Training Loop","metadata":{}},{"id":"d852eeea","cell_type":"code","source":"# === Section 4: Model Training (Simplified Output) ===\nimport torch.optim as optim # Ensure optimizer is imported\nimport torch.cuda.amp as amp # For GradScaler\n\n# Define training parameters\nnum_epochs = 30 # Or your desired number of epochs\nbest_val_loss = float('inf')\nbest_model_path = 'best_unet_model.pth'\npatience = 5 # Early stopping patience\nno_improve_epochs = 0\n\n# Assume model, train_loader, val_loader, criterion, optimizer are already defined\n# Assume train_epoch, validate_epoch, evaluate_model functions are defined elsewhere and are not overly verbose\n\n# Set up mixed precision training scaler\nscaler = None\nif torch.cuda.is_available():\n    scaler = amp.GradScaler()\n    print(\"Using mixed precision training.\")\nelse:\n    print(\"CUDA not available, using CPU (mixed precision disabled).\")\n    scaler = None # Explicitly None for clarity\n\n# Set up learning rate scheduler\n# Ensure optimizer is defined before this line\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3 # Check patience parameter here vs early stopping\n)\n\n# --- Training loop ---\nprint(f\"\\nStarting training for up to {num_epochs} epochs...\")\nprint(\"-\" * 70)\nprint(\"Epoch | Train Loss | Val Loss   | Val IoU    | LR       | Status\")\nprint(\"-\" * 70)\n\nfor epoch in range(num_epochs):\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n    # Get LR for this epoch *before* training and scheduler step\n    last_lr = scheduler.optimizer.param_groups[0]['lr'] # More robust way to get current LR\n\n    # --- Train ---\n    # Ensure train_epoch handles device and scaler correctly\n    model.train() # Set model to training mode\n    # Add dummy implementations if needed for placeholder\n    # def train_epoch(model, loader, criterion, optimizer, device, amp_scaler): return np.random.rand()\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, amp_scaler=scaler)\n\n    # --- Validate ---\n    # Ensure validate_epoch and evaluate_model handle device correctly\n    model.eval() # Set model to evaluation mode\n    # Add dummy implementations if needed for placeholder\n    # def validate_epoch(model, loader, criterion, device): return np.random.rand()\n    # def evaluate_model(model, loader, device): return np.random.rand()\n    with torch.no_grad(): # Ensure validation runs without gradients\n        val_loss = validate_epoch(model, val_loader, criterion, device)\n        val_iou = evaluate_model(model, val_loader, device) # Assumes IoU is calculated on validation set\n\n    # Update scheduler based on validation loss\n    scheduler.step(val_loss)\n\n    # --- Check for improvement and save best model ---\n    status = \"\"\n    if val_loss < best_val_loss:\n        improvement = best_val_loss - val_loss\n        best_val_loss = val_loss\n        try:\n            torch.save(model.state_dict(), best_model_path)\n            status = f\"Improved (+{improvement:.4f}), Saved\"\n        except Exception as save_e:\n            status = f\"Improved (+{improvement:.4f}), SAVE FAILED!\"\n            print(f\"Warning: Failed to save model: {save_e}\")\n        no_improve_epochs = 0\n    else:\n        no_improve_epochs += 1\n        status = f\"No improvement ({no_improve_epochs}/{patience})\"\n\n    # --- Print epoch summary ---\n    print(f\"{epoch+1:<5} | {train_loss:<10.4f} | {val_loss:<10.4f} | {val_iou:<10.4f} | {last_lr:<8.6f} | {status}\")\n\n    # --- Check for early stopping ---\n    if no_improve_epochs >= patience:\n        print(\"-\" * 70)\n        print(f\"Early stopping triggered after {epoch+1} epochs due to lack of improvement.\")\n        break\n\n# --- End of Training ---\nprint(\"-\" * 70)\nprint(\"Training completed!\")\nif os.path.exists(best_model_path) and best_val_loss != float('inf'):\n    print(f\"Best model saved to {best_model_path} with validation loss: {best_val_loss:.4f}\")\nelif best_val_loss == float('inf'):\n     print(\"No model was saved during training (validation loss never recorded).\")\nelse:\n    print(\"No model was saved during training (validation loss might not have improved or saving failed).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:50:34.757812Z","iopub.execute_input":"2025-04-14T19:50:34.758013Z","iopub.status.idle":"2025-04-14T19:54:03.458207Z","shell.execute_reply.started":"2025-04-14T19:50:34.757996Z","shell.execute_reply":"2025-04-14T19:54:03.456766Z"}},"outputs":[{"name":"stdout","text":"Using mixed precision training.\n\nStarting training for up to 30 epochs...\n----------------------------------------------------------------------\nEpoch | Train Loss | Val Loss   | Val IoU    | LR       | Status\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/248 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2353e3b8a9e408f8602f7e5c9db7c86"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f9fa392ae96e>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Add dummy implementations if needed for placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# def train_epoch(model, loader, criterion, optimizer, device, amp_scaler): return np.random.rand()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamp_scaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# --- Validate ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-003553b997f9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, amp_scaler)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Updated from torch.cuda.amp.autocast()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"id":"41eca62d","cell_type":"markdown","source":"### 3.4 Visualize Training Results","metadata":{}},{"id":"90bbe1ef","cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(12, 4))\n\n# Plot losses\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Plot IoU\nplt.subplot(1, 2, 2)\nplt.plot(val_ious, label='Validation IoU')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.title('Validation IoU')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.458753Z","iopub.status.idle":"2025-04-14T19:54:03.459067Z","shell.execute_reply":"2025-04-14T19:54:03.458933Z"}},"outputs":[],"execution_count":null},{"id":"448580ef","cell_type":"markdown","source":"## 4. Farm Size Calculation and Classification\n\nNow we'll use our trained model to segment farms and calculate their sizes.","metadata":{}},{"id":"91f254cb","cell_type":"code","source":"# Load the best model\nmodel.load_state_dict(torch.load(best_model_path))\nmodel.eval()\n\n# Function to segment farms in an image\ndef segment_farms(model, image_path, device, transform):\n    # Load image\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size  # (width, height)\n    \n    # Preprocess image\n    input_tensor = transform(image).unsqueeze(0).to(device)\n    \n    # Get model prediction\n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_mask = output.squeeze().cpu().numpy()\n    \n    # Threshold to get binary mask\n    binary_mask = (predicted_mask > 0.5).astype(np.uint8) * 255\n    \n    # Resize mask back to original image size\n    binary_mask_resized = cv2.resize(binary_mask, (original_size[0], original_size[1]))\n    \n    return binary_mask_resized, np.array(image)\n\n# Function to calculate farm sizes and classify them\ndef calculate_farm_sizes(binary_mask, pixels_per_meter=None):\n    # Use connected component analysis to identify individual farms\n    labeled_mask, num_farms = measure.label(binary_mask, connectivity=2, return_num=True)\n    \n    # Calculate properties of each labeled region\n    regions = measure.regionprops(labeled_mask)\n    \n    # Store farm areas\n    farm_areas = []\n    \n    for region in regions:\n        # Skip very small regions (likely noise)\n        if region.area < 100:  # Adjust threshold as needed\n            continue\n            \n        # Calculate area in pixels\n        area_pixels = region.area\n        \n        # Convert to real-world units if pixels_per_meter is provided\n        if pixels_per_meter is not None:\n            area_sq_meters = area_pixels / (pixels_per_meter ** 2)\n            # Convert to hectares (1 hectare = 10,000 sq meters)\n            area_hectares = area_sq_meters / 10000\n            farm_areas.append(area_hectares)\n        else:\n            farm_areas.append(area_pixels)\n    \n    return farm_areas, labeled_mask\n\n# Function to classify farms by size\ndef classify_farms(farm_areas, unit='pixels'):\n    # Define size thresholds (adjust based on your specific context)\n    if unit == 'hectares':\n        # Real-world thresholds (in hectares)\n        small_threshold = 10     # 0-10 hectares = small farm\n        medium_threshold = 50    # 10-50 hectares = medium farm\n        # > 50 hectares = large farm\n    else:\n        # Pixel-based thresholds (adjust based on your image resolution)\n        small_threshold = 5000     # 0-5000 pixels = small farm\n        medium_threshold = 20000   # 5000-20000 pixels = medium farm\n        # > 20000 pixels = large farm\n    \n    # Classify each farm\n    farm_classes = []\n    for area in farm_areas:\n        if area < small_threshold:\n            farm_classes.append('Small')\n        elif area < medium_threshold:\n            farm_classes.append('Medium')\n        else:\n            farm_classes.append('Large')\n    \n    # Count farms in each category\n    class_counts = {\n        'Small': farm_classes.count('Small'),\n        'Medium': farm_classes.count('Medium'),\n        'Large': farm_classes.count('Large')\n    }\n    \n    return farm_classes, class_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.459666Z","iopub.status.idle":"2025-04-14T19:54:03.459989Z","shell.execute_reply":"2025-04-14T19:54:03.459850Z"}},"outputs":[],"execution_count":null},{"id":"b9c6bd69","cell_type":"code","source":"# Function to visualize farm segmentation and classification\ndef visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes):\n    # Create a colormap for visualization\n    cmap = plt.cm.colors.ListedColormap(['black', 'green', 'yellow', 'red'])\n    bounds = [0, 1, 2, 3, 4]\n    norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n    \n    # Create a colored mask based on farm classification\n    colored_mask = np.zeros_like(labeled_mask)\n    \n    for i, (region, area, farm_class) in enumerate(zip(measure.regionprops(labeled_mask), farm_areas, farm_classes)):\n        # Skip very small regions\n        if region.area < 100:\n            continue\n            \n        # Assign color based on class\n        if farm_class == 'Small':\n            color_value = 1\n        elif farm_class == 'Medium':\n            color_value = 2\n        else:  # Large\n            color_value = 3\n        \n        # Fill region with corresponding color\n        colored_mask[labeled_mask == region.label] = color_value\n    \n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    \n    # Original image with segmentation overlay\n    ax1.imshow(image)\n    ax1.imshow(colored_mask, cmap=cmap, alpha=0.5, norm=norm)\n    ax1.set_title('Farm Segmentation and Classification')\n    ax1.axis('off')\n    \n    # Create custom legend\n    legend_elements = [\n        plt.Rectangle((0, 0), 1, 1, color='green', alpha=0.5, label='Small Farms'),\n        plt.Rectangle((0, 0), 1, 1, color='yellow', alpha=0.5, label='Medium Farms'),\n        plt.Rectangle((0, 0), 1, 1, color='red', alpha=0.5, label='Large Farms')\n    ]\n    ax1.legend(handles=legend_elements, loc='upper right')\n    \n    # Pie chart of farm size distribution\n    class_counts = {\n        'Small': farm_classes.count('Small'),\n        'Medium': farm_classes.count('Medium'),\n        'Large': farm_classes.count('Large')\n    }\n    \n    if sum(class_counts.values()) > 0:  # Check if we have any farms\n        labels = list(class_counts.keys())\n        sizes = list(class_counts.values())\n        colors = ['green', 'yellow', 'red']\n        \n        ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n        ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n        ax2.set_title('Farm Size Distribution')\n    else:\n        ax2.text(0.5, 0.5, 'No farms detected', horizontalalignment='center', verticalalignment='center')\n        ax2.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.462827Z","iopub.status.idle":"2025-04-14T19:54:03.463134Z","shell.execute_reply":"2025-04-14T19:54:03.463004Z"}},"outputs":[],"execution_count":null},{"id":"e73fda47","cell_type":"code","source":"# Process a sample image\ndef process_sample_image(image_path):\n    # Segment farms\n    binary_mask, image = segment_farms(model, image_path, device, transform)\n    \n    # Calculate farm sizes\n    # Note: In a real application, you would need to determine pixels_per_meter based on image metadata\n    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask)\n    \n    # Classify farms\n    farm_classes, class_counts = classify_farms(farm_areas)\n    \n    # Print results\n    print(f\"Number of farms detected: {len(farm_areas)}\")\n    print(f\"Farm size classification: {class_counts}\")\n    \n    # Visualize results\n    visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n    \n    return farm_areas, farm_classes, class_counts\n\n# Try with a test image from the validation set\ntest_img_dir = os.path.join(dataset.location, 'valid', 'images')\ntest_img_files = os.listdir(test_img_dir)\n\nif test_img_files:\n    test_img_path = os.path.join(test_img_dir, test_img_files[0])\n    print(f\"Processing test image: {test_img_path}\")\n    farm_areas, farm_classes, class_counts = process_sample_image(test_img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.463610Z","iopub.status.idle":"2025-04-14T19:54:03.463927Z","shell.execute_reply":"2025-04-14T19:54:03.463798Z"}},"outputs":[],"execution_count":null},{"id":"07fbf17d","cell_type":"code","source":"# Define recommendations based on farm size\ndef get_recommendations_by_size(farm_size):\n    recommendations = {\n        'Small': {\n            'Crop Selection': [\n                'Focus on high-value crops (e.g., specialty vegetables, herbs, berries)',\n                'Consider intercropping to maximize land use',\n                'Explore vertical farming techniques for space optimization'\n            ],\n            'Equipment': [\n                'Invest in versatile, small-scale equipment',\n                'Consider equipment sharing programs or cooperatives',\n                'Focus on precision hand tools for specialized tasks'\n            ],\n            'Marketing': [\n                'Direct-to-consumer sales (farmers markets, CSA)',\n                'Develop value-added products',\n                'Leverage organic or specialty certifications'\n            ],\n            'Sustainability': [\n                'Implement intensive organic practices',\n                'Consider agroecological approaches',\n                'Explore permaculture design principles'\n            ]\n        },\n        'Medium': {\n            'Crop Selection': [\n                'Balance between specialty and commodity crops',\n                'Consider crop rotation systems',\n                'Explore diversification strategies'\n            ],\n            'Equipment': [\n                'Invest in mid-sized tractors and implements',\n                'Consider precision agriculture technology',\n                'Develop efficient irrigation systems'\n            ],\n            'Marketing': [\n                'Develop relationships with local wholesalers and restaurants',\n                'Consider cooperative marketing',\n                'Explore agritourism opportunities'\n            ],\n            'Sustainability': [\n                'Implement integrated pest management',\n                'Consider conservation tillage practices',\n                'Develop soil health management plans'\n            ]\n        },\n        'Large': {\n            'Crop Selection': [\n                'Focus on efficient production of commodity crops',\n                'Consider dedicating portions to specialty high-value crops',\n                'Implement strategic crop rotation systems'\n            ],\n            'Equipment': [\n                'Invest in large-scale, efficient machinery',\n                'Implement precision agriculture and automation',\n                'Consider GPS guidance systems and variable rate technology'\n            ],\n            'Marketing': [\n                'Develop contracts with processors and distributors',\n                'Consider futures markets and hedging strategies',\n                'Explore export opportunities'\n            ],\n            'Sustainability': [\n                'Implement conservation agriculture practices at scale',\n                'Consider renewable energy investments',\n                'Develop comprehensive nutrient management plans'\n            ]\n        }\n    }\n    \n    return recommendations.get(farm_size, {})\n\n# Function to display recommendations for a specific farm\ndef display_farm_recommendations(farm_class):\n    recommendations = get_recommendations_by_size(farm_class)\n    \n    if not recommendations:\n        print(f\"No recommendations available for {farm_class} farms.\")\n        return\n    \n    print(f\"\\n=== Recommendations for {farm_class} Farms ===\\n\")\n    \n    for category, items in recommendations.items():\n        print(f\"\\n{category}:\")\n        for item in items:\n            print(f\"  • {item}\")\n    \n    print(\"\\n\" + \"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.464545Z","iopub.status.idle":"2025-04-14T19:54:03.464865Z","shell.execute_reply":"2025-04-14T19:54:03.464733Z"}},"outputs":[],"execution_count":null},{"id":"e5fd3d6c","cell_type":"code","source":"# Display recommendations for each farm size category\nfor size in ['Small', 'Medium', 'Large']:\n    display_farm_recommendations(size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.465345Z","iopub.status.idle":"2025-04-14T19:54:03.465667Z","shell.execute_reply":"2025-04-14T19:54:03.465522Z"}},"outputs":[],"execution_count":null},{"id":"705153ea","cell_type":"markdown","source":"## 6. End-to-End Pipeline","metadata":{}},{"id":"d62fa25b","cell_type":"code","source":"# End-to-end pipeline function\ndef process_farm_image(image_path, pixel_scale=None):\n    \"\"\"\n    Process a satellite image to detect farms, classify them by size, and provide recommendations.\n    \n    Args:\n        image_path (str): Path to the satellite image\n        pixel_scale (float, optional): Scale factor in meters per pixel (if available)\n    \n    Returns:\n        dict: A dictionary containing the results of the analysis\n    \"\"\"\n    print(f\"Processing image: {image_path}\")\n    \n    # Step 1: Segment farms using the trained U-Net model\n    binary_mask, image = segment_farms(model, image_path, device, transform)\n    \n    # Step 2: Calculate farm sizes\n    unit = 'hectares' if pixel_scale else 'pixels'\n    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask, pixel_scale)\n    \n    # Step 3: Classify farms by size\n    farm_classes, class_counts = classify_farms(farm_areas, unit)\n    \n    # Step 4: Print summary\n    print(f\"\\nFarm Analysis Summary:\")\n    print(f\"Total farms detected: {len(farm_areas)}\")\n    print(f\"Farm size distribution: {class_counts}\")\n    print(f\"Area unit: {unit}\")\n    \n    # Step 5: Calculate predominant farm size\n    if len(farm_areas) > 0:\n        predominant_size = max(class_counts, key=class_counts.get)\n        print(f\"Predominant farm size: {predominant_size}\")\n        \n        # Step 6: Provide recommendations based on predominant farm size\n        display_farm_recommendations(predominant_size)\n    else:\n        print(\"No farms detected in the image.\")\n    \n    # Step 7: Visualize results\n    if len(farm_areas) > 0:\n        visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n    \n    # Return results\n    return {\n        'num_farms': len(farm_areas),\n        'farm_areas': farm_areas,\n        'farm_classes': farm_classes,\n        'class_counts': class_counts,\n        'predominant_size': predominant_size if len(farm_areas) > 0 else None\n    }\n\n# Test the end-to-end pipeline on a sample image (if available)\nif test_img_files:\n    test_img_path = os.path.join(test_img_dir, test_img_files[0])\n    results = process_farm_image(test_img_path)\n    \n    # You could save these results to a file or database for future reference\n    import json\n    with open('farm_analysis_results.json', 'w') as f:\n        # Convert non-serializable objects (like numpy arrays) to lists\n        serializable_results = {\n            'num_farms': results['num_farms'],\n            'farm_areas': [float(area) for area in results['farm_areas']],\n            'farm_classes': results['farm_classes'],\n            'class_counts': results['class_counts'],\n            'predominant_size': results['predominant_size']\n        }\n        json.dump(serializable_results, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.466275Z","iopub.status.idle":"2025-04-14T19:54:03.466577Z","shell.execute_reply":"2025-04-14T19:54:03.466450Z"}},"outputs":[],"execution_count":null},{"id":"b346b6a0","cell_type":"markdown","source":"## 7. Conclusion and Next Steps\n\n### 7.1 Summary\n\nIn this project, we have:\n1. Loaded and processed a dataset of satellite imagery with farm annotations\n2. Implemented and trained a U-Net model for farmland segmentation\n3. Developed methods to calculate farm sizes from segmentation masks\n4. Created a classification system to categorize farms by size\n5. Built a recommendation system providing tailored advice based on farm size\n6. Integrated all components into an end-to-end pipeline\n\n### 7.2 Limitations\n\nCurrent limitations of the system include:\n- Relies on image resolution and quality for accurate segmentation\n- Size classification thresholds may need adjustment for different regions\n- Lacks real-world unit calibration (meters/hectares) without proper image metadata\n- Recommendations are general and not region-specific\n\n### 7.3 Future Improvements\n\nPotential next steps for improving the system:\n1. **Model Enhancements**:\n   - Experiment with other architectures (DeepLabv3+, HRNet)\n   - Implement data augmentation for better generalization\n   - Train on a larger, more diverse dataset\n\n2. **Size Calculation**:\n   - Integrate with GIS systems to obtain accurate geospatial coordinates\n   - Develop methods to automatically determine image scale\n   - Account for terrain variations in area calculations\n\n3. **Recommendation System**:\n   - Incorporate climate and soil data for more targeted recommendations\n   - Develop region-specific recommendation models\n   - Create a more interactive recommendation interface\n\n4. **User Interface**:\n   - Develop a web or mobile application for easier access\n   - Allow users to upload their own imagery\n   - Provide visualization tools for farmers to explore results\n\n5. **Validation**:\n   - Conduct field validation with actual farm measurements\n   - Collect feedback from farmers on recommendation usefulness","metadata":{}},{"id":"0ca07339","cell_type":"markdown","source":"## 8. Transfer Learning with Pretrained Models\n\nU-Net can benefit greatly from transfer learning by using pretrained encoders. We'll implement this to improve both training speed and model accuracy.\n\n### 8.1 Pretrained U-Net Implementation","metadata":{}},{"id":"1195f281","cell_type":"code","source":"# First, let's install the necessary packages\n!pip install segmentation-models-pytorch\n\n# Import libraries for transfer learning\nimport segmentation_models_pytorch as smp\n\n# Define a U-Net model with a pretrained encoder\nclass PretrainedUNet(nn.Module):\n    def __init__(self, encoder_name=\"resnet34\", encoder_weights=\"imagenet\"):\n        super(PretrainedUNet, self).__init__()\n        self.model = smp.Unet(\n            encoder_name=encoder_name,        # Choose encoder, e.g. resnet34, efficientnet-b0, etc.\n            encoder_weights=encoder_weights,  # Use pretrained weights (e.g. imagenet) or None\n            in_channels=3,                    # Input channels (RGB images)\n            classes=1,                        # Output channels (binary segmentation)\n            activation=\"sigmoid\"              # Final activation function\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Create a pretrained model\nif torch.cuda.is_available():\n    # Show available encoders to choose from\n    print(\"Available pretrained encoders:\")\n    for i, encoder in enumerate(smp.encoders.get_encoder_names()):\n        if i % 5 == 0 and i > 0:\n            print()  # Line break for readability\n        print(f\"{encoder}\", end=\", \")\n    print(\"\\n\")\n\n# Initialize the pretrained model\npretrained_model = PretrainedUNet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\")\n\n# Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training of pretrained model\")\n    pretrained_model = nn.DataParallel(pretrained_model)\n\npretrained_model = pretrained_model.to(device)\n\n# Define optimizer with different learning rates for encoder and decoder\n# This is a common technique for fine-tuning - lower learning rate for pretrained parts\nencoder_params = []\ndecoder_params = []\n\nif torch.cuda.device_count() > 1:\n    # Handle DataParallel case\n    for name, param in pretrained_model.module.model.named_parameters():\n        if name.startswith(\"encoder\"):\n            encoder_params.append(param)\n        else:\n            decoder_params.append(param)\nelse:\n    for name, param in pretrained_model.model.named_parameters():\n        if name.startswith(\"encoder\"):\n            encoder_params.append(param)\n        else:\n            decoder_params.append(param)\n\noptimizer_pretrained = optim.Adam([\n    {'params': encoder_params, 'lr': 0.0001},  # Lower learning rate for pretrained encoder\n    {'params': decoder_params, 'lr': 0.001}    # Higher learning rate for decoder\n])\n\ncriterion_pretrained = nn.BCELoss()\n\nprint(\"Pretrained U-Net model initialized with ResNet34 encoder\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.467371Z","iopub.status.idle":"2025-04-14T19:54:03.467692Z","shell.execute_reply":"2025-04-14T19:54:03.467544Z"}},"outputs":[],"execution_count":null},{"id":"2727e9f5","cell_type":"code","source":"# Training loop for the pretrained model\nnum_epochs_pretrained = 15  # Usually needs fewer epochs due to pretrained weights\nbest_val_loss_pretrained = float('inf')\nbest_pretrained_model_path = 'best_pretrained_unet_model.pth'\n\n# Lists to store training history\ntrain_losses_pretrained = []\nval_losses_pretrained = []\nval_ious_pretrained = []\n\n# Mixed precision training setup for faster training\nif torch.cuda.is_available():\n    # Initialize the scaler for mixed precision training\n    from torch.cuda.amp import GradScaler, autocast\n    scaler = GradScaler()\n    print(\"Using mixed precision training for faster performance\")\n\nprint(f\"Beginning transfer learning with pretrained ResNet34 encoder...\")\nfor epoch in range(num_epochs_pretrained):\n    print(f\"Epoch {epoch+1}/{num_epochs_pretrained}\")\n    \n    # Train\n    pretrained_model.train()\n    running_loss = 0.0\n    \n    for images, masks in tqdm(train_loader, desc=\"Training\"):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        # Clear gradients\n        optimizer_pretrained.zero_grad()\n        \n        # Mixed precision training\n        if torch.cuda.is_available():\n            with autocast():\n                # Forward pass\n                outputs = pretrained_model(images)\n                loss = criterion_pretrained(outputs, masks)\n            \n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer_pretrained)\n            scaler.update()\n        else:\n            # Standard training on CPU\n            outputs = pretrained_model(images)\n            loss = criterion_pretrained(outputs, masks)\n            loss.backward()\n            optimizer_pretrained.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    train_loss = running_loss / len(train_loader.dataset)\n    train_losses_pretrained.append(train_loss)\n    \n    # Validate\n    pretrained_model.eval()\n    running_loss = 0.0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            outputs = pretrained_model(images)\n            loss = criterion_pretrained(outputs, masks)\n            \n            running_loss += loss.item() * images.size(0)\n    \n    val_loss = running_loss / len(val_loader.dataset)\n    val_losses_pretrained.append(val_loss)\n    \n    # Calculate IoU\n    val_iou = evaluate_model(pretrained_model, val_loader, device)\n    val_ious_pretrained.append(val_iou)\n    \n    # Print epoch results\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    print(f\"Validation IoU: {val_iou:.4f}\")\n    \n    # Save best model\n    if val_loss < best_val_loss_pretrained:\n        best_val_loss_pretrained = val_loss\n        torch.save(pretrained_model.state_dict(), best_pretrained_model_path)\n        print(f\"Model saved to {best_pretrained_model_path}\")\n    \n    print(\"-\" * 50)\n\nprint(\"Transfer learning completed!\")\n\n# Compare the performance of the basic UNet and pretrained UNet\nplt.figure(figsize=(15, 5))\n\n# Plot losses\nplt.subplot(1, 3, 1)\nplt.plot(train_losses, label='Basic UNet - Train')\nplt.plot(val_losses, label='Basic UNet - Val')\nplt.plot(train_losses_pretrained, label='Pretrained UNet - Train')\nplt.plot(val_losses_pretrained, label='Pretrained UNet - Val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Comparison')\nplt.legend()\n\n# Plot IoU\nplt.subplot(1, 3, 2)\nplt.plot(val_ious, label='Basic UNet')\nplt.plot(val_ious_pretrained, label='Pretrained UNet')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.title('Validation IoU Comparison')\nplt.legend()\n\n# Plot convergence speed\nplt.subplot(1, 3, 3)\nplt.plot([min(val_ious[:i+1]) for i in range(len(val_ious))], label='Basic UNet')\nplt.plot([min(val_ious_pretrained[:i+1]) for i in range(len(val_ious_pretrained))], label='Pretrained UNet')\nplt.xlabel('Epoch')\nplt.ylabel('Best IoU So Far')\nplt.title('Convergence Speed Comparison')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Use the best model for inference\nif os.path.exists(best_pretrained_model_path):\n    pretrained_model.load_state_dict(torch.load(best_pretrained_model_path))\n    print(f\"Loaded best pretrained model from {best_pretrained_model_path}\")\nelse:\n    print(\"Using current pretrained model state\")\n\npretrained_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.468263Z","iopub.status.idle":"2025-04-14T19:54:03.468582Z","shell.execute_reply":"2025-04-14T19:54:03.468451Z"}},"outputs":[],"execution_count":null},{"id":"c530e36a","cell_type":"markdown","source":"## 9. Model Export and Optimization\n\nIn this section, we'll export our models to formats suitable for deployment and optimize them for inference.\n\n### 9.1 TorchScript Export","metadata":{}},{"id":"db2badbb","cell_type":"code","source":"# Export the model to TorchScript format\ndef export_to_torchscript(model, example_input_tensor, model_name=\"farm_segmentation_model.pt\"):\n    \"\"\"\n    Export a PyTorch model to TorchScript format for production deployment.\n    \n    Args:\n        model: The PyTorch model to export\n        example_input_tensor: An example input tensor with the correct shape\n        model_name: The filename to save the model to\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    \n    # For DataParallel models, we need to access the module\n    if isinstance(model, nn.DataParallel):\n        model_to_trace = model.module\n    else:\n        model_to_trace = model\n    \n    # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing\n    try:\n        # Create a trace of the model with a sample input\n        traced_model = torch.jit.trace(model_to_trace, example_input_tensor)\n        \n        # Save the traced model\n        traced_model.save(model_name)\n        print(f\"TorchScript model saved to {model_name}\")\n        return True\n    except Exception as e:\n        print(f\"Error exporting model to TorchScript: {e}\")\n        return False\n\n# Export the basic U-Net model\nprint(\"Exporting basic U-Net model to TorchScript...\")\n# Create a sample input tensor with the correct shape\nsample_input = torch.randn(1, 3, 256, 256, device=device)\nexport_to_torchscript(model, sample_input, \"unet_farm_segmentation.pt\")\n\n# Export the pretrained U-Net model if it exists\nif 'pretrained_model' in locals():\n    print(\"Exporting pretrained U-Net model to TorchScript...\")\n    export_to_torchscript(pretrained_model, sample_input, \"pretrained_unet_farm_segmentation.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.469075Z","iopub.status.idle":"2025-04-14T19:54:03.469409Z","shell.execute_reply":"2025-04-14T19:54:03.469244Z"}},"outputs":[],"execution_count":null},{"id":"0d29bd0d","cell_type":"markdown","source":"### 9.2 ONNX Export for Deployment","metadata":{}},{"id":"593e5929","cell_type":"code","source":"# Export model to ONNX format for wider compatibility\ndef export_to_onnx(model, example_input_tensor, model_name=\"farm_segmentation_model.onnx\"):\n    \"\"\"\n    Export a PyTorch model to ONNX format for cross-platform deployment.\n    \n    Args:\n        model: The PyTorch model to export\n        example_input_tensor: An example input tensor with the correct shape\n        model_name: The filename to save the model to\n    \"\"\"\n    # For DataParallel models, we need to access the module\n    if isinstance(model, nn.DataParallel):\n        model_to_export = model.module\n    else:\n        model_to_export = model\n    \n    model_to_export.eval()\n    \n    try:\n        # Export the model to ONNX format\n        torch.onnx.export(\n            model_to_export,               # model being run\n            example_input_tensor,          # model input (or a tuple for multiple inputs)\n            model_name,                    # where to save the model\n            export_params=True,            # store the trained parameter weights inside the model file\n            opset_version=12,              # the ONNX version to export the model to\n            do_constant_folding=True,      # whether to execute constant folding for optimization\n            input_names=['input'],         # the model's input names\n            output_names=['output'],       # the model's output names\n            dynamic_axes={\n                'input': {0: 'batch_size'},    # variable length axes\n                'output': {0: 'batch_size'}\n            }\n        )\n        print(f\"ONNX model saved to {model_name}\")\n        \n        # Verify the ONNX model\n        import onnx\n        onnx_model = onnx.load(model_name)\n        onnx.checker.check_model(onnx_model)\n        print(\"ONNX model checked - model is valid\")\n        return True\n    except Exception as e:\n        print(f\"Error exporting model to ONNX: {e}\")\n        return False\n\n# Export the basic U-Net model to ONNX\nprint(\"Exporting basic U-Net model to ONNX...\")\nsample_input = torch.randn(1, 3, 256, 256, device=device)\nexport_to_onnx(model, sample_input, \"unet_farm_segmentation.onnx\")\n\n# Export the pretrained U-Net model if it exists\nif 'pretrained_model' in locals():\n    print(\"Exporting pretrained U-Net model to ONNX...\")\n    export_to_onnx(pretrained_model, sample_input, \"pretrained_unet_farm_segmentation.onnx\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.469945Z","iopub.status.idle":"2025-04-14T19:54:03.470421Z","shell.execute_reply":"2025-04-14T19:54:03.470268Z"}},"outputs":[],"execution_count":null},{"id":"fcab01cd","cell_type":"code","source":"# Function to test and visualize model predictions\ndef visualize_model_predictions(model, test_img_path, device, transform):\n    \"\"\"\n    Test the model on a single image and visualize the results with original image, \n    ground truth mask (if available), and predicted segmentation mask.\n    \n    Args:\n        model: Trained segmentation model\n        test_img_path: Path to the test image\n        device: Device to run inference on (cuda/cpu)\n        transform: Image transformations for model input\n    \"\"\"\n    model.eval()\n    \n    # Load image\n    image = Image.open(test_img_path).convert(\"RGB\")\n    filename = os.path.basename(test_img_path)\n    \n    # Get original size\n    original_size = image.size  # (width, height)\n    \n    # Create figure\n    plt.figure(figsize=(16, 8))\n    \n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    # Try to find corresponding mask (if it exists in validation set)\n    mask_exists = False\n    mask_file = os.path.splitext(filename)[0] + '.txt'\n    mask_path = os.path.join(os.path.dirname(test_img_path).replace('images', 'labels'), mask_file)\n    \n    if os.path.exists(mask_path):\n        mask_exists = True\n        # Create empty mask with the same size as the image\n        ground_truth_mask = np.zeros(original_size[::-1], dtype=np.uint8)  # height, width\n        \n        # Read YOLOv8 format annotations\n        with open(mask_path, 'r') as f:\n            lines = f.readlines()\n        \n        img_width, img_height = image.size\n        for line in lines:\n            parts = line.strip().split(' ')\n            if len(parts) >= 5:\n                # Check if we have polygon points (instance segmentation)\n                if len(parts) > 5:\n                    # Extract polygon points\n                    polygon_points = []\n                    for i in range(5, len(parts), 2):\n                        if i+1 < len(parts):\n                            x = float(parts[i]) * img_width\n                            y = float(parts[i+1]) * img_height\n                            polygon_points.append((int(x), int(y)))\n                    \n                    if polygon_points:\n                        # Convert to numpy array for OpenCV\n                        pts = np.array(polygon_points, np.int32)\n                        pts = pts.reshape((-1, 1, 2))\n                        # Fill polygon with ones\n                        cv2.fillPoly(ground_truth_mask, [pts], 255)\n                else:\n                    # Use bounding box\n                    class_id = int(parts[0])\n                    x_center = float(parts[1]) * img_width\n                    y_center = float(parts[2]) * img_height\n                    width = float(parts[3]) * img_width\n                    height = float(parts[4]) * img_height\n                    \n                    x1 = max(0, int(x_center - width / 2))\n                    y1 = max(0, int(y_center - height / 2))\n                    x2 = min(img_width - 1, int(x_center + width / 2))\n                    y2 = min(img_height - 1, int(y_center + height / 2))\n                    \n                    cv2.rectangle(ground_truth_mask, (x1, y1), (x2, y2), 255, -1)\n        \n        # Plot ground truth mask if found\n        plt.subplot(1, 3, 2)\n        plt.imshow(ground_truth_mask, cmap='gray')\n        plt.title('Ground Truth Mask')\n        plt.axis('off')\n    \n    # Process image for model prediction\n    input_tensor = transform(image).unsqueeze(0).to(device)\n    \n    # Get prediction\n    with torch.no_grad():\n        output = model(input_tensor)\n        pred_mask = output.squeeze().cpu().numpy()\n    \n    # Convert prediction to binary mask\n    pred_binary = (pred_mask > 0.5).astype(np.uint8) * 255\n    \n    # Resize prediction back to original image size\n    pred_resized = cv2.resize(pred_binary, (original_size[0], original_size[1]), interpolation=cv2.INTER_NEAREST)\n    \n    # Plot prediction\n    subplot_pos = 3 if mask_exists else 2\n    plt.subplot(1, 3, subplot_pos)\n    plt.imshow(pred_resized, cmap='gray')\n    plt.title('Model Prediction')\n    plt.axis('off')\n    \n    # If ground truth exists, calculate IoU\n    if mask_exists:\n        # Calculate IoU\n        intersection = np.logical_and(ground_truth_mask > 0, pred_resized > 0).sum()\n        union = np.logical_or(ground_truth_mask > 0, pred_resized > 0).sum()\n        iou = intersection / union if union > 0 else 0\n        plt.suptitle(f'Farmland Segmentation - IoU: {iou:.4f}', fontsize=16)\n    else:\n        plt.suptitle('Farmland Segmentation Prediction', fontsize=16)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Create and show overlay image for better visualization\n    plt.figure(figsize=(12, 10))\n    \n    # Create an RGB version of the prediction for overlay\n    pred_color = np.zeros((pred_resized.shape[0], pred_resized.shape[1], 4), dtype=np.uint8)\n    pred_color[pred_resized > 0] = [0, 255, 0, 128]  # Semi-transparent green for predictions\n    \n    # Convert PIL image to numpy array\n    img_array = np.array(image)\n    \n    # Plot image with prediction overlay\n    plt.imshow(img_array)\n    plt.imshow(pred_color, alpha=0.5)\n    plt.title('Prediction Overlay on Original Image')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    return pred_resized, iou if mask_exists else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:54:03.470983Z","iopub.status.idle":"2025-04-14T19:54:03.471279Z","shell.execute_reply":"2025-04-14T19:54:03.471148Z"}},"outputs":[],"execution_count":null}]}