{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdb79df",
   "metadata": {},
   "source": [
    "# FarmWise: Farmland Segmentation and Size Classification with U-Net\n",
    "\n",
    "**Date**: April 14, 2025\n",
    "\n",
    "This notebook implements a farm segmentation system using U-Net architecture to identify agricultural fields from satellite imagery, calculate their sizes, and classify them for targeted recommendations.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal**: Create a system that can:\n",
    "1. Detect and segment farmlands from satellite imagery\n",
    "2. Calculate the size/area of each identified farm\n",
    "3. Classify farms by size (small, medium, large)\n",
    "4. Enable a recommendation system based on farm size classification\n",
    "\n",
    "**Approach**: U-Net architecture for semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32e0b7",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Agricultural recommendations are most effective when tailored to the specific context of a farm, with farm size being a crucial factor. Large farms may benefit from different techniques, equipment, and crop selections compared to small ones. This project aims to automatically classify farms by size from satellite imagery to enable targeted recommendations.\n",
    "\n",
    "### 1.2 Success Criteria\n",
    "\n",
    "- **Technical Success**: Achieve high accuracy in farmland segmentation (IoU > 0.75)\n",
    "- **Business Success**: Enable accurate size-based classification of farms for targeted recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0f576",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Understanding\n",
    "\n",
    "### 2.1 Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4dd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Kaggle environment and set up dependencies for GPU acceleration\n",
    "!pip install torch torchvision matplotlib numpy pillow scikit-learn scikit-image opencv-python roboflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from tqdm.notebook import tqdm\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability and set up CUDA device\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Use all available GPUs if there are multiple\n",
    "    if num_gpus > 1:\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using {num_gpus} GPUs for data parallel training\")\n",
    "    else:\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"Using single GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU. This will be slower.\")\n",
    "\n",
    "# Display CUDA version if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a57f6",
   "metadata": {},
   "source": [
    "### 2.2 Data Acquisition from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64caafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Roboflow and load dataset\n",
    "# Note: You will need to provide your Roboflow API key\n",
    "rf = Roboflow(api_key=\"HE9CEH5JxJ3U0vXrQTOy\")  # Replace with your actual API key\n",
    "project = rf.workspace(\"sid-mp92l\").project(\"final-detectron-2\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "\n",
    "# Print dataset path\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec85ec",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "def explore_directory(path, level=0):\n",
    "    print('  ' * level + f\"|-- {os.path.basename(path)}\")\n",
    "    if os.path.isdir(path):\n",
    "        for item in os.listdir(path)[:10]:  # Limit to first 10 items\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                explore_directory(item_path, level + 1)\n",
    "            else:\n",
    "                print('  ' * (level + 1) + f\"|-- {item}\")\n",
    "        if len(os.listdir(path)) > 10:\n",
    "            print('  ' * (level + 1) + f\"|-- ... ({len(os.listdir(path)) - 10} more items)\")\n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "explore_directory(dataset.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f90d870",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize some sample images and masks (Enhanced for Segmentation Focus)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m \u001b[38;5;66;03m# Ensure yaml is imported if not already\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "# Visualize some sample images and masks (Enhanced)\n",
    "import yaml # Ensure yaml is imported if not already\n",
    "\n",
    "def visualize_samples(data_dir, num_samples=3):\n",
    "    # Paths for train images and labels\n",
    "    train_img_dir = os.path.join(data_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(data_dir, 'train', 'labels')\n",
    "\n",
    "    img_files = os.listdir(train_img_dir)\n",
    "    # Ensure we only process image files and handle potential non-image files\n",
    "    img_files = [f for f in img_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    # Shuffle or select randomly if desired\n",
    "    # import random\n",
    "    # random.shuffle(img_files)\n",
    "    img_files = img_files[:num_samples]\n",
    "\n",
    "    if not img_files:\n",
    "        print(f\"No image files found in {train_img_dir}\")\n",
    "        return\n",
    "\n",
    "    # Read the data.yaml file to get class information\n",
    "    yaml_path = os.path.join(data_dir, 'data.yaml')\n",
    "    class_names = ['Unknown']\n",
    "    farm_class_id = None\n",
    "    if os.path.exists(yaml_path):\n",
    "        try:\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                data_yaml = yaml.safe_load(f)\n",
    "                if 'names' in data_yaml:\n",
    "                    class_names = data_yaml['names']\n",
    "                    print(f\"Classes found in dataset: {class_names}\")\n",
    "                    # Try to find the 'farm' class ID\n",
    "                    for i, name in enumerate(class_names):\n",
    "                        if \"farm\" in name.lower():\n",
    "                            farm_class_id = i\n",
    "                            print(f\"Identified 'farm' class ID: {farm_class_id}\")\n",
    "                            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading class names from data.yaml: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: data.yaml not found at {yaml_path}. Class names unknown.\")\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples)) # Increased width for 3 columns\n",
    "\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        # Load image\n",
    "        img_path = os.path.join(train_img_dir, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_np = np.array(img) # Keep a numpy copy for drawing outlines\n",
    "            img_width, img_height = img.size\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "            # Set placeholder titles if image fails to load\n",
    "            if num_samples == 1:\n",
    "                 axes[0].set_title(f\"Error loading {img_file}\")\n",
    "                 axes[1].set_title(\"Mask N/A\")\n",
    "                 axes[2].set_title(\"Raw Polygons N/A\")\n",
    "                 axes[0].axis('off'); axes[1].axis('off'); axes[2].axis('off')\n",
    "            else:\n",
    "                 axes[i, 0].set_title(f\"Error loading {img_file}\")\n",
    "                 axes[i, 1].set_title(\"Mask N/A\")\n",
    "                 axes[i, 2].set_title(\"Raw Polygons N/A\")\n",
    "                 axes[i, 0].axis('off'); axes[i, 1].axis('off'); axes[i, 2].axis('off')\n",
    "            continue # Skip to next image\n",
    "\n",
    "        # Find corresponding mask (YOLOv8 format)\n",
    "        mask_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        mask_path = os.path.join(train_mask_dir, mask_file)\n",
    "\n",
    "        # Create empty mask for filled visualization\n",
    "        filled_mask = np.zeros(img.size[::-1], dtype=np.uint8) # (height, width)\n",
    "\n",
    "        # Plot original image in first column\n",
    "        ax_img = axes[i, 0] if num_samples > 1 else axes[0]\n",
    "        ax_img.imshow(img_np)\n",
    "        ax_img.set_title(f\"Image: {img_file}\")\n",
    "        ax_img.axis('off')\n",
    "\n",
    "        # Plot image for raw polygon drawing in third column\n",
    "        ax_raw = axes[i, 2] if num_samples > 1 else axes[2]\n",
    "        ax_raw.imshow(img_np)\n",
    "        ax_raw.set_title(f\"Raw Polygons (File: {mask_file})\")\n",
    "        ax_raw.axis('off')\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            # Read YOLOv8 format annotations\n",
    "            with open(mask_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            print(f\"\\nProcessing Annotations for: {img_file}\")\n",
    "            found_polygons = False\n",
    "            for line_idx, line in enumerate(lines):\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) < 5:\n",
    "                    print(f\"  Line {line_idx+1}: Malformed annotation (less than 5 parts)\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    class_id = int(parts[0])\n",
    "                    current_class_name = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n",
    "\n",
    "                    # Filter for farm class IF farm_class_id was found, otherwise process all\n",
    "                    if farm_class_id is not None and class_id != farm_class_id:\n",
    "                         print(f\"  Line {line_idx+1}: Skipping class '{current_class_name}' (ID {class_id}), not farm class.\")\n",
    "                         continue\n",
    "                    else:\n",
    "                         print(f\"  Line {line_idx+1}: Processing class '{current_class_name}' (ID {class_id})\")\n",
    "\n",
    "\n",
    "                    # Check if we have polygon points (instance segmentation)\n",
    "                    if len(parts) > 5:\n",
    "                        found_polygons = True\n",
    "                        # Extract polygon points\n",
    "                        polygon_points_pixels = []\n",
    "                        for j in range(5, len(parts), 2):\n",
    "                            if j + 1 < len(parts):\n",
    "                                x = float(parts[j]) * img_width\n",
    "                                y = float(parts[j+1]) * img_height\n",
    "                                polygon_points_pixels.append((int(x), int(y)))\n",
    "                            else:\n",
    "                                print(f\"  Warning: Odd number of polygon coordinates on line {line_idx+1}\")\n",
    "                                break # Stop processing points for this line\n",
    "\n",
    "                        print(f\"    Found {len(polygon_points_pixels)} vertices.\")\n",
    "\n",
    "                        if len(polygon_points_pixels) >= 3: # Need at least 3 points for a polygon\n",
    "                            # Convert to numpy array for OpenCV\n",
    "                            pts = np.array(polygon_points_pixels, np.int32)\n",
    "                            pts = pts.reshape((-1, 1, 2))\n",
    "\n",
    "                            # --- Draw Filled Polygon on Mask (Column 2) ---\n",
    "                            cv2.fillPoly(filled_mask, [pts], 255)\n",
    "\n",
    "                            # --- Draw Raw Polygon Outline/Vertices (Column 3) ---\n",
    "                            # Use a unique color for each polygon if needed, here using lime\n",
    "                            outline_color_bgr = (0, 255, 0) # Lime Green in BGR for OpenCV\n",
    "                            vertex_color_bgr = (0, 0, 255) # Red in BGR\n",
    "\n",
    "                            # Draw outline directly onto the numpy image copy\n",
    "                            # We draw on img_np which is displayed by ax_raw\n",
    "                            cv2.polylines(img_np, [pts], isClosed=True, color=outline_color_bgr, thickness=1)\n",
    "\n",
    "                            # Draw vertices\n",
    "                            for k, (px, py) in enumerate(polygon_points_pixels):\n",
    "                                cv2.circle(img_np, (px, py), radius=2, color=vertex_color_bgr, thickness=-1)\n",
    "                                # Optional: Add vertex number text (can get crowded)\n",
    "                                # cv2.putText(img_np, str(k+1), (px+2, py+2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)\n",
    "\n",
    "                        else:\n",
    "                             print(f\"    Skipping polygon on line {line_idx+1}: Not enough vertices ({len(polygon_points_pixels)} found).\")\n",
    "\n",
    "                    else: # Use bounding box if no polygon points\n",
    "                        print(f\"    No polygon points found, using bounding box.\")\n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "\n",
    "                        x1 = max(0, int(x_center - width / 2))\n",
    "                        y1 = max(0, int(y_center - height / 2))\n",
    "                        x2 = min(img_width - 1, int(x_center + width / 2))\n",
    "                        y2 = min(img_height - 1, int(y_center + height / 2))\n",
    "\n",
    "                        # Draw rectangle on filled mask (Column 2)\n",
    "                        cv2.rectangle(filled_mask, (x1, y1), (x2, y2), 255, -1) # Fill rectangle\n",
    "\n",
    "                        # Draw bounding box outline (Column 3)\n",
    "                        # Use a different color for bounding boxes, e.g., Blue\n",
    "                        bbox_color_bgr = (255, 0, 0) # Blue in BGR\n",
    "                        cv2.rectangle(img_np, (x1, y1), (x2, y2), bbox_color_bgr, thickness=1)\n",
    "\n",
    "                except ValueError as ve:\n",
    "                     print(f\"  Error parsing line {line_idx+1}: {ve} - Line: '{line.strip()}'\")\n",
    "                except IndexError as ie:\n",
    "                     print(f\"  Error accessing class name for ID {class_id} on line {line_idx+1}: {ie}\")\n",
    "                except Exception as ex:\n",
    "                     print(f\"  Unexpected error processing line {line_idx+1}: {ex}\")\n",
    "\n",
    "\n",
    "            # Update the display for the third column after drawing all raw polygons/bboxes\n",
    "            ax_raw.imshow(img_np) # Re-display image with drawings\n",
    "\n",
    "            if not found_polygons:\n",
    "                 print(f\"  Note: No polygon data found in file {mask_file}, only bounding boxes (if any).\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Mask file not found: {mask_path}\")\n",
    "            # Indicate missing mask in titles\n",
    "            if num_samples == 1:\n",
    "                 axes[1].set_title(\"Mask file missing\")\n",
    "                 axes[2].set_title(\"Raw Polygons N/A\")\n",
    "            else:\n",
    "                 axes[i, 1].set_title(\"Mask file missing\")\n",
    "                 axes[i, 2].set_title(\"Raw Polygons N/A\")\n",
    "\n",
    "        # Display filled mask (Column 2)\n",
    "        ax_mask = axes[i, 1] if num_samples > 1 else axes[1]\n",
    "        ax_mask.imshow(filled_mask, cmap='gray')\n",
    "        ax_mask.set_title(f\"Generated Mask (from YOLO file)\")\n",
    "        ax_mask.axis('off')\n",
    "\n",
    "        # Turn off axis for the third column if it wasn't already\n",
    "        ax_raw.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout slightly\n",
    "    plt.suptitle(\"Sample Images, Generated Masks, and Raw Annotations\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# --- Run the visualization ---\n",
    "try:\n",
    "    # Make sure 'dataset.location' is defined from the download cell\n",
    "    if 'dataset' in locals() and hasattr(dataset, 'location'):\n",
    "         visualize_samples(dataset.location, num_samples=5) # Increase num_samples if desired\n",
    "    else:\n",
    "         print(\"Error: 'dataset' variable or 'dataset.location' not defined.\")\n",
    "         print(\"Please ensure the Roboflow download cell has been run successfully.\")\n",
    "except NameError:\n",
    "     print(\"Error: 'dataset' variable not defined.\")\n",
    "     print(\"Please ensure the Roboflow download cell has been run successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during visualization: {e}\")\n",
    "    print(\"Note: Adjust the visualization code based on the actual data format and file structure.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecd8c6",
   "metadata": {},
   "source": [
    "### 2.4 Data Preparation\n",
    "\n",
    "We need to convert YOLOv8 format annotations to segmentation masks for U-Net training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e34e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 2.3: Dataset Exploration and Definition ===\n",
    "\n",
    "# Define paths after loading the dataset\n",
    "# Ensure the 'dataset' variable is available from the download cell\n",
    "if 'dataset' in locals() and hasattr(dataset, 'location'):\n",
    "    dataset_base_dir = dataset.location # For finding data.yaml and overall root\n",
    "    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n",
    "    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n",
    "    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n",
    "    print(f\"Dataset paths set using location: {dataset.location}\")\n",
    "else:\n",
    "    # Fallback or error if dataset location is not defined\n",
    "    print(\"ERROR: 'dataset' variable or 'dataset.location' not found.\")\n",
    "    print(\"Please run the Roboflow download cell first.\")\n",
    "    # Define dummy paths to avoid crashing subsequent cells, but processing will fail\n",
    "    dataset_base_dir = \".\"\n",
    "    train_img_dir = os.path.join(dataset_base_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(dataset_base_dir, 'train', 'labels')\n",
    "    val_img_dir = os.path.join(dataset_base_dir, 'valid', 'images')\n",
    "    val_mask_dir = os.path.join(dataset_base_dir, 'valid', 'labels')\n",
    "\n",
    "# --- Custom Dataset Class ---\n",
    "# Create a custom dataset class to load images and generate masks\n",
    "class FarmlandDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading satellite images and generating segmentation masks\n",
    "    from YOLOv8 polygon annotation files.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, mask_dir, dataset_root_dir, transform=None, farm_class_name=\"farm\", img_size=256):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size # Target size for model input (used if no transform)\n",
    "        self.farm_class_name = farm_class_name\n",
    "\n",
    "        # Filter for valid image files only\n",
    "        try:\n",
    "            self.img_files = sorted([\n",
    "                f for f in os.listdir(img_dir)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ])\n",
    "            if not self.img_files:\n",
    "                 print(f\"Warning: No image files found in {self.img_dir}\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Error: Image directory not found: {self.img_dir}\")\n",
    "             self.img_files = []\n",
    "\n",
    "        self.class_names = ['Unknown']\n",
    "        self.farm_class_id = None # Determined from data.yaml\n",
    "        self._load_class_info(dataset_root_dir)\n",
    "\n",
    "\n",
    "    def _load_class_info(self, dataset_root_dir):\n",
    "        \"\"\"Loads class names and identifies the farm class ID from data.yaml.\"\"\"\n",
    "        yaml_path = os.path.join(dataset_root_dir, 'data.yaml')\n",
    "        if os.path.exists(yaml_path):\n",
    "            try:\n",
    "                import yaml\n",
    "                with open(yaml_path, 'r') as f:\n",
    "                    data_yaml = yaml.safe_load(f)\n",
    "                    if 'names' in data_yaml:\n",
    "                        self.class_names = data_yaml['names']\n",
    "                        print(f\"Dataset Class Names: {self.class_names}\")\n",
    "                        # Find the specific farm class ID using the name\n",
    "                        for i, name in enumerate(self.class_names):\n",
    "                            if self.farm_class_name.lower() in name.lower():\n",
    "                                self.farm_class_id = i\n",
    "                                print(f\"Found target class '{name}' with ID: {self.farm_class_id}\")\n",
    "                                break\n",
    "                        if self.farm_class_id is None:\n",
    "                            print(f\"Warning: Target class name '{self.farm_class_name}' not found in data.yaml names: {self.class_names}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: 'names' key not found in {yaml_path}\")\n",
    "\n",
    "            except ImportError:\n",
    "                 print(\"Warning: PyYAML not installed. Cannot read class names from data.yaml.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading class names from data.yaml: {e}\")\n",
    "        else:\n",
    "             print(f\"Warning: data.yaml not found at {yaml_path}. Cannot determine farm class ID automatically.\")\n",
    "             print(\"The dataset will attempt to use ALL annotations if farm_class_id remains None.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.img_files):\n",
    "             raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        img_filename = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            original_width, original_height = image.size\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Returning dummy data.\")\n",
    "            # Return dummy tensors to avoid crashing DataLoader worker\n",
    "            dummy_image = torch.zeros((3, self.img_size, self.img_size))\n",
    "            dummy_mask = torch.zeros((1, self.img_size, self.img_size)) # Match mask dims\n",
    "            return dummy_image, dummy_mask\n",
    "\n",
    "        # --- Mask Generation ---\n",
    "        mask_file = os.path.splitext(img_filename)[0] + '.txt'\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "\n",
    "        # Create empty mask with the *original* image size first\n",
    "        # Use float32 for direct conversion to tensor later, filled with 0.0\n",
    "        mask = np.zeros((original_height, original_width), dtype=np.float32)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            try:\n",
    "                with open(mask_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                processed_polygon = False\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split(' ')\n",
    "                    if len(parts) < 5: continue # Skip malformed lines (need at least class + 2 points)\n",
    "\n",
    "                    class_id = int(parts[0])\n",
    "\n",
    "                    # Only process the target farm class if ID is known and matches\n",
    "                    # If farm_class_id is None, process all classes found.\n",
    "                    if self.farm_class_id is not None and class_id != self.farm_class_id:\n",
    "                        continue\n",
    "\n",
    "                    # --- PRIORITIZE POLYGON DATA for segmentation ---\n",
    "                    if len(parts) > 5: # Indicates polygon points are present\n",
    "                        # Extract polygon points (normalized)\n",
    "                        polygon_points_normalized = []\n",
    "                        # Iterate over coordinate pairs (parts[1], parts[2]), (parts[3], parts[4]), ...\n",
    "                        # For YOLOv8 segmentation format, it's: class x1 y1 x2 y2 ... xN yN\n",
    "                        for j in range(1, len(parts), 2):\n",
    "                            if j + 1 < len(parts):\n",
    "                                x_norm = float(parts[j])\n",
    "                                y_norm = float(parts[j+1])\n",
    "                                polygon_points_normalized.append((x_norm, y_norm))\n",
    "\n",
    "                        # Denormalize to pixel coordinates\n",
    "                        polygon_points_pixels = [\n",
    "                            (int(x * original_width), int(y * original_height))\n",
    "                            for x, y in polygon_points_normalized\n",
    "                        ]\n",
    "\n",
    "                        if len(polygon_points_pixels) >= 3:\n",
    "                            pts = np.array(polygon_points_pixels, np.int32)\n",
    "                            pts = pts.reshape((-1, 1, 2))\n",
    "                            # Fill polygon with 1.0 (for float32 mask)\n",
    "                            cv2.fillPoly(mask, [pts], 1.0) # Use 1.0 for float mask\n",
    "                            processed_polygon = True\n",
    "                        # else: (Optional) print warning about insufficient points for polygon\n",
    "                    # else:\n",
    "                         # If len(parts) == 5, it's a bounding box. Ignore for segmentation mask.\n",
    "                         # print(f\"Skipping bounding box annotation for {img_filename} line: {line.strip()}\")\n",
    "                         # pass # Explicitly do nothing for bounding boxes\n",
    "\n",
    "                # If no valid farm polygons were found in the file, mask remains zeros\n",
    "                # if not processed_polygon and self.farm_class_id is not None:\n",
    "                #    print(f\"Warning: No valid polygons found for farm class {self.farm_class_id} in {mask_path}\")\n",
    "\n",
    "\n",
    "            except ValueError as ve:\n",
    "                 print(f\"Error parsing values in {mask_path} for image {img_filename}: {ve}. Line: '{line.strip()}'\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing annotation file {mask_path} for image {img_filename}: {e}\")\n",
    "                 # Mask remains zeros if annotation processing fails\n",
    "\n",
    "        # --- Transformations ---\n",
    "        # Convert mask numpy array to PIL Image to apply transforms consistently ONLY IF NEEDED BY TRANSFORM\n",
    "        # Usually, resizing is done separately for masks\n",
    "\n",
    "        # Default resize and tensor conversion if no transform is provided\n",
    "        if not self.transform:\n",
    "            resizer = transforms.Resize((self.img_size, self.img_size), interpolation=transforms.InterpolationMode.BILINEAR) # For image\n",
    "            to_tensor = transforms.ToTensor()\n",
    "\n",
    "            image_resized = resizer(image)\n",
    "            image_tensor = to_tensor(image_resized)\n",
    "\n",
    "            # Resize mask using NEAREST interpolation\n",
    "            mask_resized = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "            # Add channel dimension [H, W] -> [1, H, W]\n",
    "            mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "             # Apply transforms to image (usually includes resize, ToTensor, Normalize)\n",
    "             image_tensor = self.transform(image)\n",
    "\n",
    "             # Resize mask separately using NEAREST interpolation to match image tensor size\n",
    "             # Target size from the transformed image tensor\n",
    "             target_h, target_w = image_tensor.shape[-2:]\n",
    "             mask_resized = cv2.resize(mask, (target_w, target_h), interpolation=cv2.INTER_NEAREST)\n",
    "             # Convert resized mask to tensor, add channel dimension\n",
    "             mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n",
    "\n",
    "\n",
    "        # Final check for shape consistency\n",
    "        if image_tensor.shape[-2:] != mask_tensor.shape[-2:]:\n",
    "              print(f\"Warning: Final Image tensor shape {image_tensor.shape} != Final Mask tensor shape {mask_tensor.shape} for {img_filename}. Attempting F.interpolate.\")\n",
    "              # Requires: import torch.nn.functional as F\n",
    "              try:\n",
    "                  mask_tensor = F.interpolate( # Ensure torch.nn.functional is imported as F earlier\n",
    "                       mask_tensor.unsqueeze(0), # Add batch dim for interpolate [N, C, H, W]\n",
    "                       size=image_tensor.shape[-2:],\n",
    "                       mode='nearest'\n",
    "                  ).squeeze(0) # Remove batch dim\n",
    "                  print(f\"Mask reshaped via interpolate to: {mask_tensor.shape}\")\n",
    "              except Exception as interp_e:\n",
    "                   print(f\"ERROR: Could not reshape mask using F.interpolate: {interp_e}\")\n",
    "                   # Return dummy tensors if resizing fails critically\n",
    "                   dummy_image = torch.zeros((3, self.img_size, self.img_size))\n",
    "                   dummy_mask = torch.zeros((1, self.img_size, self.img_size))\n",
    "                   return dummy_image, dummy_mask\n",
    "\n",
    "\n",
    "        # Clamp mask values to be safe, although fillPoly uses 1.0\n",
    "        mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "\n",
    "# --- Visualization Function (Debugging Raw Annotations) ---\n",
    "# Visualize some sample images and how masks are generated from the raw YOLO files\n",
    "import yaml # Ensure yaml is imported\n",
    "\n",
    "def visualize_samples(data_dir, num_samples=3):\n",
    "    \"\"\"Visualizes original images, generated masks from YOLO files, and raw annotations.\"\"\"\n",
    "    print(\"\\n--- Starting Sample Visualization ---\")\n",
    "    # Paths for train images and labels\n",
    "    train_img_dir = os.path.join(data_dir, 'train', 'images')\n",
    "    train_mask_dir = os.path.join(data_dir, 'train', 'labels')\n",
    "\n",
    "    if not os.path.isdir(train_img_dir):\n",
    "        print(f\"Error: Train image directory not found: {train_img_dir}\")\n",
    "        return\n",
    "    if not os.path.isdir(train_mask_dir):\n",
    "        print(f\"Error: Train mask directory not found: {train_mask_dir}\")\n",
    "        return\n",
    "\n",
    "    img_files_all = os.listdir(train_img_dir)\n",
    "    # Ensure we only process image files and handle potential non-image files\n",
    "    img_files = [f for f in img_files_all if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    # Shuffle or select randomly if desired\n",
    "    # import random\n",
    "    # random.shuffle(img_files)\n",
    "    img_files = img_files[:num_samples]\n",
    "\n",
    "    if not img_files:\n",
    "        print(f\"No image files found in {train_img_dir}\")\n",
    "        return\n",
    "\n",
    "    # Read the data.yaml file to get class information\n",
    "    yaml_path = os.path.join(data_dir, 'data.yaml')\n",
    "    class_names = ['Unknown']\n",
    "    farm_class_id = None\n",
    "    if os.path.exists(yaml_path):\n",
    "        try:\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                data_yaml = yaml.safe_load(f)\n",
    "                if 'names' in data_yaml:\n",
    "                    class_names = data_yaml['names']\n",
    "                    print(f\"Classes found in dataset: {class_names}\")\n",
    "                    # Try to find the 'farm' class ID (case-insensitive)\n",
    "                    for i, name in enumerate(class_names):\n",
    "                        if \"farm\" in name.lower():\n",
    "                            farm_class_id = i\n",
    "                            print(f\"Identified 'farm' class ID for visualization: {farm_class_id}\")\n",
    "                            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading class names from data.yaml: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: data.yaml not found at {yaml_path}. Class names unknown for visualization.\")\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))\n",
    "    # Adjust title if only one sample\n",
    "    if num_samples == 1:\n",
    "        fig.suptitle(\"Sample Image, Generated Mask, and Raw Annotations\", fontsize=16)\n",
    "    else:\n",
    "        plt.suptitle(\"Sample Images, Generated Masks, and Raw Annotations\", fontsize=16)\n",
    "\n",
    "\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        # Define axes for this row\n",
    "        ax_img = axes[i, 0] if num_samples > 1 else axes[0]\n",
    "        ax_mask = axes[i, 1] if num_samples > 1 else axes[1]\n",
    "        ax_raw = axes[i, 2] if num_samples > 1 else axes[2]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(train_img_dir, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_np = np.array(img) # Keep a numpy copy for drawing outlines\n",
    "            img_height, img_width = img_np.shape[:2] # Correct order for numpy array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "            ax_img.set_title(f\"Error loading {img_file}\"); ax_img.axis('off')\n",
    "            ax_mask.set_title(\"Mask N/A\"); ax_mask.axis('off')\n",
    "            ax_raw.set_title(\"Raw Annotations N/A\"); ax_raw.axis('off')\n",
    "            continue # Skip to next image\n",
    "\n",
    "        # Find corresponding mask file (YOLOv8 format)\n",
    "        mask_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        mask_path = os.path.join(train_mask_dir, mask_file)\n",
    "\n",
    "        # Create empty mask for filled visualization (same size as original image)\n",
    "        filled_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "        # Plot original image in first column\n",
    "        ax_img.imshow(img_np)\n",
    "        ax_img.set_title(f\"Image: {img_file}\")\n",
    "        ax_img.axis('off')\n",
    "\n",
    "        # Copy original image for drawing raw annotations in third column\n",
    "        img_np_raw = img_np.copy()\n",
    "        ax_raw.set_title(f\"Raw Annots (File: {mask_file})\")\n",
    "        ax_raw.axis('off') # Turn off axis first\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            # Read YOLOv8 format annotations\n",
    "            try:\n",
    "                with open(mask_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                print(f\"\\nProcessing Annotations for visualization: {img_file}\")\n",
    "                found_farm_polygon = False\n",
    "                found_farm_bbox = False\n",
    "\n",
    "                for line_idx, line in enumerate(lines):\n",
    "                    parts = line.strip().split(' ')\n",
    "                    if len(parts) < 5:\n",
    "                        print(f\"  Line {line_idx+1}: Malformed annotation (less than 5 parts)\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                        current_class_name = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n",
    "\n",
    "                        # Filter: Only draw annotations for the 'farm' class if ID is known\n",
    "                        if farm_class_id is not None and class_id != farm_class_id:\n",
    "                             # print(f\"  Line {line_idx+1}: Skipping class '{current_class_name}' (ID {class_id}), not farm class.\")\n",
    "                             continue\n",
    "                        # else: # If farm_class_id is None, process all classes\n",
    "                             # print(f\"  Line {line_idx+1}: Processing class '{current_class_name}' (ID {class_id}) for visualization\")\n",
    "\n",
    "                        # Check if we have polygon points (YOLOv8 segmentation format)\n",
    "                        # class x1 y1 x2 y2 ... xN yN\n",
    "                        if len(parts) > 5 and len(parts) % 2 == 1: # Must have odd number of parts (class + pairs)\n",
    "                            is_polygon = True\n",
    "                            coords = parts[1:]\n",
    "                        # Check if we have bounding box points (YOLOv8 detection format)\n",
    "                        # class x_center y_center width height\n",
    "                        elif len(parts) == 5:\n",
    "                            is_polygon = False\n",
    "                            coords = parts[1:]\n",
    "                        else:\n",
    "                            print(f\"  Line {line_idx+1}: Ambiguous annotation format (parts: {len(parts)})\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if is_polygon:\n",
    "                            # --- Polygon Processing ---\n",
    "                            polygon_points_pixels = []\n",
    "                            for j in range(0, len(coords), 2): # Iterate through x, y pairs\n",
    "                                x = float(coords[j]) * img_width\n",
    "                                y = float(coords[j+1]) * img_height\n",
    "                                polygon_points_pixels.append((int(x), int(y)))\n",
    "\n",
    "                            # print(f\"    Found {len(polygon_points_pixels)} polygon vertices for class {class_id}.\")\n",
    "\n",
    "                            if len(polygon_points_pixels) >= 3:\n",
    "                                found_farm_polygon = True # Mark that we found one for this class\n",
    "                                pts = np.array(polygon_points_pixels, np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "                                # --- Draw Filled Polygon on Mask (Column 2) ---\n",
    "                                cv2.fillPoly(filled_mask, [pts], 255) # Fill with white\n",
    "\n",
    "                                # --- Draw Raw Polygon Outline/Vertices (Column 3) ---\n",
    "                                outline_color_bgr = (0, 255, 0) # Lime Green (BGR)\n",
    "                                vertex_color_bgr = (0, 0, 255) # Red (BGR)\n",
    "\n",
    "                                cv2.polylines(img_np_raw, [pts], isClosed=True, color=outline_color_bgr, thickness=1)\n",
    "                                for k, (px, py) in enumerate(polygon_points_pixels):\n",
    "                                    cv2.circle(img_np_raw, (px, py), radius=2, color=vertex_color_bgr, thickness=-1)\n",
    "                            else:\n",
    "                                 print(f\"    Skipping polygon on line {line_idx+1}: Not enough vertices ({len(polygon_points_pixels)} found).\")\n",
    "\n",
    "                        else: # Bounding Box Processing\n",
    "                            # print(f\"    Found bounding box for class {class_id}.\")\n",
    "                            found_farm_bbox = True # Mark that we found one for this class\n",
    "                            x_center, y_center, width, height = map(float, coords)\n",
    "\n",
    "                            x1 = max(0, int((x_center - width / 2) * img_width))\n",
    "                            y1 = max(0, int((y_center - height / 2) * img_height))\n",
    "                            x2 = min(img_width - 1, int((x_center + width / 2) * img_width))\n",
    "                            y2 = min(img_height - 1, int((y_center + height / 2) * img_height))\n",
    "\n",
    "                            # --- Draw Filled Rectangle on Mask (Column 2) ---\n",
    "                            # NOTE: This is only for VISUALIZATION. Dataset ignores bboxes for masks.\n",
    "                            cv2.rectangle(filled_mask, (x1, y1), (x2, y2), 128, -1) # Fill with gray to differentiate\n",
    "\n",
    "                            # --- Draw Raw Bounding Box Outline (Column 3) ---\n",
    "                            bbox_color_bgr = (255, 0, 0) # Blue (BGR)\n",
    "                            cv2.rectangle(img_np_raw, (x1, y1), (x2, y2), bbox_color_bgr, thickness=1)\n",
    "\n",
    "                    except ValueError as ve:\n",
    "                         print(f\"  Error parsing values on line {line_idx+1}: {ve} - Line: '{line.strip()}'\")\n",
    "                    except IndexError as ie:\n",
    "                         print(f\"  Error accessing class name for ID {class_id} on line {line_idx+1}: {ie}\")\n",
    "                    except Exception as ex:\n",
    "                         print(f\"  Unexpected error processing line {line_idx+1}: {ex}\")\n",
    "\n",
    "                # Report findings for the specific farm class\n",
    "                if farm_class_id is not None:\n",
    "                    if found_farm_polygon:\n",
    "                        print(f\"  Visualized polygons for farm class {farm_class_id}.\")\n",
    "                    if found_farm_bbox:\n",
    "                        print(f\"  Visualized bounding boxes for farm class {farm_class_id} (Note: Dataset ignores these for masks).\")\n",
    "                    if not found_farm_polygon and not found_farm_bbox:\n",
    "                         print(f\"  No annotations found for farm class {farm_class_id} in {mask_file}.\")\n",
    "\n",
    "            except Exception as read_ex:\n",
    "                 print(f\"Error reading or processing mask file {mask_path}: {read_ex}\")\n",
    "                 ax_mask.set_title(\"Error reading mask file\")\n",
    "                 ax_raw.set_title(\"Error reading mask file\")\n",
    "\n",
    "\n",
    "        else: # Mask file doesn't exist\n",
    "            print(f\"Mask file not found: {mask_path}\")\n",
    "            ax_mask.set_title(\"Mask file missing\")\n",
    "            ax_raw.set_title(\"Annotation file missing\")\n",
    "\n",
    "        # Display filled mask (Column 2) - Shows polygons (white) and bboxes (gray) if found\n",
    "        ax_mask.imshow(filled_mask, cmap='gray', vmin=0, vmax=255)\n",
    "        ax_mask.set_title(\"Generated Mask (Polygons=White, BBox=Gray)\")\n",
    "        ax_mask.axis('off')\n",
    "\n",
    "        # Display image with raw annotations drawn (Column 3)\n",
    "        ax_raw.imshow(img_np_raw)\n",
    "        ax_raw.axis('off') # Ensure axis is off\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout slightly\n",
    "    plt.show()\n",
    "    print(\"--- Finished Sample Visualization ---\")\n",
    "\n",
    "\n",
    "# --- Run the visualization ---\n",
    "# Make sure 'dataset_base_dir' is defined and valid\n",
    "if 'dataset_base_dir' in locals() and os.path.isdir(dataset_base_dir):\n",
    "    try:\n",
    "        visualize_samples(dataset_base_dir, num_samples=5) # Increase num_samples if desired\n",
    "    except NameError as ne:\n",
    "         print(f\"A NameError occurred during visualization: {ne}\")\n",
    "         print(\"Ensure all necessary variables and functions are defined.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "        print(\"Check the visualization code and dataset paths.\")\n",
    "else:\n",
    "     print(\"Error: 'dataset_base_dir' is not defined or is not a valid directory.\")\n",
    "     print(\"Please ensure the Roboflow download cell and path definition cell have been run successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ced9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up augmented data transformations to prevent overfitting\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flips\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Random vertical flips\n",
    "    transforms.RandomRotation(10),           # Random rotations up to 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Keep validation transform simple\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets with separate transforms\n",
    "train_dataset = FarmlandDataset(train_img_dir, train_mask_dir, transform=train_transform)\n",
    "val_dataset = FarmlandDataset(val_img_dir, val_mask_dir, transform=val_transform)\n",
    "\n",
    "# Optimize batch size based on available GPUs and model complexity\n",
    "if torch.cuda.device_count() > 1:\n",
    "    batch_size = 24  # Increased batch size for multiple GPUs\n",
    "else:\n",
    "    batch_size = 8   # Default batch size for single GPU\n",
    "\n",
    "# Determine optimal number of workers for data loading\n",
    "num_workers = min(os.cpu_count(), 16) if os.cpu_count() else 4\n",
    "\n",
    "# Configure DataLoader with more aggressive prefetching and optimized memory usage\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,        # More workers for faster data loading\n",
    "    pin_memory=True,                # Use pinned memory for faster CPU->GPU transfer\n",
    "    prefetch_factor=4,              # Prefetch more batches\n",
    "    persistent_workers=True,        # Keep worker processes alive between iterations\n",
    "    drop_last=True                  # Drop last incomplete batch for better performance\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Adjust CUDA settings for optimal performance\n",
    "torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner to find the best algorithm\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "torch.backends.cudnn.allow_tf32 = True        # Allow TF32 on Ampere GPUs\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Using batch size: {batch_size} with {torch.cuda.device_count()} GPUs\")\n",
    "print(f\"DataLoader configured with {num_workers} worker processes\")\n",
    "print(f\"Data augmentation enabled for training to prevent overfitting\")\n",
    "print(f\"CUDA optimizations enabled: benchmark={torch.backends.cudnn.benchmark}, TF32={torch.backends.cuda.matmul.allow_tf32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e03722",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "### 3.1 U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Model implementation with optimized memory usage\n",
    "class EfficientUNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(EfficientUNet, self).__init__()\n",
    "        \n",
    "        # Use GroupNorm instead of BatchNorm for better performance with large batch sizes\n",
    "        # and more efficiency on multiple GPUs\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder with memory-efficient skip connections\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.outconv = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights properly for faster convergence\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.GroupNorm):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        e4 = self.enc4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        u4 = self.upconv4(b)\n",
    "        u4 = torch.cat([u4, e4], dim=1)  # Skip connection\n",
    "        d4 = self.dec4(u4)\n",
    "        \n",
    "        u3 = self.upconv3(d4)\n",
    "        u3 = torch.cat([u3, e3], dim=1)  # Skip connection\n",
    "        d3 = self.dec3(u3)\n",
    "        \n",
    "        u2 = self.upconv2(d3)\n",
    "        u2 = torch.cat([u2, e2], dim=1)  # Skip connection\n",
    "        d2 = self.dec2(u2)\n",
    "        \n",
    "        u1 = self.upconv1(d2)\n",
    "        u1 = torch.cat([u1, e1], dim=1)  # Skip connection\n",
    "        d1 = self.dec1(u1)\n",
    "        \n",
    "        # Final output\n",
    "        out = self.outconv(d1)\n",
    "        return torch.sigmoid(out)  # Apply sigmoid for binary segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c16ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the efficient model and optimizer\n",
    "model = EfficientUNet(n_channels=3, n_classes=1)\n",
    "\n",
    "# Use DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer with weight decay for regularization\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines sigmoid and BCE for better numerical stability\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW with weight decay to reduce overfitting\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d17c6",
   "metadata": {},
   "source": [
    "### 3.2 Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, amp_scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for images, masks in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device, non_blocking=True)  # Use non_blocking for async transfer\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()  # Removed flush=True which is not supported in this version\n",
    "        \n",
    "        # Mixed precision training - updated to use newer API\n",
    "        if amp_scaler is not None:\n",
    "            with torch.amp.autocast(device_type='cuda'):  # Updated from torch.cuda.amp.autocast()\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            amp_scaler.scale(loss).backward()\n",
    "            amp_scaler.step(optimizer)\n",
    "            amp_scaler.update()\n",
    "        else:\n",
    "            # Standard training\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Function to calculate IoU (Intersection over Union)\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    intersection = (pred_binary * target).sum()\n",
    "    union = pred_binary.sum() + target.sum() - intersection\n",
    "    \n",
    "    iou = (intersection + 1e-8) / (union + 1e-8)  # Adding small epsilon to avoid division by zero\n",
    "    return iou.item()\n",
    "\n",
    "# Function to evaluate model on validation set\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate IoU for each image in batch\n",
    "            for i in range(outputs.size(0)):\n",
    "                iou = calculate_iou(outputs[i], masks[i], threshold)\n",
    "                iou_scores.append(iou)\n",
    "    \n",
    "    mean_iou = sum(iou_scores) / len(iou_scores)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a111f",
   "metadata": {},
   "source": [
    "### 3.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loop with mixed precision, gradient accumulation, and early stopping\n",
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_unet_model.pth'\n",
    "patience = 5  # Number of epochs to wait for improvement before early stopping\n",
    "no_improve_epochs = 0\n",
    "# current_lr = 0.001 # Initial LR is set in the optimizer, no need to track separately here initially\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_ious = []\n",
    "\n",
    "# Set up mixed precision training for faster computation\n",
    "scaler = None # Initialize scaler\n",
    "if torch.cuda.is_available():\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    print(\"Using mixed precision training for faster performance\")\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "# Set up learning rate scheduler (removed verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory before each epoch\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, amp_scaler=scaler)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validate\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate IoU\n",
    "    val_iou = evaluate_model(model, val_loader, device)\n",
    "    val_ious.append(val_iou)\n",
    "\n",
    "    # Get the current learning rate from the scheduler\n",
    "    # Use get_last_lr() which returns a list of LRs for each param group\n",
    "    last_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # Update learning rate based on validation performance\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation IoU: {val_iou:.4f}\")\n",
    "    # Print the LR *before* the scheduler potentially reduces it for the *next* epoch\n",
    "    print(f\"Learning rate for this epoch: {last_lr:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        improvement = best_val_loss - val_loss\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Model improved by {improvement:.6f} and saved to {best_model_path}\")\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\"No improvement for {no_improve_epochs} epochs\")\n",
    "\n",
    "        # Optional: Print a message if the LR was reduced by the scheduler\n",
    "        if scheduler.get_last_lr()[0] < last_lr:\n",
    "             print(f\"Learning rate reduced to {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eca62d",
   "metadata": {},
   "source": [
    "### 3.4 Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbe1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot IoU\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_ious, label='Validation IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('Validation IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448580ef",
   "metadata": {},
   "source": [
    "## 4. Farm Size Calculation and Classification\n",
    "\n",
    "Now we'll use our trained model to segment farms and calculate their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f254cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "# Function to segment farms in an image\n",
    "def segment_farms(model, image_path, device, transform):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = image.size  # (width, height)\n",
    "    \n",
    "    # Preprocess image\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_mask = output.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Threshold to get binary mask\n",
    "    binary_mask = (predicted_mask > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    # Resize mask back to original image size\n",
    "    binary_mask_resized = cv2.resize(binary_mask, (original_size[0], original_size[1]))\n",
    "    \n",
    "    return binary_mask_resized, np.array(image)\n",
    "\n",
    "# Function to calculate farm sizes and classify them\n",
    "def calculate_farm_sizes(binary_mask, pixels_per_meter=None):\n",
    "    # Use connected component analysis to identify individual farms\n",
    "    labeled_mask, num_farms = measure.label(binary_mask, connectivity=2, return_num=True)\n",
    "    \n",
    "    # Calculate properties of each labeled region\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "    \n",
    "    # Store farm areas\n",
    "    farm_areas = []\n",
    "    \n",
    "    for region in regions:\n",
    "        # Skip very small regions (likely noise)\n",
    "        if region.area < 100:  # Adjust threshold as needed\n",
    "            continue\n",
    "            \n",
    "        # Calculate area in pixels\n",
    "        area_pixels = region.area\n",
    "        \n",
    "        # Convert to real-world units if pixels_per_meter is provided\n",
    "        if pixels_per_meter is not None:\n",
    "            area_sq_meters = area_pixels / (pixels_per_meter ** 2)\n",
    "            # Convert to hectares (1 hectare = 10,000 sq meters)\n",
    "            area_hectares = area_sq_meters / 10000\n",
    "            farm_areas.append(area_hectares)\n",
    "        else:\n",
    "            farm_areas.append(area_pixels)\n",
    "    \n",
    "    return farm_areas, labeled_mask\n",
    "\n",
    "# Function to classify farms by size\n",
    "def classify_farms(farm_areas, unit='pixels'):\n",
    "    # Define size thresholds (adjust based on your specific context)\n",
    "    if unit == 'hectares':\n",
    "        # Real-world thresholds (in hectares)\n",
    "        small_threshold = 10     # 0-10 hectares = small farm\n",
    "        medium_threshold = 50    # 10-50 hectares = medium farm\n",
    "        # > 50 hectares = large farm\n",
    "    else:\n",
    "        # Pixel-based thresholds (adjust based on your image resolution)\n",
    "        small_threshold = 5000     # 0-5000 pixels = small farm\n",
    "        medium_threshold = 20000   # 5000-20000 pixels = medium farm\n",
    "        # > 20000 pixels = large farm\n",
    "    \n",
    "    # Classify each farm\n",
    "    farm_classes = []\n",
    "    for area in farm_areas:\n",
    "        if area < small_threshold:\n",
    "            farm_classes.append('Small')\n",
    "        elif area < medium_threshold:\n",
    "            farm_classes.append('Medium')\n",
    "        else:\n",
    "            farm_classes.append('Large')\n",
    "    \n",
    "    # Count farms in each category\n",
    "    class_counts = {\n",
    "        'Small': farm_classes.count('Small'),\n",
    "        'Medium': farm_classes.count('Medium'),\n",
    "        'Large': farm_classes.count('Large')\n",
    "    }\n",
    "    \n",
    "    return farm_classes, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize farm segmentation and classification\n",
    "def visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes):\n",
    "    # Create a colormap for visualization\n",
    "    cmap = plt.cm.colors.ListedColormap(['black', 'green', 'yellow', 'red'])\n",
    "    bounds = [0, 1, 2, 3, 4]\n",
    "    norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    # Create a colored mask based on farm classification\n",
    "    colored_mask = np.zeros_like(labeled_mask)\n",
    "    \n",
    "    for i, (region, area, farm_class) in enumerate(zip(measure.regionprops(labeled_mask), farm_areas, farm_classes)):\n",
    "        # Skip very small regions\n",
    "        if region.area < 100:\n",
    "            continue\n",
    "            \n",
    "        # Assign color based on class\n",
    "        if farm_class == 'Small':\n",
    "            color_value = 1\n",
    "        elif farm_class == 'Medium':\n",
    "            color_value = 2\n",
    "        else:  # Large\n",
    "            color_value = 3\n",
    "        \n",
    "        # Fill region with corresponding color\n",
    "        colored_mask[labeled_mask == region.label] = color_value\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    # Original image with segmentation overlay\n",
    "    ax1.imshow(image)\n",
    "    ax1.imshow(colored_mask, cmap=cmap, alpha=0.5, norm=norm)\n",
    "    ax1.set_title('Farm Segmentation and Classification')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color='green', alpha=0.5, label='Small Farms'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color='yellow', alpha=0.5, label='Medium Farms'),\n",
    "        plt.Rectangle((0, 0), 1, 1, color='red', alpha=0.5, label='Large Farms')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Pie chart of farm size distribution\n",
    "    class_counts = {\n",
    "        'Small': farm_classes.count('Small'),\n",
    "        'Medium': farm_classes.count('Medium'),\n",
    "        'Large': farm_classes.count('Large')\n",
    "    }\n",
    "    \n",
    "    if sum(class_counts.values()) > 0:  # Check if we have any farms\n",
    "        labels = list(class_counts.keys())\n",
    "        sizes = list(class_counts.values())\n",
    "        colors = ['green', 'yellow', 'red']\n",
    "        \n",
    "        ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "        ax2.set_title('Farm Size Distribution')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No farms detected', horizontalalignment='center', verticalalignment='center')\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fda47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample image\n",
    "def process_sample_image(image_path):\n",
    "    # Segment farms\n",
    "    binary_mask, image = segment_farms(model, image_path, device, transform)\n",
    "    \n",
    "    # Calculate farm sizes\n",
    "    # Note: In a real application, you would need to determine pixels_per_meter based on image metadata\n",
    "    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask)\n",
    "    \n",
    "    # Classify farms\n",
    "    farm_classes, class_counts = classify_farms(farm_areas)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Number of farms detected: {len(farm_areas)}\")\n",
    "    print(f\"Farm size classification: {class_counts}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n",
    "    \n",
    "    return farm_areas, farm_classes, class_counts\n",
    "\n",
    "# Try with a test image from the validation set\n",
    "test_img_dir = os.path.join(dataset.location, 'valid', 'images')\n",
    "test_img_files = os.listdir(test_img_dir)\n",
    "\n",
    "if test_img_files:\n",
    "    test_img_path = os.path.join(test_img_dir, test_img_files[0])\n",
    "    print(f\"Processing test image: {test_img_path}\")\n",
    "    farm_areas, farm_classes, class_counts = process_sample_image(test_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6babe",
   "metadata": {},
   "source": [
    "## 5. Recommendation System Based on Farm Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recommendations based on farm size\n",
    "def get_recommendations_by_size(farm_size):\n",
    "    recommendations = {\n",
    "        'Small': {\n",
    "            'Crop Selection': [\n",
    "                'Focus on high-value crops (e.g., specialty vegetables, herbs, berries)',\n",
    "                'Consider intercropping to maximize land use',\n",
    "                'Explore vertical farming techniques for space optimization'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in versatile, small-scale equipment',\n",
    "                'Consider equipment sharing programs or cooperatives',\n",
    "                'Focus on precision hand tools for specialized tasks'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Direct-to-consumer sales (farmers markets, CSA)',\n",
    "                'Develop value-added products',\n",
    "                'Leverage organic or specialty certifications'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement intensive organic practices',\n",
    "                'Consider agroecological approaches',\n",
    "                'Explore permaculture design principles'\n",
    "            ]\n",
    "        },\n",
    "        'Medium': {\n",
    "            'Crop Selection': [\n",
    "                'Balance between specialty and commodity crops',\n",
    "                'Consider crop rotation systems',\n",
    "                'Explore diversification strategies'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in mid-sized tractors and implements',\n",
    "                'Consider precision agriculture technology',\n",
    "                'Develop efficient irrigation systems'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Develop relationships with local wholesalers and restaurants',\n",
    "                'Consider cooperative marketing',\n",
    "                'Explore agritourism opportunities'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement integrated pest management',\n",
    "                'Consider conservation tillage practices',\n",
    "                'Develop soil health management plans'\n",
    "            ]\n",
    "        },\n",
    "        'Large': {\n",
    "            'Crop Selection': [\n",
    "                'Focus on efficient production of commodity crops',\n",
    "                'Consider dedicating portions to specialty high-value crops',\n",
    "                'Implement strategic crop rotation systems'\n",
    "            ],\n",
    "            'Equipment': [\n",
    "                'Invest in large-scale, efficient machinery',\n",
    "                'Implement precision agriculture and automation',\n",
    "                'Consider GPS guidance systems and variable rate technology'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'Develop contracts with processors and distributors',\n",
    "                'Consider futures markets and hedging strategies',\n",
    "                'Explore export opportunities'\n",
    "            ],\n",
    "            'Sustainability': [\n",
    "                'Implement conservation agriculture practices at scale',\n",
    "                'Consider renewable energy investments',\n",
    "                'Develop comprehensive nutrient management plans'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return recommendations.get(farm_size, {})\n",
    "\n",
    "# Function to display recommendations for a specific farm\n",
    "def display_farm_recommendations(farm_class):\n",
    "    recommendations = get_recommendations_by_size(farm_class)\n",
    "    \n",
    "    if not recommendations:\n",
    "        print(f\"No recommendations available for {farm_class} farms.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Recommendations for {farm_class} Farms ===\\n\")\n",
    "    \n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  • {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recommendations for each farm size category\n",
    "for size in ['Small', 'Medium', 'Large']:\n",
    "    display_farm_recommendations(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705153ea",
   "metadata": {},
   "source": [
    "## 6. End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end pipeline function\n",
    "def process_farm_image(image_path, pixel_scale=None):\n",
    "    \"\"\"\n",
    "    Process a satellite image to detect farms, classify them by size, and provide recommendations.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the satellite image\n",
    "        pixel_scale (float, optional): Scale factor in meters per pixel (if available)\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the results of the analysis\n",
    "    \"\"\"\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    # Step 1: Segment farms using the trained U-Net model\n",
    "    binary_mask, image = segment_farms(model, image_path, device, transform)\n",
    "    \n",
    "    # Step 2: Calculate farm sizes\n",
    "    unit = 'hectares' if pixel_scale else 'pixels'\n",
    "    farm_areas, labeled_mask = calculate_farm_sizes(binary_mask, pixel_scale)\n",
    "    \n",
    "    # Step 3: Classify farms by size\n",
    "    farm_classes, class_counts = classify_farms(farm_areas, unit)\n",
    "    \n",
    "    # Step 4: Print summary\n",
    "    print(f\"\\nFarm Analysis Summary:\")\n",
    "    print(f\"Total farms detected: {len(farm_areas)}\")\n",
    "    print(f\"Farm size distribution: {class_counts}\")\n",
    "    print(f\"Area unit: {unit}\")\n",
    "    \n",
    "    # Step 5: Calculate predominant farm size\n",
    "    if len(farm_areas) > 0:\n",
    "        predominant_size = max(class_counts, key=class_counts.get)\n",
    "        print(f\"Predominant farm size: {predominant_size}\")\n",
    "        \n",
    "        # Step 6: Provide recommendations based on predominant farm size\n",
    "        display_farm_recommendations(predominant_size)\n",
    "    else:\n",
    "        print(\"No farms detected in the image.\")\n",
    "    \n",
    "    # Step 7: Visualize results\n",
    "    if len(farm_areas) > 0:\n",
    "        visualize_farm_classification(image, labeled_mask, farm_areas, farm_classes)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'num_farms': len(farm_areas),\n",
    "        'farm_areas': farm_areas,\n",
    "        'farm_classes': farm_classes,\n",
    "        'class_counts': class_counts,\n",
    "        'predominant_size': predominant_size if len(farm_areas) > 0 else None\n",
    "    }\n",
    "\n",
    "# Test the end-to-end pipeline on a sample image (if available)\n",
    "if test_img_files:\n",
    "    test_img_path = os.path.join(test_img_dir, test_img_files[0])\n",
    "    results = process_farm_image(test_img_path)\n",
    "    \n",
    "    # You could save these results to a file or database for future reference\n",
    "    import json\n",
    "    with open('farm_analysis_results.json', 'w') as f:\n",
    "        # Convert non-serializable objects (like numpy arrays) to lists\n",
    "        serializable_results = {\n",
    "            'num_farms': results['num_farms'],\n",
    "            'farm_areas': [float(area) for area in results['farm_areas']],\n",
    "            'farm_classes': results['farm_classes'],\n",
    "            'class_counts': results['class_counts'],\n",
    "            'predominant_size': results['predominant_size']\n",
    "        }\n",
    "        json.dump(serializable_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346b6a0",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "### 7.1 Summary\n",
    "\n",
    "In this project, we have:\n",
    "1. Loaded and processed a dataset of satellite imagery with farm annotations\n",
    "2. Implemented and trained a U-Net model for farmland segmentation\n",
    "3. Developed methods to calculate farm sizes from segmentation masks\n",
    "4. Created a classification system to categorize farms by size\n",
    "5. Built a recommendation system providing tailored advice based on farm size\n",
    "6. Integrated all components into an end-to-end pipeline\n",
    "\n",
    "### 7.2 Limitations\n",
    "\n",
    "Current limitations of the system include:\n",
    "- Relies on image resolution and quality for accurate segmentation\n",
    "- Size classification thresholds may need adjustment for different regions\n",
    "- Lacks real-world unit calibration (meters/hectares) without proper image metadata\n",
    "- Recommendations are general and not region-specific\n",
    "\n",
    "### 7.3 Future Improvements\n",
    "\n",
    "Potential next steps for improving the system:\n",
    "1. **Model Enhancements**:\n",
    "   - Experiment with other architectures (DeepLabv3+, HRNet)\n",
    "   - Implement data augmentation for better generalization\n",
    "   - Train on a larger, more diverse dataset\n",
    "\n",
    "2. **Size Calculation**:\n",
    "   - Integrate with GIS systems to obtain accurate geospatial coordinates\n",
    "   - Develop methods to automatically determine image scale\n",
    "   - Account for terrain variations in area calculations\n",
    "\n",
    "3. **Recommendation System**:\n",
    "   - Incorporate climate and soil data for more targeted recommendations\n",
    "   - Develop region-specific recommendation models\n",
    "   - Create a more interactive recommendation interface\n",
    "\n",
    "4. **User Interface**:\n",
    "   - Develop a web or mobile application for easier access\n",
    "   - Allow users to upload their own imagery\n",
    "   - Provide visualization tools for farmers to explore results\n",
    "\n",
    "5. **Validation**:\n",
    "   - Conduct field validation with actual farm measurements\n",
    "   - Collect feedback from farmers on recommendation usefulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca07339",
   "metadata": {},
   "source": [
    "## 8. Transfer Learning with Pretrained Models\n",
    "\n",
    "U-Net can benefit greatly from transfer learning by using pretrained encoders. We'll implement this to improve both training speed and model accuracy.\n",
    "\n",
    "### 8.1 Pretrained U-Net Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the necessary packages\n",
    "!pip install segmentation-models-pytorch\n",
    "\n",
    "# Import libraries for transfer learning\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Define a U-Net model with a pretrained encoder\n",
    "class PretrainedUNet(nn.Module):\n",
    "    def __init__(self, encoder_name=\"resnet34\", encoder_weights=\"imagenet\"):\n",
    "        super(PretrainedUNet, self).__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,        # Choose encoder, e.g. resnet34, efficientnet-b0, etc.\n",
    "            encoder_weights=encoder_weights,  # Use pretrained weights (e.g. imagenet) or None\n",
    "            in_channels=3,                    # Input channels (RGB images)\n",
    "            classes=1,                        # Output channels (binary segmentation)\n",
    "            activation=\"sigmoid\"              # Final activation function\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create a pretrained model\n",
    "if torch.cuda.is_available():\n",
    "    # Show available encoders to choose from\n",
    "    print(\"Available pretrained encoders:\")\n",
    "    for i, encoder in enumerate(smp.encoders.get_encoder_names()):\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            print()  # Line break for readability\n",
    "        print(f\"{encoder}\", end=\", \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Initialize the pretrained model\n",
    "pretrained_model = PretrainedUNet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\")\n",
    "\n",
    "# Use DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for parallel training of pretrained model\")\n",
    "    pretrained_model = nn.DataParallel(pretrained_model)\n",
    "\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Define optimizer with different learning rates for encoder and decoder\n",
    "# This is a common technique for fine-tuning - lower learning rate for pretrained parts\n",
    "encoder_params = []\n",
    "decoder_params = []\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Handle DataParallel case\n",
    "    for name, param in pretrained_model.module.model.named_parameters():\n",
    "        if name.startswith(\"encoder\"):\n",
    "            encoder_params.append(param)\n",
    "        else:\n",
    "            decoder_params.append(param)\n",
    "else:\n",
    "    for name, param in pretrained_model.model.named_parameters():\n",
    "        if name.startswith(\"encoder\"):\n",
    "            encoder_params.append(param)\n",
    "        else:\n",
    "            decoder_params.append(param)\n",
    "\n",
    "optimizer_pretrained = optim.Adam([\n",
    "    {'params': encoder_params, 'lr': 0.0001},  # Lower learning rate for pretrained encoder\n",
    "    {'params': decoder_params, 'lr': 0.001}    # Higher learning rate for decoder\n",
    "])\n",
    "\n",
    "criterion_pretrained = nn.BCELoss()\n",
    "\n",
    "print(\"Pretrained U-Net model initialized with ResNet34 encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the pretrained model\n",
    "num_epochs_pretrained = 15  # Usually needs fewer epochs due to pretrained weights\n",
    "best_val_loss_pretrained = float('inf')\n",
    "best_pretrained_model_path = 'best_pretrained_unet_model.pth'\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses_pretrained = []\n",
    "val_losses_pretrained = []\n",
    "val_ious_pretrained = []\n",
    "\n",
    "# Mixed precision training setup for faster training\n",
    "if torch.cuda.is_available():\n",
    "    # Initialize the scaler for mixed precision training\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    scaler = GradScaler()\n",
    "    print(\"Using mixed precision training for faster performance\")\n",
    "\n",
    "print(f\"Beginning transfer learning with pretrained ResNet34 encoder...\")\n",
    "for epoch in range(num_epochs_pretrained):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_pretrained}\")\n",
    "    \n",
    "    # Train\n",
    "    pretrained_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer_pretrained.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if torch.cuda.is_available():\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                outputs = pretrained_model(images)\n",
    "                loss = criterion_pretrained(outputs, masks)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer_pretrained)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Standard training on CPU\n",
    "            outputs = pretrained_model(images)\n",
    "            loss = criterion_pretrained(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer_pretrained.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses_pretrained.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    pretrained_model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = pretrained_model(images)\n",
    "            loss = criterion_pretrained(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses_pretrained.append(val_loss)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    val_iou = evaluate_model(pretrained_model, val_loader, device)\n",
    "    val_ious_pretrained.append(val_iou)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation IoU: {val_iou:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss_pretrained:\n",
    "        best_val_loss_pretrained = val_loss\n",
    "        torch.save(pretrained_model.state_dict(), best_pretrained_model_path)\n",
    "        print(f\"Model saved to {best_pretrained_model_path}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Transfer learning completed!\")\n",
    "\n",
    "# Compare the performance of the basic UNet and pretrained UNet\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Basic UNet - Train')\n",
    "plt.plot(val_losses, label='Basic UNet - Val')\n",
    "plt.plot(train_losses_pretrained, label='Pretrained UNet - Train')\n",
    "plt.plot(val_losses_pretrained, label='Pretrained UNet - Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Plot IoU\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_ious, label='Basic UNet')\n",
    "plt.plot(val_ious_pretrained, label='Pretrained UNet')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('Validation IoU Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Plot convergence speed\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot([min(val_ious[:i+1]) for i in range(len(val_ious))], label='Basic UNet')\n",
    "plt.plot([min(val_ious_pretrained[:i+1]) for i in range(len(val_ious_pretrained))], label='Pretrained UNet')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Best IoU So Far')\n",
    "plt.title('Convergence Speed Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use the best model for inference\n",
    "if os.path.exists(best_pretrained_model_path):\n",
    "    pretrained_model.load_state_dict(torch.load(best_pretrained_model_path))\n",
    "    print(f\"Loaded best pretrained model from {best_pretrained_model_path}\")\n",
    "else:\n",
    "    print(\"Using current pretrained model state\")\n",
    "\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530e36a",
   "metadata": {},
   "source": [
    "## 9. Model Export and Optimization\n",
    "\n",
    "In this section, we'll export our models to formats suitable for deployment and optimize them for inference.\n",
    "\n",
    "### 9.1 TorchScript Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2badbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to TorchScript format\n",
    "def export_to_torchscript(model, example_input_tensor, model_name=\"farm_segmentation_model.pt\"):\n",
    "    \"\"\"\n",
    "    Export a PyTorch model to TorchScript format for production deployment.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to export\n",
    "        example_input_tensor: An example input tensor with the correct shape\n",
    "        model_name: The filename to save the model to\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # For DataParallel models, we need to access the module\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_to_trace = model.module\n",
    "    else:\n",
    "        model_to_trace = model\n",
    "    \n",
    "    # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing\n",
    "    try:\n",
    "        # Create a trace of the model with a sample input\n",
    "        traced_model = torch.jit.trace(model_to_trace, example_input_tensor)\n",
    "        \n",
    "        # Save the traced model\n",
    "        traced_model.save(model_name)\n",
    "        print(f\"TorchScript model saved to {model_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting model to TorchScript: {e}\")\n",
    "        return False\n",
    "\n",
    "# Export the basic U-Net model\n",
    "print(\"Exporting basic U-Net model to TorchScript...\")\n",
    "# Create a sample input tensor with the correct shape\n",
    "sample_input = torch.randn(1, 3, 256, 256, device=device)\n",
    "export_to_torchscript(model, sample_input, \"unet_farm_segmentation.pt\")\n",
    "\n",
    "# Export the pretrained U-Net model if it exists\n",
    "if 'pretrained_model' in locals():\n",
    "    print(\"Exporting pretrained U-Net model to TorchScript...\")\n",
    "    export_to_torchscript(pretrained_model, sample_input, \"pretrained_unet_farm_segmentation.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29bd0d",
   "metadata": {},
   "source": [
    "### 9.2 ONNX Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX format for wider compatibility\n",
    "def export_to_onnx(model, example_input_tensor, model_name=\"farm_segmentation_model.onnx\"):\n",
    "    \"\"\"\n",
    "    Export a PyTorch model to ONNX format for cross-platform deployment.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to export\n",
    "        example_input_tensor: An example input tensor with the correct shape\n",
    "        model_name: The filename to save the model to\n",
    "    \"\"\"\n",
    "    # For DataParallel models, we need to access the module\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_to_export = model.module\n",
    "    else:\n",
    "        model_to_export = model\n",
    "    \n",
    "    model_to_export.eval()\n",
    "    \n",
    "    try:\n",
    "        # Export the model to ONNX format\n",
    "        torch.onnx.export(\n",
    "            model_to_export,               # model being run\n",
    "            example_input_tensor,          # model input (or a tuple for multiple inputs)\n",
    "            model_name,                    # where to save the model\n",
    "            export_params=True,            # store the trained parameter weights inside the model file\n",
    "            opset_version=12,              # the ONNX version to export the model to\n",
    "            do_constant_folding=True,      # whether to execute constant folding for optimization\n",
    "            input_names=['input'],         # the model's input names\n",
    "            output_names=['output'],       # the model's output names\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},    # variable length axes\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(f\"ONNX model saved to {model_name}\")\n",
    "        \n",
    "        # Verify the ONNX model\n",
    "        import onnx\n",
    "        onnx_model = onnx.load(model_name)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"ONNX model checked - model is valid\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting model to ONNX: {e}\")\n",
    "        return False\n",
    "\n",
    "# Export the basic U-Net model to ONNX\n",
    "print(\"Exporting basic U-Net model to ONNX...\")\n",
    "sample_input = torch.randn(1, 3, 256, 256, device=device)\n",
    "export_to_onnx(model, sample_input, \"unet_farm_segmentation.onnx\")\n",
    "\n",
    "# Export the pretrained U-Net model if it exists\n",
    "if 'pretrained_model' in locals():\n",
    "    print(\"Exporting pretrained U-Net model to ONNX...\")\n",
    "    export_to_onnx(pretrained_model, sample_input, \"pretrained_unet_farm_segmentation.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test and visualize model predictions\n",
    "def visualize_model_predictions(model, test_img_path, device, transform):\n",
    "    \"\"\"\n",
    "    Test the model on a single image and visualize the results with original image, \n",
    "    ground truth mask (if available), and predicted segmentation mask.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained segmentation model\n",
    "        test_img_path: Path to the test image\n",
    "        device: Device to run inference on (cuda/cpu)\n",
    "        transform: Image transformations for model input\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(test_img_path).convert(\"RGB\")\n",
    "    filename = os.path.basename(test_img_path)\n",
    "    \n",
    "    # Get original size\n",
    "    original_size = image.size  # (width, height)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Try to find corresponding mask (if it exists in validation set)\n",
    "    mask_exists = False\n",
    "    mask_file = os.path.splitext(filename)[0] + '.txt'\n",
    "    mask_path = os.path.join(os.path.dirname(test_img_path).replace('images', 'labels'), mask_file)\n",
    "    \n",
    "    if os.path.exists(mask_path):\n",
    "        mask_exists = True\n",
    "        # Create empty mask with the same size as the image\n",
    "        ground_truth_mask = np.zeros(original_size[::-1], dtype=np.uint8)  # height, width\n",
    "        \n",
    "        # Read YOLOv8 format annotations\n",
    "        with open(mask_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        img_width, img_height = image.size\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) >= 5:\n",
    "                # Check if we have polygon points (instance segmentation)\n",
    "                if len(parts) > 5:\n",
    "                    # Extract polygon points\n",
    "                    polygon_points = []\n",
    "                    for i in range(5, len(parts), 2):\n",
    "                        if i+1 < len(parts):\n",
    "                            x = float(parts[i]) * img_width\n",
    "                            y = float(parts[i+1]) * img_height\n",
    "                            polygon_points.append((int(x), int(y)))\n",
    "                    \n",
    "                    if polygon_points:\n",
    "                        # Convert to numpy array for OpenCV\n",
    "                        pts = np.array(polygon_points, np.int32)\n",
    "                        pts = pts.reshape((-1, 1, 2))\n",
    "                        # Fill polygon with ones\n",
    "                        cv2.fillPoly(ground_truth_mask, [pts], 255)\n",
    "                else:\n",
    "                    # Use bounding box\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center = float(parts[1]) * img_width\n",
    "                    y_center = float(parts[2]) * img_height\n",
    "                    width = float(parts[3]) * img_width\n",
    "                    height = float(parts[4]) * img_height\n",
    "                    \n",
    "                    x1 = max(0, int(x_center - width / 2))\n",
    "                    y1 = max(0, int(y_center - height / 2))\n",
    "                    x2 = min(img_width - 1, int(x_center + width / 2))\n",
    "                    y2 = min(img_height - 1, int(y_center + height / 2))\n",
    "                    \n",
    "                    cv2.rectangle(ground_truth_mask, (x1, y1), (x2, y2), 255, -1)\n",
    "        \n",
    "        # Plot ground truth mask if found\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(ground_truth_mask, cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Process image for model prediction\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_mask = output.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Convert prediction to binary mask\n",
    "    pred_binary = (pred_mask > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    # Resize prediction back to original image size\n",
    "    pred_resized = cv2.resize(pred_binary, (original_size[0], original_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Plot prediction\n",
    "    subplot_pos = 3 if mask_exists else 2\n",
    "    plt.subplot(1, 3, subplot_pos)\n",
    "    plt.imshow(pred_resized, cmap='gray')\n",
    "    plt.title('Model Prediction')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # If ground truth exists, calculate IoU\n",
    "    if mask_exists:\n",
    "        # Calculate IoU\n",
    "        intersection = np.logical_and(ground_truth_mask > 0, pred_resized > 0).sum()\n",
    "        union = np.logical_or(ground_truth_mask > 0, pred_resized > 0).sum()\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        plt.suptitle(f'Farmland Segmentation - IoU: {iou:.4f}', fontsize=16)\n",
    "    else:\n",
    "        plt.suptitle('Farmland Segmentation Prediction', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create and show overlay image for better visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create an RGB version of the prediction for overlay\n",
    "    pred_color = np.zeros((pred_resized.shape[0], pred_resized.shape[1], 4), dtype=np.uint8)\n",
    "    pred_color[pred_resized > 0] = [0, 255, 0, 128]  # Semi-transparent green for predictions\n",
    "    \n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Plot image with prediction overlay\n",
    "    plt.imshow(img_array)\n",
    "    plt.imshow(pred_color, alpha=0.5)\n",
    "    plt.title('Prediction Overlay on Original Image')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_resized, iou if mask_exists else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
